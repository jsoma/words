[
  {
    "objectID": "multi-language-qa-gpt.html",
    "href": "multi-language-qa-gpt.html",
    "title": "Multi-language document Q&A with LangChain and ChatGPT",
    "section": "",
    "text": "Hi, I’m Soma! You can find me on email at jonathan.soma@gmail.com, on Twitter at @dangerscarf, or maybe even on this newsletter I’ve never sent."
  },
  {
    "objectID": "multi-language-qa-gpt.html#using-gpt-langchain-and-vector-stores-to-ask-questions-of-documents-in-languages-you-dont-speak",
    "href": "multi-language-qa-gpt.html#using-gpt-langchain-and-vector-stores-to-ask-questions-of-documents-in-languages-you-dont-speak",
    "title": "Multi-language document Q&A with LangChain and ChatGPT",
    "section": "Using GPT, LangChain, and vector stores to ask questions of documents in languages you don’t speak",
    "text": "Using GPT, LangChain, and vector stores to ask questions of documents in languages you don’t speak\nWe don’t speak Hungarian, but we demand to have my questions about Hungarian folktales answered! Let’s use GPT to do this for us!\nThis might be useful if you’re doing a cross-border investigation, are interested in academic papers outside of your native tongue, or are just interested in learning how LangChain and document Q&A works.\nIn this tutorial, we’ll look at:\n\nWhy making ChatGPT read an whole book is impossible\nHow to provide GPT (and other AI tools) with context to provide answers\n\nIf you don’t want to read all of this nonsense you can go directly to the source and check out Question Answering or Question Answering with Sources. This just adds a bit of multi-language sparkle on top!"
  },
  {
    "objectID": "multi-language-qa-gpt.html#our-source-material",
    "href": "multi-language-qa-gpt.html#our-source-material",
    "title": "Multi-language document Q&A with LangChain and ChatGPT",
    "section": "Our source material",
    "text": "Our source material\nWe’ll begin by downloading the source material. If your original documents are in PDF form or anything like that, you’ll want to convert them to text first.\nOur reference is a book of folktales called Eredeti népmesék by László Arany on Project Gutenberg. It’s just a basic text file so we can download it easily.\n\nimport requests\nimport re\n\n# Gutenberg pretends everything is English, which\n# means \"Hát gyöngyömadta\" gets really mangled\nresponse = requests.get(\"https://www.gutenberg.org/files/38852/38852-0.txt\")\ntext = response.content.decode(\"utf-8\")\n\n# Cleaning up newlines\ntext = text.replace(\"\\r\", \"\")\ntext = re.sub(\"\\n(?=[^\\n])\", \"\", text)\n\n# Saving the book\nwith open('book.txt', 'w') as f:\n    f.write(text)\n\nAnd the text is indeed in Hungarian:\n\ntext[3000:3300]\n\n'be, de az is épen úgy járt, mint abátyja, ez is kiszaladt a szobából.\\nHarmadik nap a legfiatalabb királyfin volt a sor; a bátyjai be se’akarták ereszteni, hogy ha ők ki nem tudták venni az apjokból, biz’ e’se’ sokra megy, de a királyfi nem tágitott, hanem bement. Mikor elmondtahogy m’ért jött, ehez '\n\n\nLife would be nice if we could just feed it directly to ChatGPT and start asking questions, but you can’t make ChatGPT read a whole book. After it gets partway through the book ChatGPT starts forgetting the earlier pieces!\nThere are a few tricks to get around this. We’ll work with one of the simplest for now:\n\nSplit our original text up into pieces\nFind the pieces most relevant to our question\nSend those pieces to ChatGPT along with our question\n\nNewer LLMs can deal with a lot more tokens at a time – GPT-4 has both an 8k and 32k version – but hey, we work with what we’ve got."
  },
  {
    "objectID": "multi-language-qa-gpt.html#part-1-split-our-original-text-up-into-pieces",
    "href": "multi-language-qa-gpt.html#part-1-split-our-original-text-up-into-pieces",
    "title": "Multi-language document Q&A with LangChain and ChatGPT",
    "section": "Part 1: Split our original text up into pieces",
    "text": "Part 1: Split our original text up into pieces\nTo do pretty much everything from here on out we’re relying on LangChain, a really fun library that allows you to bundle together different common tasks when working with language models. It’s best trick is chaining together AI at different steps in the process, but for the moment we’re just using its text search abilities.\nWe’re going to split our text up into 1000-character chunks, which should be around 150-200 words apiece. I’m also going to add a little overlap.\n\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nloader = TextLoader('book.txt')\ndocuments = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\ndocs = text_splitter.split_documents(documents)\n\nTechnically speaking I’m using a RecursiveCharacterTextSplitter, which tries to keep paragraphs and sentences and all of those things together, so it might go above or below 1000. But it should generally hit the mark.\n\nlen(docs)\n\n440\n\n\nOverall this gave us just over 400 documents. Let’s pick one at random to check out, just to make sure things went okay.\n\ndocs[109]\n\nDocument(page_content='Mikor aztán eljött a lakodalom napja, felöltözött, de olyan ruhába, hogyTündérországban se igen látni párját sátoros ünnepkor se, csak elfogta acselédje szemefényét. Mire a királyi palotához ért, már ott ugyancsakszólott a muzsika, úgy tánczoltak, majd leszakadt a ház, még a süketnekis bokájába ment a szép muzsika.', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0)\n\n\nIt’s a little short, but it’s definitely part of the folktales. According to Google Translate:\n\nWhen the day of the wedding came, she dressed up, but in such a dress that one would not see her partner in a fairyland even during a tent festival, she only caught the eye of her mistress. By the time he got to the royal palace, the music was already playing there too, they were dancing like that, and then the house was torn apart, the beautiful music even went to the deaf man’s ankles\n\nSounds like a pretty fun party!"
  },
  {
    "objectID": "multi-language-qa-gpt.html#part-2-find-the-pieces-most-relevant-to-our-question",
    "href": "multi-language-qa-gpt.html#part-2-find-the-pieces-most-relevant-to-our-question",
    "title": "Multi-language document Q&A with LangChain and ChatGPT",
    "section": "Part 2: Find the pieces most relevant to our question",
    "text": "Part 2: Find the pieces most relevant to our question\n\nFeaturing text embeddings and semantic search!\nIf we’re asking questions about a wedding, we can’t just look for the text wedding – our documents are in Hungarian, so that’s lakodalom (I think). Instead, we’re going to use someting called embeddings.\nEmbeddings take a word, sentence, or snippet of text and turn it into a string of numbers. Take the sentences below as an example: I’ve scored each one of them as to how much they’re about shopping, home, and animals.\n\n\n\nsentence\nshopping\nhome\nanimals\nresult\n\n\n\n\nYou should buy a house\n0.9\n0.8\n0\n(0.9, 0.8, 0.0)\n\n\nThe cat is in the house\n0\n1\n0.8\n(0.0, 1.0, 0.8)\n\n\nThe dog bought a pet mouse\n1\n0.2\n1\n(1.0, 0.2, 1.0)\n\n\n\nLet’s say we a fourth sentence – the dog is at home. Our sentence might score (0.0 1.0 0.9) since it’s about home and animals, but not shipping. How can we find similar ones?\nEven though it doesn’t have any words that match the cat is in the house, its numbers are pretty close. That way a computer can say yes, sure, they’re probably about the same thing!\nInstead of reasonable categories like mine, though, actual embeddings have read the entire internet and decided on something like 384 or 512 difference things to rate your text on (and they definitely aren’t as simple as “animal”).\n\nYou might want to read my introduction to word embeddings for more details and conceptual document similarity.\n\nThere are all sorts of embeddings out there, that each score text based on different parameters. While not all of them support multiple languages, it isn’t hard to find some that do. We’re going to pick paraphrase-multilingual-MiniLM-L12-v2 since it supports a delightful 50 languages. These multilingual embeddings have read enough sentences across the all-languages-speaking internet to somehow know things like that cat and lion and Katze and tygrys and 狮 are all vaguely feline.\n\nfrom langchain.embeddings import HuggingFaceEmbeddings\nembeddings = HuggingFaceEmbeddings(model_name='paraphrase-multilingual-MiniLM-L12-v2')\n\nIn order to find the most relevant pieces of text, we’ll also need something that can store and search embeddings. That way when we want to find anything about weddings it won’t have a problem finding lakodalom.\nWe’re going to use Chroma for no real reason, just because it has a convenient LangChain extension.\n\n# You'll probably need to install chromadb\n# !pip install chromadb\n\n\nfrom langchain.vectorstores import Chroma\n\ndb = Chroma.from_documents(docs, embeddings)\n\nRunning Chroma using direct local API.\nUsing DuckDB in-memory for database. Data will be transient.\n\n\nNow that this is stored, we can search for weddings at a festival.\n\ndb.similarity_search(\"weddings at a festival with loud music\", k=1)\n\n[Document(page_content='Eltelt az egy hónap, elérkezett az esküvő napja, ott volt a sok vendég,köztök a boltos is, csak a vőlegényt meg a menyasszonyt nem lehetettlátni. Bekövetkezett az ebéd ideje is, mindnyájan vígan ültek le azasztalhoz, elkezdtek enni. Az volt a szokás a gróf házánál, hogy mindenembernek egy kis külön tálban vitték az ételt; a boltos amint a magatáljából szedett levest, hát csak alig tudta megenni, olyan sótalanvolt, nézett körül só után, de nem volt az egész asztalon; a másodikétel még sótalanabb volt, a harmadik meg már olyan volt, hogy hozzá se’tudott nyúlni. Kérdezték tőle hogy mért nem eszik? tán valami baja vanaz ételnek? amint ott vallatták, eszébe jutott a lyánya, hogy az nekiazt mondta, hogy úgy szereti, mint a sót, elkezdett sírni; kérdeztékaztán tőle, hogy mért sír, akkor elbeszélt mindent, hogy volt neki egylyánya, az egyszer neki azt mondta, hogy úgy szereti mint a sót, őmegharagudott érte, elkergette a házától, lám most látja, hogy milyenigazságtalan volt iránta, milyen jó a só, ,,de hej ha még egyszervisszahozná az isten hozzám, majd meg is becsülném, első lenne aházamnál; meg is bántam én azt már sokszor, de már akkor késő volt.’’', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0)]\n\n\nIt’s a match!"
  },
  {
    "objectID": "multi-language-qa-gpt.html#part-3-send-the-matches-to-chatgpt-along-with-our-question",
    "href": "multi-language-qa-gpt.html#part-3-send-the-matches-to-chatgpt-along-with-our-question",
    "title": "Multi-language document Q&A with LangChain and ChatGPT",
    "section": "Part 3: Send the matches to ChatGPT along with our question",
    "text": "Part 3: Send the matches to ChatGPT along with our question\nThis is the part where LangChain really shines. We just say “hey, go get the relevant pieces from our database, then go talk to GPT for us!”\nFirst, we’ll fire up our connection to GPT.\n\nfrom langchain.llms import OpenAI\n\n# Connect to GPT-3.5 turbo\nopenai_api_key = \"sk-...\"\n\nllm = OpenAI(\n    model_name=\"gpt-3.5-turbo\",\n    temperature=0,\n    openai_api_key=openai_api_key)\n\nSecond, we’ll put together our vector-based Q&A. This is a custom LangChain tool that hooks the pieces together for us.\n\n# Vector-database-based Q&A\nqa = VectorDBQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    vectorstore=db\n)"
  },
  {
    "objectID": "multi-language-qa-gpt.html#lets-see-it-in-action",
    "href": "multi-language-qa-gpt.html#lets-see-it-in-action",
    "title": "Multi-language document Q&A with LangChain and ChatGPT",
    "section": "Let’s see it in action!",
    "text": "Let’s see it in action!\nI’m going to ask some questions about Zsuzska, who according to some passages apparently stole some of the devil’s belongings\n\nquery = \"What did Zsuzska steal from the devil?\"\nqa.run(query)\n\n'The tenger-ütő pálczát (sea-beating stick).'\n\n\n\nquery = \"Why did Zsuzska steal from the devil?\"\nqa.run(query)\n\n\"Zsuzska was forced to steal from the devil by the king, who threatened her with death if she didn't.\"\n\n\n\nquery = \"Why were the king's aunts jealous of Zsuzska?\"\nqa.run(query)\n\n\"The king's aunts were jealous of Zsuzskát because the king had grown to love her and they wanted to undermine her by claiming that she could not steal the devil's golden cabbage head.\"\n\n\n\nquery = \"Who did Janko marry?\"\nqa.run(query)\n\n'Janko married a beautiful princess.'\n\n\n\nquery = \"How did Janko meet the king's daughter?\"\nqa.run(query)\n\n\"The context does not provide information on a character named Janko meeting the king's daughter.\"\n\n\n\nquery = \"Who was the bloody cow?\"\nqa.run(query)\n\n'The bloody cow was a cow that Ferkó rode away on after throwing the lasso at it.'\n\n\n\nquery = \"Why was Ferko's mother disguised as a cow?\"\nqa.run(query)\n\n\"Ferko's mother was not disguised as a cow, but rather the red cow was actually Ferko's mother, the first queen.\""
  },
  {
    "objectID": "multi-language-qa-gpt.html#improving-our-matches",
    "href": "multi-language-qa-gpt.html#improving-our-matches",
    "title": "Multi-language document Q&A with LangChain and ChatGPT",
    "section": "Improving our matches",
    "text": "Improving our matches\nWhen we asked what was stolen from the devil, we were told “The tenger-ütő pálczát (sea-beating stick).” I know for a fact more things were stolen than that!\nIf we provide more context, we can hopefully get better answers. We have two options:\n\nIncrease the size of our window/include more overlap\nProvide more documents to GPT as context when asking for an answer\n\nSince I haven’t seen the second one show up too many places, let’s do that instead. We’ll increase the number of results to provide as context to eight by passing with k=8.\n\nqa = VectorDBQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    vectorstore=db,\n    k=8\n)\n\nAt this point we have to be careful of two things: money and token limits.\n\nMoney: Larger requests cost more.\nToken limits We have around 3,000 words to work with for each GPT-3.5 request. If each chunk is up to 250 words long, this gets us up to 2,000 words. We should be safe!\n\nBut we want good answers, right??? Let’s see if it works:\n\nquery = \"What did Zsuzská steal from the devil?\"\nqa.run(query)\n\n\"Zsuzska stole the devil's tenger-ütő pálczája (sea-beating stick), tenger-lépő czipője (sea-stepping shoes), and arany kis gyermek (golden baby) in an arany bölcső (golden cradle). She also previously stole the devil's tenger-ütőpálczát (sea-beating stick) and arany fej káposztát (golden head cabbage).\"\n\n\nPerfect! That gold cabbage sounds great, and it’s almost time for lunch, so let’s wrap up with one more thing."
  },
  {
    "objectID": "multi-language-qa-gpt.html#seeing-the-context",
    "href": "multi-language-qa-gpt.html#seeing-the-context",
    "title": "Multi-language document Q&A with LangChain and ChatGPT",
    "section": "Seeing the context",
    "text": "Seeing the context\nIf you’re having trouble getting good answers to your questions, it might be because the context you’re providing isn’t very good. I was actually having this issue earlier on with distiluse-base-multilingual-cased-v2 before I switched to paraphrase-multilingual-MiniLM-L12-v2! I honestly don’t know the difference between them, just that one provided more relevant snippets to GPT.\nLet’s see what context is being provided to GPT for each search!\n\nMethod one: Context from the question\nTo see what context is being sent to GPT, include the return_source_documents=True parameter.\n\nqa = VectorDBQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    vectorstore=db,\n    return_source_documents=True\n)\n\n\nquery = \"What did Zsuzská steal from the devil?\"\nresult = qa({\"query\": query})\n\n\nresult[\"result\"]\n\n'Zsuzská stole the tenger-ütő pálczát (sea-beater stick) from the devil.'\n\n\n\nresult[\"source_documents\"]\n\n[Document(page_content='Hiába tagadta szegény Zsuzska, nem használt semmit, elindult hát nagyszomorúan. Épen éjfél volt, mikor az ördög házához ért, aludt az ördögis, a felesége is. Zsuzska csendesen belopódzott, ellopta a tenger-ütőpálczát, avval bekiáltott az ablakon.\\n– Hej ördög, viszem ám már a tenger-ütő pálczádat is.\\n– Hej kutya Zsuzska, megöletted három szép lyányomat, elloptad atenger-lépő czipőmet, most viszed a tenger-ütő pálczámat, de majdmeglakolsz te ezért.\\nUtána is szaladt, de megint csak a tengerparton tudott közel jutnihozzá, ott meg Zsuzska megütötte a tengert a tenger-ütő pálczával,kétfelé vált előtte, utána meg összecsapódott, megint nem foghatta megaz ördög. Zsuzska ment egyenesen a királyhoz.\\n– No felséges király, elhoztam már a tengerütő pálczát is.', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0),\n Document(page_content='De Zsuzska nem adta;,,Tán bolond vagyok, hogy visszaadjam, mikor kivülvagyok már vele az udvaron?!’’ Az ördög kergette egy darabig, de sehogyse tudta utolérni, utoljára is visszafordult, Zsuzska pedig mentegyenesen a király elibe, od’adta neki az arany fej káposztát.\\n– No felséges király elhoztam már ezt is!\\nA két nénjét Zsuzskának, majd hogy meg nem ütötte a guta, mikormegtudták, hogy Zsuzskának most se’ lett semmi baja, másnap megintbementek a királyhoz.\\n– Jaj felséges király van még annak az ördögnek egy arany kis gyermekeis arany bölcsőben, Zsuzska azt beszéli fűnek-fának, hogy ő azt is eltudná lopni.\\nMegint behivatta a király Zsuzskát.\\n– Fiam Zsuzska, azt hallottam, hogy van annak az ördögnek egy arany kisgyermeke is, arany bölcsőben, te azt is el tudod lopni, azt beszélted,azért ha az éjjel el nem lopod, halálnak halálával halsz meg.', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0),\n Document(page_content='– No felséges király, elhoztam már a tengerütő pálczát is.\\nA király még jobban megszerette Zsuzskát, hogy olyan életre való, de anénjei még jobban irigykedtek rá, csakhamar megint avval árulták be,hogy van annak az ördögnek egy arany fej káposztája is, Zsuzska azt isel tudná lopni, azt mondta. A király megint ráparancsolt Zsuzskára erősparancsolattal, hogy ha a káposztát el nem lopja, halálnak halálával halmeg.\\nElindult hát szegény Zsuzska megint, el is ért szerencsésen épen éjfélreaz ördög kertjibe, levágta az arany fej káposztát, avval bekiáltott azablakon.\\n– Hej ördög, viszem ám már az arany fej káposztádat is.\\n– Hej kutya Zsuzska, megöletted három szép lyányomat, elloptad atenger-lépő czipőmet, elloptad a tenger-ütő pálczámat, most viszed azarany fej káposztámat, csak ezt az egyet add vissza, soha szemedre sevetem.', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0),\n Document(page_content='Zsuzska csak nevette, de majd hogy sírás nem lett a nevetésből, mert azördög utána iramodott, Zsuzska meg nem igen tudott a nehéz bölcsővelszaladni, úgy annyira, hogy mire a tengerparthoz értek, tiz lépés nemsok, de annyi se volt köztök, hanem ott aztán Zsuzska felrántotta atenger-lépő czipőt, úgy átlépte vele a tengert, mint ha ott se lettvolna, avval mént egyenesen a király elibe, od’adta neki az arany kisgyermeket.\\nA király a mint meglátta, csak egy szikrába mult, hogy össze-vissza nemcsókolta Zsuzskát, de az is csak egy cseppbe mult ám, hogy a két nénjemeg nem pukkadt mérgibe, mikor meghallotta, hogy Zsuzska megintvisszakerült. Fúrta az oldalukat rettenetesen az irigység, mert látták,hogy a király napról-napra jobban szereti Zsuzskát. Bementek hát akirályhoz megint, azt hazudták neki hogy Zsuzska azt mondta, hogy vanannak az ördögnek egy zsák arany diója, ő azt is el tudná lopni.\\nMaga elibe parancsolta a király megint Zsuzskát:', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0)]\n\n\n\n\nMethod two: Just ask your database\nIf you already know what GPT is going to say in response and you’re debugging on specific query, you can just ask your database what the relevant snippets are!\n\ndb.similarity_search(\"What did Zsuzská steal from the devil?\", k=2)\n\n[Document(page_content='Hiába tagadta szegény Zsuzska, nem használt semmit, elindult hát nagyszomorúan. Épen éjfél volt, mikor az ördög házához ért, aludt az ördögis, a felesége is. Zsuzska csendesen belopódzott, ellopta a tenger-ütőpálczát, avval bekiáltott az ablakon.\\n– Hej ördög, viszem ám már a tenger-ütő pálczádat is.\\n– Hej kutya Zsuzska, megöletted három szép lyányomat, elloptad atenger-lépő czipőmet, most viszed a tenger-ütő pálczámat, de majdmeglakolsz te ezért.\\nUtána is szaladt, de megint csak a tengerparton tudott közel jutnihozzá, ott meg Zsuzska megütötte a tengert a tenger-ütő pálczával,kétfelé vált előtte, utána meg összecsapódott, megint nem foghatta megaz ördög. Zsuzska ment egyenesen a királyhoz.\\n– No felséges király, elhoztam már a tengerütő pálczát is.', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0),\n Document(page_content='De Zsuzska nem adta;,,Tán bolond vagyok, hogy visszaadjam, mikor kivülvagyok már vele az udvaron?!’’ Az ördög kergette egy darabig, de sehogyse tudta utolérni, utoljára is visszafordult, Zsuzska pedig mentegyenesen a király elibe, od’adta neki az arany fej káposztát.\\n– No felséges király elhoztam már ezt is!\\nA két nénjét Zsuzskának, majd hogy meg nem ütötte a guta, mikormegtudták, hogy Zsuzskának most se’ lett semmi baja, másnap megintbementek a királyhoz.\\n– Jaj felséges király van még annak az ördögnek egy arany kis gyermekeis arany bölcsőben, Zsuzska azt beszéli fűnek-fának, hogy ő azt is eltudná lopni.\\nMegint behivatta a király Zsuzskát.\\n– Fiam Zsuzska, azt hallottam, hogy van annak az ördögnek egy arany kisgyermeke is, arany bölcsőben, te azt is el tudod lopni, azt beszélted,azért ha az éjjel el nem lopod, halálnak halálával halsz meg.', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0)]\n\n\nYou can keep playing with your k values until you get what you think is enough context."
  },
  {
    "objectID": "multi-language-qa-gpt.html#improvements-and-next-steps",
    "href": "multi-language-qa-gpt.html#improvements-and-next-steps",
    "title": "Multi-language document Q&A with LangChain and ChatGPT",
    "section": "Improvements and next steps",
    "text": "Improvements and next steps\nThis is a collection of folktales, not one long story. That means asking about something like a wedding might end up mixing together all sorts of different stories! Our next step will allow us to add other books, filter stories from one another, and more techniques that can help with larger, more complex datasets.\nIf you’re interested in hearing when it comes out, feel free to follow me @dangerscarf or hop on my mailing list. Questions, comments, and blind cat adoption inquiries can go to jonathan.soma@gmail.com."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "J Soma’s website???",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMar 18, 2023\n\n\nMulti-language document Q&A with LangChain and ChatGPT\n\n\nJonathan Soma\n\n\n\n\n\n\nNo matching items"
  }
]