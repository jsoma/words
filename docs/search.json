[
  {
    "objectID": "spotify-langchain-chatgpt.html",
    "href": "spotify-langchain-chatgpt.html",
    "title": "Building Spotify playlists based on vibes using LangChain and GPT",
    "section": "",
    "text": "Hi, I’m Soma! You can find me on email at jonathan.soma@gmail.com, on Twitter at @dangerscarf, or maybe even on this newsletter I’ve never sent."
  },
  {
    "objectID": "spotify-langchain-chatgpt.html#getting-my-api-keys",
    "href": "spotify-langchain-chatgpt.html#getting-my-api-keys",
    "title": "Building Spotify playlists based on vibes using LangChain and GPT",
    "section": "Getting my API keys",
    "text": "Getting my API keys\nBoth GPT and Spotify require me to prove my identity using API keys. If you had my keys you’d be able to impersonate me, talk to my chatbots, and make a bunch of awful playlists – we don’t want any of those happening. Instead of putting the API keys in my notebook, I’m using dotenv-python to keep them nice and secret. I recommend it!\n\n%load_ext dotenv\n%dotenv"
  },
  {
    "objectID": "spotify-langchain-chatgpt.html#accessing-openaigpt",
    "href": "spotify-langchain-chatgpt.html#accessing-openaigpt",
    "title": "Building Spotify playlists based on vibes using LangChain and GPT",
    "section": "Accessing OpenAI/GPT",
    "text": "Accessing OpenAI/GPT\nTo access GPT-3.5-turbo, we’re use to use a LangChain chain.\nIt’s a little more complicated than when we were talking to fairy tales, but the former method of using a plain OpenAI object is being deprecated in favor of ChatOpenAI.\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain import LLMChain\n\nllm = ChatOpenAI(model_name='gpt-3.5-turbo')\n\nLet’s test it out with a sample PromptTemplate about energetic songs.\n\nprompt = PromptTemplate(\n    input_variables=[\"question\"],\n    template=\"{question}\",\n)\n\nchain = LLMChain(llm=llm, prompt=prompt)\n\nNow that it’s assembled, let’s use it.\n\nresponse = chain.run(\"The Promise Ring\")\nprint(response)\n\nSome of the more energetic songs by The Promise Ring include:\n\n1. \"Is This Thing On?\"\n2. \"Emergency! Emergency!\"\n3. \"Red & Blue Jeans\"\n4. \"Happiness is All the Rage\"\n5. \"Make Me a Chevy\"\n6. \"Jersey Shore\"\n7. \"B Is for Bethlehem\"\n8. \"Why Did We Ever Meet?\"\n9. \"Stop Playing Guitar\"\n10. \"Pink Chimneys\"\n\n\nYes, that’s an answer, but it isn’t good enough. Asking GPT for energetic songs works, I want to at least pretend that we’re basing this on science! You never know if record labels from the late 90’s are funnelling money to OpenAI to bias the results.\nLuckily, Spotify has that information: their API includes access a track’s audio features including loudness, danceability and energy! With that in mind, our goal is now to create a way for GPT and Spotify to interface so that we can leverage that information when building our playlist."
  },
  {
    "objectID": "spotify-langchain-chatgpt.html#accessing-spotify",
    "href": "spotify-langchain-chatgpt.html#accessing-spotify",
    "title": "Building Spotify playlists based on vibes using LangChain and GPT",
    "section": "Accessing Spotify",
    "text": "Accessing Spotify\nWe’re going to use the Spotipy Python library to access Spotify. It handles all of the OAuth login, the refreshing of tokens (they’re only good for 5 minutes!), and everything of that ilk.\n\nimport os\nimport spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\n\nauth = SpotifyClientCredentials(\n    client_id=os.environ['SPOTIPY_CLIENT_ID'],\n    client_secret=os.environ['SPOTIPY_CLIENT_SECRET']\n)\nsp = spotipy.Spotify(auth_manager=auth)\n\nIf we want songs by the band Wet Leg, we can’t just say “give me Wet Legs top tracks.” Instead, we need to find Wet Leg’s URI - uniform resource indicator, Spotify’s cataloguing ID – and then use that to get the top tracks. It’s important to note that it’s almost always a multi-step process to get anything useful from Spotify.\nFor example, let’s say we want to filter for energetic songs, which means we want a track’s audio features. If you wanted the audio features for a track but only know the band name, the process looks like what is outlined below:\n\nStep one: Find the artist URI\nWe’ll use Spotify’s search to try to find the artist Wet Leg, and then assume the first result is the right one.\n\nresponse = sp.search('Wet Leg', type='artist')\nuri = response['artists']['items'][0]['uri']\nuri\n\n'spotify:artist:2TwOrUcYnAlIiKmVQkkoSZ'\n\n\n\n\nStep two: Find the tracks URIs\nWe’ll then use the artist’s URI to find some top tracks from that artist (there’s actually an endpoint for top tracks!).\n\nresponse = sp.artist_top_tracks(uri)\ntop_five = response['tracks'][:5]\n\nfor track in top_five:\n    print(track['popularity'], track['name'], track['uri'])\n\n70 Wet Dream spotify:track:260Ub1Yuj4CobdISTOBvM9\n66 Chaise Longue spotify:track:0nys6GusuHnjSYLW0PYYb7\n63 Being In Love spotify:track:4VBE0mwU8Nmm8hiqfCe4Ve\n62 Angelica spotify:track:3EwTIu5qka2l5ZekB0b6QC\n60 Ur Mum spotify:track:4ug5wsIcbAPBun8TCKn2t6\n\n\n\n\nStep three: Find the audio features\nInstead of coming with the track results, the danceability and all of those scores are in a completely different endpoint! So we’ll now use the track URIs to access the audio features.\n\nimport pandas as pd\n\nuris = [track['uri'] for track in top_five]\naudio_features = sp.audio_features(uris)\npd.DataFrame(audio_features)\n\n\n\n\n\n\n\n\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntype\nid\nuri\ntrack_href\nanalysis_url\nduration_ms\ntime_signature\n\n\n\n\n0\n0.721\n0.701\n2\n-5.941\n1\n0.0306\n0.000927\n0.026000\n0.234\n0.892\n130.091\naudio_features\n260Ub1Yuj4CobdISTOBvM9\nspotify:track:260Ub1Yuj4CobdISTOBvM9\nhttps://api.spotify.com/v1/tracks/260Ub1Yuj4Co...\nhttps://api.spotify.com/v1/audio-analysis/260U...\n140080\n3\n\n\n1\n0.684\n0.749\n7\n-6.565\n1\n0.0600\n0.001350\n0.111000\n0.141\n0.935\n160.021\naudio_features\n0nys6GusuHnjSYLW0PYYb7\nspotify:track:0nys6GusuHnjSYLW0PYYb7\nhttps://api.spotify.com/v1/tracks/0nys6GusuHnj...\nhttps://api.spotify.com/v1/audio-analysis/0nys...\n196905\n4\n\n\n2\n0.716\n0.687\n9\n-4.940\n0\n0.0342\n0.009220\n0.110000\n0.123\n0.342\n126.030\naudio_features\n4VBE0mwU8Nmm8hiqfCe4Ve\nspotify:track:4VBE0mwU8Nmm8hiqfCe4Ve\nhttps://api.spotify.com/v1/tracks/4VBE0mwU8Nmm...\nhttps://api.spotify.com/v1/audio-analysis/4VBE...\n122467\n4\n\n\n3\n0.491\n0.870\n0\n-5.138\n1\n0.0393\n0.000141\n0.000729\n0.368\n0.314\n131.989\naudio_features\n3EwTIu5qka2l5ZekB0b6QC\nspotify:track:3EwTIu5qka2l5ZekB0b6QC\nhttps://api.spotify.com/v1/tracks/3EwTIu5qka2l...\nhttps://api.spotify.com/v1/audio-analysis/3EwT...\n232320\n4\n\n\n4\n0.685\n0.720\n4\n-5.553\n1\n0.0280\n0.007020\n0.275000\n0.425\n0.554\n133.016\naudio_features\n4ug5wsIcbAPBun8TCKn2t6\nspotify:track:4ug5wsIcbAPBun8TCKn2t6\nhttps://api.spotify.com/v1/tracks/4ug5wsIcbAPB...\nhttps://api.spotify.com/v1/audio-analysis/4ug5...\n201253\n4\n\n\n\n\n\n\n\nSo what I’m saying is: we can’t just hit one endpoint and run away. This is a lot of work!"
  },
  {
    "objectID": "spotify-langchain-chatgpt.html#how-the-apichain-works",
    "href": "spotify-langchain-chatgpt.html#how-the-apichain-works",
    "title": "Building Spotify playlists based on vibes using LangChain and GPT",
    "section": "How the APIChain works",
    "text": "How the APIChain works\nAn APIChain can be used to access an API! This is a slightly adapted version of the APIChain example from the docs.\n\nfrom langchain.chains import APIChain\nfrom langchain.chains.api import open_meteo_docs\n\nchain_new = APIChain.from_llm_and_api_docs(llm, open_meteo_docs.OPEN_METEO_DOCS, verbose=True)\nchain_new.run('What is the weather like right now in Munich, Germany in degrees Farenheit? Do not include a forecast.')\n\n\n\n&gt; Entering new APIChain chain...\nhttps://api.open-meteo.com/v1/forecast?latitude=48.137154&longitude=11.576124&current_weather=true&temperature_unit=fahrenheit\n{\"latitude\":48.14,\"longitude\":11.58,\"generationtime_ms\":0.1989603042602539,\"utc_offset_seconds\":0,\"timezone\":\"GMT\",\"timezone_abbreviation\":\"GMT\",\"elevation\":526.0,\"current_weather\":{\"temperature\":50.0,\"windspeed\":16.1,\"winddirection\":254.0,\"weathercode\":3,\"time\":\"2023-03-26T16:00\"}}\n\n&gt; Finished chain.\n\n\n'The weather in Munich, Germany right now is 50 degrees Fahrenheit.'\n\n\nAn important thing to take note of here is open_meteo_docs.OPEN_METEO_DOCS: along with our prompt and an llm, we’re also sending the documentation for the Open-Meteo API. It looks like this:\n\nprint(open_meteo_docs.OPEN_METEO_DOCS[:1000])\n\nBASE URL: https://api.open-meteo.com/\n\nAPI Documentation\nThe API endpoint /v1/forecast accepts a geographical coordinate, a list of weather variables and responds with a JSON hourly weather forecast for 7 days. Time always starts at 0:00 today and contains 168 hours. All URL parameters are listed below:\n\nParameter   Format  Required    Default Description\nlatitude, longitude Floating point  Yes     Geographical WGS84 coordinate of the location\nhourly  String array    No      A list of weather variables which should be returned. Values can be comma separated, or multiple &hourly= parameter in the URL can be used.\ndaily   String array    No      A list of daily weather variable aggregations which should be returned. Values can be comma separated, or multiple &daily= parameter in the URL can be used. If daily weather variables are specified, parameter timezone is required.\ncurrent_weather Bool    No  false   Include current weather conditions in the JSON output.\ntemperature_unit    String  No  celsius If fahrenheit is set, al\n\n\nBut what is the chain doing with the Open-Meteo docs? If we dig around in the source code we can find a few lines of code that get into the details:\nget_request_chain = LLMChain(llm=llm, prompt=api_url_prompt)\nrequests_wrapper = RequestsWrapper(headers=headers)\nget_answer_chain = LLMChain(llm=llm, prompt=api_response_prompt)\nThese are used in a three-step process:\n\nGet the API URL\nUse the API URL to get the data\nProcess the data into an answer to the question\n\nThe first step builds an LLMChain to talk to GPT. LangChain then provides the API documentation to GPT, and asks it to determine the API endpoint to visit.\n\"\"\"You are given the below API Documentation:\n\n    {api_docs}\n\nUsing this documentation, generate the full API url to call for answering the user question.\nYou should build the API url in order to get a response that is as short as possible, while still getting the necessary information to answer the question. Pay attention to deliberately exclude any unnecessary pieces of data in the API call.\n\nQuestion:{question}\nAPI url:\"\"\"\nThe second step builds a RequestsWrapper to access the API URL and returns the response. But it isn’t a human-readable response to our question yet, it’s almost always going to be a bunch of JSON.\nThe final step uses another LLMChain to talk to GPT again: LangChain sends the API response to GPT and asks for a human-readable summary to answer the question.\n\"\"\"Here is the response from the API:\n\n{api_response}\n\nSummarize this response to answer the original question.\nSummary:\"\"\"\n\nWhy this doesn’t work for our Spotify use case\nEven though we want to talk to an API, we want to talk to an API through the Spotipy Python library, not a series of URLs. Since the APIChain is based around making an actual request to somewhere on the internet, this isn’t going to work for us.\nIf the Spotify API were a nice simple REST API we could just feed APIChain the documentation, but that isn’t the case."
  },
  {
    "objectID": "spotify-langchain-chatgpt.html#how-the-palchain-works",
    "href": "spotify-langchain-chatgpt.html#how-the-palchain-works",
    "title": "Building Spotify playlists based on vibes using LangChain and GPT",
    "section": "How the PALChain works",
    "text": "How the PALChain works\nA PALChain can be used to create and run arbitrary Python code! This is the PALChain example from the docs.\n\nfrom langchain.chains import PALChain\n\npal_chain = PALChain.from_math_prompt(llm, verbose=True)\nquestion = \"Jan has three times the number of pets as Marcia. Marcia has two more pets than Cindy. If Cindy has four pets, how many total pets do the three have?\"\npal_chain.run(question)\n\n\n\n&gt; Entering new PALChain chain...\ndef solution():\n    \"\"\"Jan has three times the number of pets as Marcia. Marcia has two more pets than Cindy. If Cindy has four pets, how many total pets do the three have?\"\"\"\n    cindy_pets = 4\n    marcia_pets = cindy_pets + 2\n    jan_pets = marcia_pets * 3\n    total_pets = cindy_pets + marcia_pets + jan_pets\n    result = total_pets\n    return result\n\n&gt; Finished chain.\n\n\n'28'\n\n\nIf we look at the code for the math chain’s prompt it’s very long. Here’s a portion of it:\nfrom langchain.prompts.prompt import PromptTemplate\n\ntemplate = (\n    '''\nQ: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n\n# solution in Python:\n\n\ndef solution():\n    \"\"\"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\"\"\"\n    money_initial = 23\n    bagels = 5\n    bagel_cost = 3\n    money_spent = bagels * bagel_cost\n    money_left = money_initial - money_spent\n    result = money_left\n    return result\n\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n\n# solution in Python:\n\n\ndef solution():\n    \"\"\"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\"\"\"\n    trees_initial = 15\n    trees_after = 21\n    trees_added = trees_after - trees_initial\n    result = trees_added\n    return result\n\n\n\n\n\nQ: {question}\n\n# solution in Python:\n'''.strip()\n    + \"\\n\\n\\n\"\n)\nMATH_PROMPT = PromptTemplate(input_variables=[\"question\"], template=template)\nThat prompt only gives you Python code, though, not the actual result! To see what happens with the result we need to check the PALChain code itself, lightly edited for clarity:\ndef _call(self, inputs: Dict[str, str]) -&gt; Dict[str, str]:\n    llm_chain = LLMChain(llm=self.llm, prompt=self.prompt)\n    code = llm_chain.predict(stop=[self.stop], **inputs)\n    repl = PythonREPL(_globals=self.python_globals, _locals=self.python_locals)\n    res = repl.run(code + f\"\\n{self.get_answer_expr}\")\n    output = {self.output_key: res.strip()}\n    return output\nThe code looks a little wild, but the process is pretty simple:\n\nUse the LLM and the prompt to generate some code\nCreate a Python REPL to run the generated code, sending along some global and local variables\nRun the code contained in self.get_answer_expr to get the result of the solution.\n\nIn this case, the get_answer_expr is print(solution()). Our prompt insists the answer be put in a function called solution, so this is how we call the function and obtain the result.\n\nWhy this doesn’t work for our Spotify use case\nWith a little work, we actually can make it work! It just needs a little extra effort and inspiration from APIChain."
  },
  {
    "objectID": "spotify-langchain-chatgpt.html#the-spotipy-code-prompt",
    "href": "spotify-langchain-chatgpt.html#the-spotipy-code-prompt",
    "title": "Building Spotify playlists based on vibes using LangChain and GPT",
    "section": "The Spotipy code prompt",
    "text": "The Spotipy code prompt\nHow do we get GPT to write Spotipy code for us? It’s similar to the API example – giving the documentation to GPT along with our question – but in this case we wouldn’t use Spotify’s API documentation, we’d use documentation for the Spotipy library.\nWhile we could try feeding GPT the entire documentation page for Spotipy, it’s too long to do it all at once. We could chunk it and feed it into a reference database that is selectively queried for relevant content… but that’s just too much work. We want something simple.\nInstead, we’re going to go the lazy route: the Spotify library has been around for ages and GPT already knows how it works, so we’ll just rely on its in-built knowledge. We just need to provide a few examples of how we like to work with the library and what we need returned, and GPT will follow our lead.\nHere’s our prompt for generating Spotipy code to access the Spotify API:\n\nfrom langchain.prompts.prompt import PromptTemplate\n\n\nSPOTIPY_PROMPT_TEMPLATE = (\n    '''\nAPI LIMITATIONS TO NOTE\n* When requesting track information, the limit is 50 at a time\n* When requesting audio features, the limit is 100 at a time\n* When selecting multiple artists, the limit is 50 at a time\n* When asking for recommendations, the limit is 100 at a time\n=====\n\nQ: What albums has the band Green Day made?\n\n# solution in Python:\n\n\ndef solution():\n    \"\"\"What albums has the band Green Day made?\"\"\"\n    search_results = sp.search(q='Green Day', type='artist')\n    uri = search_results['artists']['items'][0]['uri']\n    albums = sp.artist_albums(green_day_uri, album_type='album')\n    return albums\n\n\n\n\nQ: Who are some musicians similar to Fiona Apple?\n\n# solution in Python:\n\n\ndef solution():\n    \"\"\"Who are some musicians similar to Fiona Apple?\"\"\"\n    search_results = sp.search(q='Fiona Apple', type='artist')\n    uri = search_results['artists']['items'][0].get('uri')\n    artists = sp.artist_related_artists(uri)\n    return artists\n\n\n\nQ: Tell me what songs by The Promise Ring sound like\n\n# solution in Python:\n\n\ndef solution():\n    \"\"\"Tell me what songs by The Promise Ring sound like?\"\"\"\n    search_results = sp.search(q='The Promise Ring', type='artist')\n    uri = search_results['artists']['items'][0].get('uri')\n    tracks = sp.artist_top_tracks(uri)\n    track_uris = [track.get('uri') for track in tracks['tracks']]\n    audio_details = sp.audio_features(track_uris)\n    return audio_details\n\n\n\nQ: Get me the URI for the album The Colour And The Shape\n\n# solution in Python:\n\n\ndef solution():\n    \"\"\"Get me the URI for the album The Colour And The Shape\"\"\"\n    search_results = sp.search(q='The Colour And The Shape', type='album')\n    uri = search_results['albums']['items'][0].get('uri')\n    return uri\n\n\n\nQ: What are the first three songs on Diet Cig's Over Easy?\n\n# solution in Python:\n\n\ndef solution():\n    \"\"\"What are the first three songs on Diet Cig's Over Easy?\"\"\"\n    # Get the URI for the album\n    search_results = sp.search(q='Diet Cig Over Easy', type='album')\n    album = search_results['albums']['items'][0]\n    album_uri = album['uri']\n    # Get the album tracks\n    album_tracks = sp.album_tracks(album_uri)['items']\n    # Sort the tracks by duration\n    first_three = album_tracks[:3]\n    tracks = []\n    # Only include relevant fields\n    for i, track in enumerate(first_three):\n        # track['album'] does NOT work with sp.album_tracks\n        # you need to use album['name'] instead\n        tracks.append({{\n            'position': i+1,\n            'song_name': track.get('name'),\n            'song_uri': track['artists'][0].get('uri'),\n            'artist_uri': track['artists'][0].get('uri'),\n            'album_uri': album.get('uri'),\n            'album_name': album.get('name')\n        }})\n    return tracks\n\n\nQ: What are the thirty most danceable songs by Metallica?\n\n# solution in Python:\n\n\ndef solution():\n    \"\"\"What are most danceable songs by Metallica?\"\"\"\n    search_results = sp.search(q='Metallica', type='artist')\n    uri = search_results['artists']['items'][0]['uri']\n    albums = sp.artist_albums(uri, album_type='album')\n    album_uris = [album['uri'] for album in albums['items']]\n    tracks = []\n    for album_uri in album_uris:\n        album_tracks = sp.album_tracks(album_uri)\n        tracks.extend(album_tracks['items'])\n    track_uris = [track['uri'] for track in tracks]\n    danceable_tracks = []\n    # You can only have 100 at a time\n    for i in range(0, len(track_uris), 100):\n        subset_track_uris = track_uris[i:i+100]\n        audio_details = sp.audio_features(subset_track_uris)\n        for j, details in enumerate(audio_details):\n            if details['danceability'] &gt; 0.7:\n                track = tracks[i+j]\n                danceable_tracks.append({{\n                    'song': track.get('name')\n                    'album': track.get('album').get('name')\n                    'danceability': details.get('danceability'),\n                    'tempo': details.get('tempo'),\n                }})\n                # Be sure to add the audio details to the track\n                danceable_tracks.append(track)\n    return danceable_tracks\n\n\n\nQ: {question}. Return a list or dictionary, only including the fields necessary to answer the question, including relevant scores and the uris to the albums/songs/artists mentioned. Only return the data – if the prompt asks for a format such as markdown or a simple string, ignore it: you are only meant to provide the information, not the formatting. A later step in the process will convert the data into the new format (table, sentence, etc).\n\n# solution in Python:\n'''.strip()\n    + \"\\n\\n\\n\"\n)\n\nSPOTIPY_PROMPT = PromptTemplate(input_variables=[\"question\"], template=SPOTIPY_PROMPT_TEMPLATE)"
  },
  {
    "objectID": "spotify-langchain-chatgpt.html#palchain-for-data-access",
    "href": "spotify-langchain-chatgpt.html#palchain-for-data-access",
    "title": "Building Spotify playlists based on vibes using LangChain and GPT",
    "section": "PALChain for data access",
    "text": "PALChain for data access\nThe PALChain example from the docs makes it look so simple:\npal_chain = PALChain.from_math_prompt(llm, verbose=True)\nquestion = \"Jan has three times the number of pets as Marcia. Marcia has two more pets than Cindy. If Cindy has four pets, how many total pets do the three have?\"\npal_chain.run(question)\nBut a lot of work is happening behind the scenes! In our case, we’re going to be building a PALChain from scratch instead of relying on a constructor.\nThere are a couple important additions we make as we initialize the PALChain. First, we need to provide our initialized and authenticated Spotipy sp instance so the PythonREPL can access Spotipy. We’ll do this using python_globals=.\npython_globals={\n    'sp': sp\n},\nThe next step is wrangling our data. Results from chains come as strings, but the Spotify API returns JSON (or more specifically, a Python dictionary). To nicely convert our dictionary into a string we’ll be using json.dumps. The json module isn’t included by default, so this requires importing hte json library before we do the conversion.\nBoth of these steps are squished into the get_answer_expr parameter. It’s a bit garish but it works!\nget_answer_expr=\"import json; print(json.dumps(solution()))\",\nFinally, we’re also adding return_intermediate_steps=True to make sure it returns the result of the code running and the code it ran.\nThis is what it looks like all put together:\n\nfrom langchain.chains import PALChain\n\nspotify_chain = PALChain(\n    llm=llm,\n    prompt=SPOTIPY_PROMPT,\n    python_globals={\n        'sp': sp\n    },\n    stop='\\n\\n\\n',\n    verbose=True,\n    return_intermediate_steps=True,\n    get_answer_expr=\"import json; print(json.dumps(solution()))\",\n)\n\nIt’s complicated enough, but does it work?\n\nspotify_response = spotify_chain({'question': \"What are the most popular Bouncing Souls songs?\"})\nspotify_response['result']\n\n\n\n&gt; Entering new PALChain chain...\ndef solution():\n    \"\"\"What are the most popular Bouncing Souls songs?\"\"\"\n    search_results = sp.search(q='Bouncing Souls', type='artist')\n    uri = search_results['artists']['items'][0].get('uri')\n    top_tracks = sp.artist_top_tracks(uri)\n    top_track_uris = [track.get('uri') for track in top_tracks['tracks']]\n    audio_details = sp.audio_features(top_track_uris)\n    popular_songs = []\n    for i, track in enumerate(top_tracks['tracks']):\n        details = audio_details[i]\n        popular_songs.append({\n            'song_name': track.get('name'),\n            'song_uri': track.get('uri'),\n            'artist_name': track.get('artists')[0].get('name'),\n            'artist_uri': track.get('artists')[0].get('uri'),\n            'album_name': track.get('album').get('name'),\n            'album_uri': track.get('album').get('uri'),\n            'popularity': track.get('popularity'),\n            'danceability': details.get('danceability'),\n            'energy': details.get('energy'),\n            'key': details.get('key'),\n            'loudness': details.get('loudness'),\n            'mode': details.get('mode'),\n            'speechiness': details.get('speechiness'),\n            'acousticness': details.get('acousticness'),\n            'instrumentalness': details.get('instrumentalness'),\n            'liveness': details.get('liveness'),\n            'valence': details.get('valence'),\n            'tempo': details.get('tempo'),\n        })\n    return popular_songs[:10] # Return top 10 songs\n\n&gt; Finished chain.\n\n\n'[{\"song_name\": \"True Believers\", \"song_uri\": \"spotify:track:4fRmFVMd0c1SGfzazBJIM8\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"How I Spent My Summer Vacation\", \"album_uri\": \"spotify:album:64zbLX1ze8N3kcAMX0qq7G\", \"popularity\": 55, \"danceability\": 0.237, \"energy\": 0.981, \"key\": 0, \"loudness\": -4.32, \"mode\": 1, \"speechiness\": 0.0989, \"acousticness\": 0.000296, \"instrumentalness\": 3.81e-05, \"liveness\": 0.202, \"valence\": 0.475, \"tempo\": 98.181}, {\"song_name\": \"Lean On Sheena\", \"song_uri\": \"spotify:track:7IR7GUO0dUyUsBp7BfQ3vJ\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"The Gold Record\", \"album_uri\": \"spotify:album:3MF7PvmrMjEXGvA8fP3L6l\", \"popularity\": 51, \"danceability\": 0.491, \"energy\": 0.866, \"key\": 11, \"loudness\": -4.431, \"mode\": 1, \"speechiness\": 0.0583, \"acousticness\": 0.16, \"instrumentalness\": 0.000211, \"liveness\": 0.13, \"valence\": 0.694, \"tempo\": 175.969}, {\"song_name\": \"Hopeless Romantic\", \"song_uri\": \"spotify:track:180mXjN61yhrKhbY2yQc0E\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"Hopeless Romantic\", \"album_uri\": \"spotify:album:56CbFyDsG65LI1Eoh7hsOT\", \"popularity\": 49, \"danceability\": 0.243, \"energy\": 0.981, \"key\": 4, \"loudness\": -5.251, \"mode\": 1, \"speechiness\": 0.074, \"acousticness\": 0.000164, \"instrumentalness\": 1.11e-05, \"liveness\": 0.207, \"valence\": 0.216, \"tempo\": 105.022}, {\"song_name\": \"Manthem\", \"song_uri\": \"spotify:track:5pSjxUAwOol5e0TWp1ecHC\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"How I Spent My Summer Vacation\", \"album_uri\": \"spotify:album:64zbLX1ze8N3kcAMX0qq7G\", \"popularity\": 46, \"danceability\": 0.524, \"energy\": 0.986, \"key\": 2, \"loudness\": -2.865, \"mode\": 1, \"speechiness\": 0.0634, \"acousticness\": 9.44e-05, \"instrumentalness\": 0.000214, \"liveness\": 0.0772, \"valence\": 0.724, \"tempo\": 94.348}, {\"song_name\": \"Sing Along Forever\", \"song_uri\": \"spotify:track:5feYKXxg4HL2APTQGCfAav\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"Anchors Aweigh\", \"album_uri\": \"spotify:album:1xgfRXjCoynPLqtdNu50pR\", \"popularity\": 46, \"danceability\": 0.592, \"energy\": 0.964, \"key\": 0, \"loudness\": -3.672, \"mode\": 1, \"speechiness\": 0.0817, \"acousticness\": 0.00912, \"instrumentalness\": 0, \"liveness\": 0.27, \"valence\": 0.585, \"tempo\": 101.252}, {\"song_name\": \"Say Anything\", \"song_uri\": \"spotify:track:06peZfvxR5721oGqHwogha\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"The Bouncing Souls\", \"album_uri\": \"spotify:album:7LgICzKkhaLV9Gttn8xM7a\", \"popularity\": 45, \"danceability\": 0.448, \"energy\": 0.995, \"key\": 6, \"loudness\": -3.111, \"mode\": 1, \"speechiness\": 0.0539, \"acousticness\": 0.00275, \"instrumentalness\": 0, \"liveness\": 0.297, \"valence\": 0.643, \"tempo\": 101.405}, {\"song_name\": \"Ole\", \"song_uri\": \"spotify:track:2McQQA5nCLVL0XvzcxWhFC\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"Hopeless Romantic\", \"album_uri\": \"spotify:album:56CbFyDsG65LI1Eoh7hsOT\", \"popularity\": 43, \"danceability\": 0.33, \"energy\": 0.833, \"key\": 7, \"loudness\": -6.507, \"mode\": 1, \"speechiness\": 0.0768, \"acousticness\": 0.0491, \"instrumentalness\": 0, \"liveness\": 0.687, \"valence\": 0.553, \"tempo\": 128.329}, {\"song_name\": \"Ten Stories High\", \"song_uri\": \"spotify:track:1t9Y1HGwikUCCo5xCupAnT\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"Ten Stories High\", \"album_uri\": \"spotify:album:0wdbr46ndnwB1cgZoNzT48\", \"popularity\": 30, \"danceability\": 0.361, \"energy\": 0.984, \"key\": 5, \"loudness\": -1.913, \"mode\": 1, \"speechiness\": 0.0883, \"acousticness\": 0.000322, \"instrumentalness\": 0.000873, \"liveness\": 0.329, \"valence\": 0.491, \"tempo\": 198.064}, {\"song_name\": \"Kids and Heroes\", \"song_uri\": \"spotify:track:7ru4QA7k7ViuLS9oDtdRBI\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"Anchors Aweigh\", \"album_uri\": \"spotify:album:1xgfRXjCoynPLqtdNu50pR\", \"popularity\": 43, \"danceability\": 0.405, \"energy\": 0.963, \"key\": 0, \"loudness\": -5.216, \"mode\": 1, \"speechiness\": 0.0697, \"acousticness\": 0.0127, \"instrumentalness\": 0.000256, \"liveness\": 0.289, \"valence\": 0.198, \"tempo\": 101.759}, {\"song_name\": \"Kate Is Great\", \"song_uri\": \"spotify:track:1VT2wLreLu0l7E4T0JDedh\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"The Bouncing Souls\", \"album_uri\": \"spotify:album:7LgICzKkhaLV9Gttn8xM7a\", \"popularity\": 42, \"danceability\": 0.358, \"energy\": 0.93, \"key\": 2, \"loudness\": -4.726, \"mode\": 1, \"speechiness\": 0.164, \"acousticness\": 0.0822, \"instrumentalness\": 0, \"liveness\": 0.0699, \"valence\": 0.809, \"tempo\": 175.011}]'\n\n\nLet’s look at the three separate keys the PALChain response gives us.\nFirst, the question:\n\nspotify_response['question']\n\n'What are the most popular Bouncing Souls songs?'\n\n\nSecond, the intermediate steps (the code that it ran):\n\nprint(spotify_response['intermediate_steps'])\n\ndef solution():\n    \"\"\"What are the most popular Bouncing Souls songs?\"\"\"\n    search_results = sp.search(q='Bouncing Souls', type='artist')\n    uri = search_results['artists']['items'][0].get('uri')\n    top_tracks = sp.artist_top_tracks(uri)\n    top_track_uris = [track.get('uri') for track in top_tracks['tracks']]\n    audio_details = sp.audio_features(top_track_uris)\n    popular_songs = []\n    for i, track in enumerate(top_tracks['tracks']):\n        details = audio_details[i]\n        popular_songs.append({\n            'song_name': track.get('name'),\n            'song_uri': track.get('uri'),\n            'artist_name': track.get('artists')[0].get('name'),\n            'artist_uri': track.get('artists')[0].get('uri'),\n            'album_name': track.get('album').get('name'),\n            'album_uri': track.get('album').get('uri'),\n            'popularity': track.get('popularity'),\n            'danceability': details.get('danceability'),\n            'energy': details.get('energy'),\n            'key': details.get('key'),\n            'loudness': details.get('loudness'),\n            'mode': details.get('mode'),\n            'speechiness': details.get('speechiness'),\n            'acousticness': details.get('acousticness'),\n            'instrumentalness': details.get('instrumentalness'),\n            'liveness': details.get('liveness'),\n            'valence': details.get('valence'),\n            'tempo': details.get('tempo'),\n        })\n    return popular_songs[:10] # Return top 10 songs\n\n\nFinally, the actual response. In the PALChain examples it’s mostly the result of a quick calculation, but this time it’s a whole big mess of JSON:\n\nspotify_response['result']\n\n'[{\"song_name\": \"True Believers\", \"song_uri\": \"spotify:track:4fRmFVMd0c1SGfzazBJIM8\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"How I Spent My Summer Vacation\", \"album_uri\": \"spotify:album:64zbLX1ze8N3kcAMX0qq7G\", \"popularity\": 55, \"danceability\": 0.237, \"energy\": 0.981, \"key\": 0, \"loudness\": -4.32, \"mode\": 1, \"speechiness\": 0.0989, \"acousticness\": 0.000296, \"instrumentalness\": 3.81e-05, \"liveness\": 0.202, \"valence\": 0.475, \"tempo\": 98.181}, {\"song_name\": \"Lean On Sheena\", \"song_uri\": \"spotify:track:7IR7GUO0dUyUsBp7BfQ3vJ\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"The Gold Record\", \"album_uri\": \"spotify:album:3MF7PvmrMjEXGvA8fP3L6l\", \"popularity\": 51, \"danceability\": 0.491, \"energy\": 0.866, \"key\": 11, \"loudness\": -4.431, \"mode\": 1, \"speechiness\": 0.0583, \"acousticness\": 0.16, \"instrumentalness\": 0.000211, \"liveness\": 0.13, \"valence\": 0.694, \"tempo\": 175.969}, {\"song_name\": \"Hopeless Romantic\", \"song_uri\": \"spotify:track:180mXjN61yhrKhbY2yQc0E\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"Hopeless Romantic\", \"album_uri\": \"spotify:album:56CbFyDsG65LI1Eoh7hsOT\", \"popularity\": 49, \"danceability\": 0.243, \"energy\": 0.981, \"key\": 4, \"loudness\": -5.251, \"mode\": 1, \"speechiness\": 0.074, \"acousticness\": 0.000164, \"instrumentalness\": 1.11e-05, \"liveness\": 0.207, \"valence\": 0.216, \"tempo\": 105.022}, {\"song_name\": \"Manthem\", \"song_uri\": \"spotify:track:5pSjxUAwOol5e0TWp1ecHC\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"How I Spent My Summer Vacation\", \"album_uri\": \"spotify:album:64zbLX1ze8N3kcAMX0qq7G\", \"popularity\": 46, \"danceability\": 0.524, \"energy\": 0.986, \"key\": 2, \"loudness\": -2.865, \"mode\": 1, \"speechiness\": 0.0634, \"acousticness\": 9.44e-05, \"instrumentalness\": 0.000214, \"liveness\": 0.0772, \"valence\": 0.724, \"tempo\": 94.348}, {\"song_name\": \"Sing Along Forever\", \"song_uri\": \"spotify:track:5feYKXxg4HL2APTQGCfAav\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"Anchors Aweigh\", \"album_uri\": \"spotify:album:1xgfRXjCoynPLqtdNu50pR\", \"popularity\": 46, \"danceability\": 0.592, \"energy\": 0.964, \"key\": 0, \"loudness\": -3.672, \"mode\": 1, \"speechiness\": 0.0817, \"acousticness\": 0.00912, \"instrumentalness\": 0, \"liveness\": 0.27, \"valence\": 0.585, \"tempo\": 101.252}, {\"song_name\": \"Say Anything\", \"song_uri\": \"spotify:track:06peZfvxR5721oGqHwogha\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"The Bouncing Souls\", \"album_uri\": \"spotify:album:7LgICzKkhaLV9Gttn8xM7a\", \"popularity\": 45, \"danceability\": 0.448, \"energy\": 0.995, \"key\": 6, \"loudness\": -3.111, \"mode\": 1, \"speechiness\": 0.0539, \"acousticness\": 0.00275, \"instrumentalness\": 0, \"liveness\": 0.297, \"valence\": 0.643, \"tempo\": 101.405}, {\"song_name\": \"Ole\", \"song_uri\": \"spotify:track:2McQQA5nCLVL0XvzcxWhFC\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"Hopeless Romantic\", \"album_uri\": \"spotify:album:56CbFyDsG65LI1Eoh7hsOT\", \"popularity\": 43, \"danceability\": 0.33, \"energy\": 0.833, \"key\": 7, \"loudness\": -6.507, \"mode\": 1, \"speechiness\": 0.0768, \"acousticness\": 0.0491, \"instrumentalness\": 0, \"liveness\": 0.687, \"valence\": 0.553, \"tempo\": 128.329}, {\"song_name\": \"Ten Stories High\", \"song_uri\": \"spotify:track:1t9Y1HGwikUCCo5xCupAnT\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"Ten Stories High\", \"album_uri\": \"spotify:album:0wdbr46ndnwB1cgZoNzT48\", \"popularity\": 30, \"danceability\": 0.361, \"energy\": 0.984, \"key\": 5, \"loudness\": -1.913, \"mode\": 1, \"speechiness\": 0.0883, \"acousticness\": 0.000322, \"instrumentalness\": 0.000873, \"liveness\": 0.329, \"valence\": 0.491, \"tempo\": 198.064}, {\"song_name\": \"Kids and Heroes\", \"song_uri\": \"spotify:track:7ru4QA7k7ViuLS9oDtdRBI\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"Anchors Aweigh\", \"album_uri\": \"spotify:album:1xgfRXjCoynPLqtdNu50pR\", \"popularity\": 43, \"danceability\": 0.405, \"energy\": 0.963, \"key\": 0, \"loudness\": -5.216, \"mode\": 1, \"speechiness\": 0.0697, \"acousticness\": 0.0127, \"instrumentalness\": 0.000256, \"liveness\": 0.289, \"valence\": 0.198, \"tempo\": 101.759}, {\"song_name\": \"Kate Is Great\", \"song_uri\": \"spotify:track:1VT2wLreLu0l7E4T0JDedh\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"The Bouncing Souls\", \"album_uri\": \"spotify:album:7LgICzKkhaLV9Gttn8xM7a\", \"popularity\": 42, \"danceability\": 0.358, \"energy\": 0.93, \"key\": 2, \"loudness\": -4.726, \"mode\": 1, \"speechiness\": 0.164, \"acousticness\": 0.0822, \"instrumentalness\": 0, \"liveness\": 0.0699, \"valence\": 0.809, \"tempo\": 175.011}]'\n\n\nLooks great! These don’t answer our question, they only provides the data, so we’ll need one more step."
  },
  {
    "objectID": "spotify-langchain-chatgpt.html#llmchain-for-cleanup",
    "href": "spotify-langchain-chatgpt.html#llmchain-for-cleanup",
    "title": "Building Spotify playlists based on vibes using LangChain and GPT",
    "section": "LLMChain for cleanup",
    "text": "LLMChain for cleanup\nThis is similar to what happens in the APIChain: we have an API response, but we want something a little more human. We’ll use an LLMChain to send the JSON to GPT along with our question, then get back a readable response.\n\nRESPONSE_CLEANUP_PROMPT_TEMPLATE = (\"\"\" \nUsing this code:\n\n```python\n{intermediate_steps}\n```\n\nWe got the following output from the Spotify API:\n\n```json\n{result}\n```\n\nUsing the output above as your data source, answer the question {question}. Don't describe the code or process, just answer the question.\nAnswer:\"\"\"\n)\n\nRESPONSE_CLEANUP_PROMPT = PromptTemplate(\n    input_variables=[\"question\", \"intermediate_steps\", \"result\"],\n    template=RESPONSE_CLEANUP_PROMPT_TEMPLATE,\n)\n\nIn the prompt above, we’re providing three things to the prompt:\n\nThe original question we want an answer to\nThe intermediate steps, which is the actual Python code the PALChain created\nThe result, the output of the Python code from the PALChain (aka the JSON)\n\nWe can now use this prompt with an LLMChain to turn the JSON into an actual answer.\n\nexplainer_chain = LLMChain(\n    llm=llm,\n    prompt=RESPONSE_CLEANUP_PROMPT,\n    verbose=True,\n    output_key='answer'\n)\n\nNow that we’ve built the structure of the explainer, let’s feed it the previous Spotify response and see what happens.\n\nexplainer_response = explainer_chain(spotify_response)\n\n\n\n&gt; Entering new LLMChain chain...\nPrompt after formatting:\n \nUsing this code:\n\n```python\ndef solution():\n    \"\"\"What are the most popular Bouncing Souls songs?\"\"\"\n    search_results = sp.search(q='Bouncing Souls', type='artist')\n    uri = search_results['artists']['items'][0]['uri']\n    top_tracks = sp.artist_top_tracks(uri)\n    tracks = []\n    for i, track in enumerate(top_tracks['tracks']):\n        # Only include relevant fields\n        tracks.append({\n            'position': i+1,\n            'song_name': track.get('name'),\n            'song_uri': track.get('uri'),\n            'artist_uri': uri,\n            'album_uri': track.get('album').get('uri'),\n            'album_name': track.get('album').get('name'),\n            'popularity': track.get('popularity')\n        })\n    return tracks\n```\n\nWe got the following output from the Spotify API:\n\n```json\n[{\"position\": 1, \"song_name\": \"True Believers\", \"song_uri\": \"spotify:track:4fRmFVMd0c1SGfzazBJIM8\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_uri\": \"spotify:album:64zbLX1ze8N3kcAMX0qq7G\", \"album_name\": \"How I Spent My Summer Vacation\", \"popularity\": 54}, {\"position\": 2, \"song_name\": \"Lean On Sheena\", \"song_uri\": \"spotify:track:7IR7GUO0dUyUsBp7BfQ3vJ\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_uri\": \"spotify:album:3MF7PvmrMjEXGvA8fP3L6l\", \"album_name\": \"The Gold Record\", \"popularity\": 51}, {\"position\": 3, \"song_name\": \"Hopeless Romantic\", \"song_uri\": \"spotify:track:180mXjN61yhrKhbY2yQc0E\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_uri\": \"spotify:album:56CbFyDsG65LI1Eoh7hsOT\", \"album_name\": \"Hopeless Romantic\", \"popularity\": 49}, {\"position\": 4, \"song_name\": \"Manthem\", \"song_uri\": \"spotify:track:5pSjxUAwOol5e0TWp1ecHC\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_uri\": \"spotify:album:64zbLX1ze8N3kcAMX0qq7G\", \"album_name\": \"How I Spent My Summer Vacation\", \"popularity\": 46}, {\"position\": 5, \"song_name\": \"Sing Along Forever\", \"song_uri\": \"spotify:track:5feYKXxg4HL2APTQGCfAav\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_uri\": \"spotify:album:1xgfRXjCoynPLqtdNu50pR\", \"album_name\": \"Anchors Aweigh\", \"popularity\": 46}, {\"position\": 6, \"song_name\": \"Say Anything\", \"song_uri\": \"spotify:track:06peZfvxR5721oGqHwogha\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_uri\": \"spotify:album:7LgICzKkhaLV9Gttn8xM7a\", \"album_name\": \"The Bouncing Souls\", \"popularity\": 45}, {\"position\": 7, \"song_name\": \"Ole\", \"song_uri\": \"spotify:track:2McQQA5nCLVL0XvzcxWhFC\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_uri\": \"spotify:album:56CbFyDsG65LI1Eoh7hsOT\", \"album_name\": \"Hopeless Romantic\", \"popularity\": 43}, {\"position\": 8, \"song_name\": \"Kids and Heroes\", \"song_uri\": \"spotify:track:7ru4QA7k7ViuLS9oDtdRBI\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_uri\": \"spotify:album:1xgfRXjCoynPLqtdNu50pR\", \"album_name\": \"Anchors Aweigh\", \"popularity\": 42}, {\"position\": 9, \"song_name\": \"Kate Is Great\", \"song_uri\": \"spotify:track:1VT2wLreLu0l7E4T0JDedh\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_uri\": \"spotify:album:7LgICzKkhaLV9Gttn8xM7a\", \"album_name\": \"The Bouncing Souls\", \"popularity\": 41}, {\"position\": 10, \"song_name\": \"Ten Stories High\", \"song_uri\": \"spotify:track:0Wz9RJySVFtUTFQk8sjRBv\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_uri\": \"spotify:album:5xEwUAv3WJiDtHScEPliQl\", \"album_name\": \"Ten Stories High\", \"popularity\": 40}]\n```\n\nUsing the output above as your data source, answer the question What are the most popular Bouncing Souls songs?. Don't describe the code or process, just answer the question.\nAnswer:\n\n&gt; Finished chain.\n\n\n\nprint(explainer_response['answer'])\n\nThe most popular Bouncing Souls songs, based on the provided data, are:\n\n1. True Believers\n2. Lean On Sheena\n3. Hopeless Romantic\n4. Manthem\n5. Sing Along Forever\n6. Say Anything\n7. Ole\n8. Kids and Heroes\n9. Kate Is Great\n10. Ten Stories High\n\n\nI completely disagree with everyone’s taste in music, but it’s a perfect response!"
  },
  {
    "objectID": "drafts/multi-prong-spotify.html",
    "href": "drafts/multi-prong-spotify.html",
    "title": "Pitchfork",
    "section": "",
    "text": "%load_ext dotenv\n%dotenv\n\nThe dotenv extension is already loaded. To reload it, use:\n  %reload_ext dotenv\n\n\n\nfrom langchain.agents import initialize_agent, Tool\nfrom langchain.tools import BaseTool\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.agents import tool\nimport requests\nfrom bs4 import BeautifulSoup\nimport json\n\n\nllm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n\n\nimport yaml\nimport urllib\n\ndef review_to_json(review):\n    album = review['tombstone']['albums'][0]\n    return {\n        'dek': review['seoDescription'],\n        'pubDate': review['pubDate'],\n        'authors': ', '.join([a['name'] for a in review['authors']]),\n        'title': review['seoTitle'],\n        'artist': ', '.join([a['display_name'] for a in review['artists']]),\n        'genres': ', '.join([g['display_name'] for a in review['artists'] for g in a['genres']]),\n        'name': album['album']['display_name'],\n        'score': float(album['rating']['rating'])\n    }\n\n@tool\ndef pitchfork_review_search_querystring(query: str) -&gt; str:\n    \"\"\"\n    Get Pitchfork album reviews by album name or artist name.\n    You must provide this tool with the name of an album, artist, or a combination of both\n    \"\"\"\n    \n    encoded_query = urllib.parse.quote(query)\n    response = requests.get(\"https://pitchfork.com/api/v2/search/faceted/?query=\" + encoded_query)\n    data = response.json()\n            \n    reviews = [review_to_json(review) for review in data['results']['albumreviews']['items']]\n\n    return yaml.dump({'albums': reviews}, default_flow_style=False)\n\n@tool\ndef pitchfork_review_search_category(query: str) -&gt; str:\n    \"\"\"\n    Get a list of Pitchfork album reviews using categories and/or album review scores\n    You cannot use this tool to filter by artist or album\n    To use this tool you must provide a complete URL/endpoint as the parameter.\n\n    The base of the endpoint is https://pitchfork.com/api/v2/search/\n    An example URL would be\n    https://pitchfork.com/api/v2/search/?genre=experimental&genre=global&genre=jazz&genre=metal&genre=pop&genre=rap&genre=rock&types=reviews&sort=publishdate%20desc%2Cposition%20asc&size=500&start=0&rating_from=0.0\n    valid genres are [electronic, experimental, folk, global, jazz, metal, pop, rap, rock]\n    rating_from is the minimum score for an album to appear in the results\n    An album is scored from 0.0-10.0, with 10 being the highest\n    You cannot change the sort order\n    \"\"\"\n    \n    response = requests.get(query)\n    data = response.json()\n        \n    reviews = [review_to_json(review) for review in data['results']['list']]\n\n    return yaml.dump({'albums': reviews}, default_flow_style=False)\n\n\nKEXP\n\nfrom langchain.chains import APIChain\n\n\nfrom langchain.chains.api import open_meteo_docs\n#print(open_meteo_docs.OPEN_METEO_DOCS)\nKEXP_DOCS = \"\"\"\n20 songs played at a certain time:\nExample URL: https://api.kexp.org/v2/plays/?limit=20&airdate_before=2023-03-29T14:00:00Z&ordering=-airdate&offset=0\n\"\"\"\n\n\n\nMake it so you can send a sample of the API results to the LLM\n\nagent = APIChain.from_llm_and_api_docs(llm, KEXP_DOCS, verbose=True)\n\n\nagent.run('What are the first ten songs KEXP played on the first of 2023?')\n\n\n\n&gt; Entering new APIChain chain...\nhttps://api.kexp.org/v2/plays/?limit=10&airdate_after=2023-01-01T00:00:00Z&airdate_before=2023-01-01T23:59:59Z&ordering=airdate&offset=0\n{\"next\":\"https://api.kexp.org/v2/plays/?airdate_after=2023-01-01T00%3A00%3A00Z&airdate_before=2023-01-01T23%3A59%3A59Z&limit=10&offset=10&ordering=airdate\",\"previous\":null,\"results\":[{\"id\":3136268,\"uri\":\"https://api.kexp.org/v2/plays/3136268/\",\"airdate\":\"2022-12-31T16:03:14-08:00\",\"show\":55761,\"show_uri\":\"https://api.kexp.org/v2/shows/55761/\",\"image_uri\":\"https://ia804704.us.archive.org/6/items/mbid-82eff979-38f7-4c7f-be8a-4fd3f390646e/mbid-82eff979-38f7-4c7f-be8a-4fd3f390646e-16284947122_thumb500.jpg\",\"thumbnail_uri\":\"https://ia904704.us.archive.org/6/items/mbid-82eff979-38f7-4c7f-be8a-4fd3f390646e/mbid-82eff979-38f7-4c7f-be8a-4fd3f390646e-16284947122_thumb250.jpg\",\"song\":\"All I Want\",\"track_id\":\"db2daf8d-c90a-46e9-b0ed-0134d9b3cb2d\",\"recording_id\":\"6883ec1c-33ee-4db9-8457-c4a69a731b9a\",\"artist\":\"Ride\",\"artist_ids\":[\"3f575ecd-627d-4f08-a89f-abd46d469c7e\"],\"album\":\"Weather Diaries\",\"release_id\":\"82eff979-38f7-4c7f-be8a-4fd3f390646e\",\"release_group_id\":\"8589d191-7d6f-450f-a380-ae9a4a18a8b8\",\"labels\":[\"Wichita\"],\"label_ids\":[\"1c7066e4-c0ca-42c9-b1ae-d29a39855c0a\"],\"release_date\":\"2017-06-16\",\"rotation_status\":null,\"is_local\":false,\"is_request\":false,\"is_live\":false,\"comment\":null,\"play_type\":\"trackplay\"},{\"id\":3136269,\"uri\":\"https://api.kexp.org/v2/plays/3136269/\",\"airdate\":\"2022-12-31T16:07:18-08:00\",\"show\":55761,\"show_uri\":\"https://api.kexp.org/v2/shows/55761/\",\"image_uri\":\"\",\"thumbnail_uri\":\"\",\"song\":\"God Is a Circle\",\"track_id\":\"2b5d40b1-b3c8-4cb9-8cb7-3dac6a54090b\",\"recording_id\":\"43a1d3d9-3282-431a-b7e4-44a7572107ef\",\"artist\":\"Yves Tumor\",\"artist_ids\":[\"b675f327-4738-4af4-bb83-1adb0003f2d3\"],\"album\":\"God Is a Circle\",\"release_id\":\"bb3164cd-eab1-40ac-aa57-2e306bc7b9ce\",\"release_group_id\":\"7ff289fa-569f-449e-835b-91b12b1d67fe\",\"labels\":[\"Warp\"],\"label_ids\":[\"46f0f4cd-8aab-4b33-b698-f459faf64190\"],\"release_date\":\"2022-11-03\",\"rotation_status\":\"Heavy\",\"is_local\":false,\"is_request\":false,\"is_live\":false,\"comment\":\"Frank Ocean collaborator Noah Goldstein produced \\\"God Is A Circle,\\\" legendary Alan Moulder mixed the track, and backup vocals come from Thoom and Drain Gang member Ecco2K. \\n\\nhttps://yves-tumor.bandcamp.com/album/god-is-a-circle\",\"play_type\":\"trackplay\"},{\"id\":3136270,\"uri\":\"https://api.kexp.org/v2/plays/3136270/\",\"airdate\":\"2022-12-31T16:10:48-08:00\",\"show\":55761,\"show_uri\":\"https://api.kexp.org/v2/shows/55761/\",\"image_uri\":\"https://ia600204.us.archive.org/5/items/mbid-e4df002b-a142-40bf-a3af-934ac2eae04d/mbid-e4df002b-a142-40bf-a3af-934ac2eae04d-18547238069_thumb500.jpg\",\"thumbnail_uri\":\"https://ia800204.us.archive.org/5/items/mbid-e4df002b-a142-40bf-a3af-934ac2eae04d/mbid-e4df002b-a142-40bf-a3af-934ac2eae04d-18547238069_thumb250.jpg\",\"song\":\"Dancing Choose\",\"track_id\":\"024b4fe6-89bf-3c71-a7dd-cbca2fea07d9\",\"recording_id\":\"c06109b5-f8c5-4b89-94b1-a6173cb5f7c0\",\"artist\":\"TV on the Radio\",\"artist_ids\":[\"eb872766-98f6-453d-883f-2ae908a18315\"],\"album\":\"Dear Science\",\"release_id\":\"e4df002b-a142-40bf-a3af-934ac2eae04d\",\"release_group_id\":\"572aeb84-6e76-3dc6-aea0-bf37ae6584c6\",\"labels\":[\"Interscope Records\"],\"label_ids\":[\"2182a316-c4bd-4605-936a-5e2fac52bdd2\"],\"release_date\":\"2008-09-23\",\"rotation_status\":\"Library\",\"is_local\":false,\"is_request\":false,\"is_live\":false,\"comment\":null,\"play_type\":\"trackplay\"},{\"id\":3136271,\"uri\":\"https://api.kexp.org/v2/plays/3136271/\",\"airdate\":\"2022-12-31T16:13:30-08:00\",\"show\":55761,\"show_uri\":\"https://api.kexp.org/v2/shows/55761/\",\"image_uri\":\"\",\"thumbnail_uri\":\"\",\"comment\":\"\",\"play_type\":\"airbreak\"},{\"id\":3136273,\"uri\":\"https://api.kexp.org/v2/plays/3136273/\",\"airdate\":\"2022-12-31T16:16:31-08:00\",\"show\":55761,\"show_uri\":\"https://api.kexp.org/v2/shows/55761/\",\"image_uri\":\"https://ia801508.us.archive.org/15/items/mbid-42be5c6a-9fd7-447e-9ca0-b67d0d88d92e/mbid-42be5c6a-9fd7-447e-9ca0-b67d0d88d92e-33952454076_thumb500.jpg\",\"thumbnail_uri\":\"https://ia801508.us.archive.org/15/items/mbid-42be5c6a-9fd7-447e-9ca0-b67d0d88d92e/mbid-42be5c6a-9fd7-447e-9ca0-b67d0d88d92e-33952454076_thumb250.jpg\",\"song\":\"Monte Carlo\",\"track_id\":\"8a934c47-bd56-417d-8ec9-ba16c754d975\",\"recording_id\":\"e12b1064-1a94-4b36-9988-27daef77f7af\",\"artist\":\"Abraxas\",\"artist_ids\":[\"2f9fe821-f7b7-4e9a-be10-0d3ccbba151d\"],\"album\":\"Monte Carlo\",\"release_id\":\"42be5c6a-9fd7-447e-9ca0-b67d0d88d92e\",\"release_group_id\":\"db8eadf1-e244-4f32-9ebb-5d4e225e47d3\",\"labels\":[\"Suicide Squeeze Records\"],\"label_ids\":[\"bb2ac3f3-baac-4e22-9b31-386691f06228\"],\"release_date\":\"2022-10-28\",\"rotation_status\":\"Medium\",\"is_local\":false,\"is_request\":false,\"is_live\":false,\"comment\":\"A new addition to my \\\"My Shazams\\\" playlist :)\",\"play_type\":\"trackplay\"},{\"id\":3136274,\"uri\":\"https://api.kexp.org/v2/plays/3136274/\",\"airdate\":\"2022-12-31T16:20:08-08:00\",\"show\":55761,\"show_uri\":\"https://api.kexp.org/v2/shows/55761/\",\"image_uri\":\"https://ia902701.us.archive.org/26/items/mbid-25e09e99-26cb-4414-9b58-b8f99883551a/mbid-25e09e99-26cb-4414-9b58-b8f99883551a-9357865980_thumb500.jpg\",\"thumbnail_uri\":\"https://ia802701.us.archive.org/26/items/mbid-25e09e99-26cb-4414-9b58-b8f99883551a/mbid-25e09e99-26cb-4414-9b58-b8f99883551a-9357865980_thumb250.jpg\",\"song\":\"Tropics of Love\",\"track_id\":\"1de9fad1-aaa1-3dc9-b448-1edae94a53bd\",\"recording_id\":\"fd480c90-6248-46eb-9d95-67cce09687d1\",\"artist\":\"The Black Heart Procession\",\"artist_ids\":[\"fd0f1d5f-5d8c-4fcd-9d3c-0c9f20cddc08\"],\"album\":\"Amore del Tropico\",\"release_id\":\"25e09e99-26cb-4414-9b58-b8f99883551a\",\"release_group_id\":\"55b96c10-da2d-3539-80ca-3a0f7ef65a71\",\"labels\":[\"Touch and Go Records\"],\"label_ids\":[\"9a2cad26-48de-4e7c-97a5-354f52c6e027\"],\"release_date\":\"2002-10-08\",\"rotation_status\":null,\"is_local\":false,\"is_request\":false,\"is_live\":false,\"comment\":\"\\\"It's a sultry, mystical book of romance. The crime of love is the culprit in this murder mystery and the Black Heart Procession mix a tantalizing session of lounge and blues aesthetics, dark guitars, and lush string arrangements. It's an epic move for this Californian foursome -- the concept is brilliant and musically the Black Heart Procession are sonically at their best.\\\"\\nhttps://www.allmusic.com/album/amore-del-tropico-mw0000225410\\n--\\nhttps://theblackheartprocession.bandcamp.com/music\",\"play_type\":\"trackplay\"},{\"id\":3136275,\"uri\":\"https://api.kexp.org/v2/plays/3136275/\",\"airdate\":\"2022-12-31T16:24:55-08:00\",\"show\":55761,\"show_uri\":\"https://api.kexp.org/v2/shows/55761/\",\"image_uri\":\"https://ia801008.us.archive.org/30/items/mbid-b3e6833e-ac18-410b-a762-63c3800a05af/mbid-b3e6833e-ac18-410b-a762-63c3800a05af-20101117183_thumb500.jpg\",\"thumbnail_uri\":\"https://ia601008.us.archive.org/30/items/mbid-b3e6833e-ac18-410b-a762-63c3800a05af/mbid-b3e6833e-ac18-410b-a762-63c3800a05af-20101117183_thumb250.jpg\",\"song\":\"Heji\",\"track_id\":\"d27b4662-f667-39ea-a813-f9b907fb9c20\",\"recording_id\":\"4d3e02dc-e3a7-46ef-bc14-191c33fb242e\",\"artist\":\"The Hidden Cameras\",\"artist_ids\":[\"cf998a99-5b7e-4342-b372-9c5d45c2ca62\"],\"album\":\"Awoo\",\"release_id\":\"b3e6833e-ac18-410b-a762-63c3800a05af\",\"release_group_id\":\"153caab4-284b-3c9e-846e-c263d746567f\",\"labels\":[\"Arts & Crafts\"],\"label_ids\":[\"43f1132f-9796-4a96-b898-b71ea7ad9b9b\"],\"release_date\":\"2006-12-31\",\"rotation_status\":null,\"is_local\":false,\"is_request\":false,\"is_live\":false,\"comment\":null,\"play_type\":\"trackplay\"},{\"id\":3136276,\"uri\":\"https://api.kexp.org/v2/plays/3136276/\",\"airdate\":\"2022-12-31T16:27:09-08:00\",\"show\":55761,\"show_uri\":\"https://api.kexp.org/v2/shows/55761/\",\"image_uri\":\"https://ia902708.us.archive.org/7/items/mbid-0762fd5f-99f2-4501-8b4b-c294e09f90db/mbid-0762fd5f-99f2-4501-8b4b-c294e09f90db-2580247402_thumb500.jpg\",\"thumbnail_uri\":\"https://ia802708.us.archive.org/7/items/mbid-0762fd5f-99f2-4501-8b4b-c294e09f90db/mbid-0762fd5f-99f2-4501-8b4b-c294e09f90db-2580247402_thumb250.jpg\",\"song\":\"Packt Like Sardines in a Crushd Tin Box\",\"track_id\":\"4a80a31c-a353-3de4-a981-1aee82210cbd\",\"recording_id\":\"91960108-a251-48fb-a6b7-0587926a7549\",\"artist\":\"Radiohead\",\"artist_ids\":[\"a74b1b7f-71a5-4011-9441-d0b5e4122711\"],\"album\":\"Amnesiac\",\"release_id\":\"0762fd5f-99f2-4501-8b4b-c294e09f90db\",\"release_group_id\":\"bca9280e-28b4-327f-8fe0-fd918579e486\",\"labels\":[\"Parlophone\"],\"label_ids\":[\"df7d1c7f-ef95-425f-8eef-445b3d7bcbd9\"],\"release_date\":\"2001-12-31\",\"rotation_status\":null,\"is_local\":false,\"is_request\":false,\"is_live\":false,\"comment\":null,\"play_type\":\"trackplay\"},{\"id\":3136278,\"uri\":\"https://api.kexp.org/v2/plays/3136278/\",\"airdate\":\"2022-12-31T16:34:11-08:00\",\"show\":55761,\"show_uri\":\"https://api.kexp.org/v2/shows/55761/\",\"image_uri\":\"\",\"thumbnail_uri\":\"\",\"comment\":\"\",\"play_type\":\"airbreak\"},{\"id\":3136277,\"uri\":\"https://api.kexp.org/v2/plays/3136277/\",\"airdate\":\"2022-12-31T16:34:55-08:00\",\"show\":55761,\"show_uri\":\"https://api.kexp.org/v2/shows/55761/\",\"image_uri\":\"https://ia601408.us.archive.org/9/items/mbid-4465906f-05e4-4574-91f1-3a56a3a0a2a5/mbid-4465906f-05e4-4574-91f1-3a56a3a0a2a5-34050370213_thumb500.jpg\",\"thumbnail_uri\":\"https://ia801408.us.archive.org/9/items/mbid-4465906f-05e4-4574-91f1-3a56a3a0a2a5/mbid-4465906f-05e4-4574-91f1-3a56a3a0a2a5-34050370213_thumb250.jpg\",\"song\":\"Ghetto to Meadow\",\"track_id\":\"a4afb1f4-06cb-46f9-90b1-3296ed906314\",\"recording_id\":\"102e6f71-93b9-4ff3-9093-92aa52eb5210\",\"artist\":\"Oddisee feat. Freeway\",\"artist_ids\":[\"aaae09b3-70d9-4a65-bb3f-57e85dfe9b83\",\"5d008bf9-2958-4640-a5de-dcdcebe711a4\"],\"album\":\"To What End\",\"release_id\":\"4465906f-05e4-4574-91f1-3a56a3a0a2a5\",\"release_group_id\":\"76994cc2-6d22-43ea-b036-249dee0d1ad2\",\"labels\":[\"Outer Note Label\"],\"label_ids\":[\"c995690b-f6e4-4efe-851d-4dc87f17a1f2\"],\"release_date\":\"2023-01-20\",\"rotation_status\":\"Medium\",\"is_local\":true,\"is_request\":false,\"is_live\":false,\"comment\":\"More to look forward to in 2023!\\nNew Music! \\\"Ghetto To Meadow” featuring Freeway from Oddisee's upcoming release 'To What End' - due out January 20, 2023. \\n\\nCheck out Oddisee's 2020 conversation with Gabriel and exclusive Live at Home KEXP set:\\nhttps://youtu.be/L2qePpYRnNw\\n\\nhttp://oddisee.co\\nhttps://oddisee.bandcamp.com\",\"play_type\":\"trackplay\"}]}\n\n\nInvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4264 tokens. Please reduce the length of the messages.\n\n\n\ntools = [kexp_api]\nagent = initialize_agent(tools, llm, agent=\"chat-zero-shot-react-description\", verbose=True)\n\n\ntools = [pitchfork_review_search_querystring, pitchfork_review_search_category]\nagent = initialize_agent(tools, llm, agent=\"chat-zero-shot-react-description\", verbose=True)\n\n\n\nTunefind\n\n@tool\ndef tunefind_search(query: str) -&gt; str:\n    \"\"\"Searches Tunefind for a given TV show. Required to find the base URL for\n    a given show so you can find its seasons, episodes or songs\"\"\"\n    url = f\"https://www.tunefind.com/search/site?q={query}\"\n    headers = {\n            'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'\n    }\n    response = requests.get(url, headers=headers)\n\n    soup = BeautifulSoup(response.text)\n\n    link = soup.select_one(\".tf-promo-tile\")\n    url = f\"https://www.tunefind.com{link['href']}\"\n    name = link['title']\n\n    return f\"{name} can be found at {url}\"\n\n\n@tool\ndef get_shows_from_season(query: str) -&gt; str:\n    \"\"\"Queries Tunefind for the episodes from a given season, given the URL of the season.\n    The input to this tool should be a URL to that season. The URL should be formatted as\n    the show's Tunefind URL with /season-NUM after it. For example,\n    https://www.tunefind.com/show/a-million-little-things/season-3\n    would be season 3 of the show A Million Little Things\"\"\"\n    \n    if not query.startswith(\"https://www.tunefind.com\"):\n        url = \"https://www.tunefind.com\" + query\n    else:\n        url = query\n        \n    headers = {\n            'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'\n    }\n    response = requests.get(url, headers=headers)\n\n    soup = BeautifulSoup(response.text)\n    return str(soup.select(\"[class^='EpisodeListItem_title']\"))\n\n\n@tool\ndef get_shows_from_episode(query:str) -&gt; str:\n    \"\"\"Queries Tunefind for the songs from a given show, season and episode,\n    given the URL of the episode. The input to this tool should be a URL to\n    that episode. The URL should be formatted as\n    the show's Tunefind URL with /season-NUM/SHOW_ID after it. For example,\n    https://www.tunefind.com/show/greys-anatomy/season-6/4120\n    would be an episode of Greys Anatomy from season 6.\"\"\"\n\n    if not query.startswith(\"https://www.tunefind.com\"):\n        url = \"https://www.tunefind.com\" + query\n    else:\n        url = query\n\n    headers = {\n            'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'\n    }\n    response = requests.get(url, headers=headers)\n\n    soup = BeautifulSoup(response.text)\n\n    titles = soup.select(\"[class^='SongTitle']\")\n    artists = soup.select(\"[class^='ArtistSub']\")\n    return json.dumps([{'artist': a.text, 'title': t.text} for t, a in (zip(titles, artists))])\n\n\ntools = [tunefind_search, get_shows_from_season, get_shows_from_episode]\n\n\nagent = initialize_agent(tools, llm, agent=\"chat-zero-shot-react-description\", verbose=True)\n\n\nresult = agent.run(\"What was the song by Lisa Loeb on Grey's Anatomy season 1 episode 3?\")\n\n\n\n&gt; Entering new AgentExecutor chain...\nThought: We need to use the `tunefind_search` tool to find the base URL for Grey's Anatomy, then use `get_shows_from_season` to find the URL for season 1, and finally use `get_shows_from_episode` to find the song by Lisa Loeb on episode 3.\n\nAction:\n```\n{\n  \"action\": \"tunefind_search\",\n  \"action_input\": \"Grey's Anatomy\"\n}\n```\n\n\nObservation: Grey's Anatomy can be found at https://www.tunefind.com/show/greys-anatomy\nThought:Now that we have the base URL for Grey's Anatomy, we can use `get_shows_from_season` to find the URL for season 1.\n\nAction:\n```\n{\n  \"action\": \"get_shows_from_season\",\n  \"action_input\": \"https://www.tunefind.com/show/greys-anatomy/season-1\"\n}\n```\n\n\n\nObservation: [&lt;h3 class=\"EpisodeListItem_title__PkSzj\"&gt;&lt;a href=\"/show/greys-anatomy/season-1/238\"&gt;S1 · E1 · A Hard Day's Night&lt;/a&gt;&lt;/h3&gt;, &lt;h3 class=\"EpisodeListItem_title__PkSzj\"&gt;&lt;a href=\"/show/greys-anatomy/season-1/239\"&gt;S1 · E2 · The First Cut is the Deepest&lt;/a&gt;&lt;/h3&gt;, &lt;h3 class=\"EpisodeListItem_title__PkSzj\"&gt;&lt;a href=\"/show/greys-anatomy/season-1/240\"&gt;S1 · E3 · Winning a Battle, Losing the War&lt;/a&gt;&lt;/h3&gt;, &lt;h3 class=\"EpisodeListItem_title__PkSzj\"&gt;&lt;a href=\"/show/greys-anatomy/season-1/255\"&gt;S1 · E4 · No Man's Land&lt;/a&gt;&lt;/h3&gt;, &lt;h3 class=\"EpisodeListItem_title__PkSzj\"&gt;&lt;a href=\"/show/greys-anatomy/season-1/256\"&gt;S1 · E5 · Shake Your Groove Thing&lt;/a&gt;&lt;/h3&gt;, &lt;h3 class=\"EpisodeListItem_title__PkSzj\"&gt;&lt;a href=\"/show/greys-anatomy/season-1/280\"&gt;S1 · E6 · If Tomorrow Never Comes&lt;/a&gt;&lt;/h3&gt;, &lt;h3 class=\"EpisodeListItem_title__PkSzj\"&gt;&lt;a href=\"/show/greys-anatomy/season-1/283\"&gt;S1 · E7 · The Self-Destruct Button&lt;/a&gt;&lt;/h3&gt;, &lt;h3 class=\"EpisodeListItem_title__PkSzj\"&gt;&lt;a href=\"/show/greys-anatomy/season-1/378\"&gt;S1 · E8 · Save Me&lt;/a&gt;&lt;/h3&gt;, &lt;h3 class=\"EpisodeListItem_title__PkSzj\"&gt;&lt;a href=\"/show/greys-anatomy/season-1/381\"&gt;S1 · E9 · Who's Zoomin' Who?&lt;/a&gt;&lt;/h3&gt;]\nThought:We can see that the URL for season 1 is correct. Now we can use `get_shows_from_episode` to find the song by Lisa Loeb on episode 3.\n\nAction:\n```\n{\n  \"action\": \"get_shows_from_episode\",\n  \"action_input\": \"https://www.tunefind.com/show/greys-anatomy/season-1/240\"\n}\n```\n\n\n\nObservation: [{\"artist\": \"The Ditty Bops\", \"title\": \"There's a Girl\"}, {\"artist\": \"Tegan and Sara\", \"title\": \"There's a Girl\"}, {\"artist\": \"The Ditty Bops\", \"title\": \"I Won't Be Left\"}, {\"artist\": \"stuart reid\", \"title\": \"I Won't Be Left\"}, {\"artist\": \"Reindeer Section\", \"title\": \"Wishful Thinking\"}, {\"artist\": \"Lisa Loeb\", \"title\": \"Wishful Thinking\"}, {\"artist\": \"Psapp\", \"title\": \"Hear You Breathing\"}, {\"artist\": \"Jem\", \"title\": \"Hear You Breathing\"}, {\"artist\": \"Butterfly Boucher\", \"title\": \"You Are My Joy\"}, {\"artist\": \"Thirteen Senses\", \"title\": \"You Are My Joy\"}, {\"artist\": \"Keane\", \"title\": \"Fools Like Me\"}, {\"artist\": \"Psapp\", \"title\": \"Fools Like Me\"}]\nThought:We can see that the song by Lisa Loeb on Grey's Anatomy season 1 episode 3 is \"Wishful Thinking\".\n\nFinal Answer: Wishful Thinking\n\n&gt; Finished chain.\n\n\n\nprint(result)\n\nWishful Thinking\n\n\n\nresult = agent.run(\"What are all the songs from Grey's Anatomy season 1?\")\n\n\n\n&gt; Entering new AgentExecutor chain...\nQuestion: What are all the songs from Grey's Anatomy season 1?\nThought: We need to use the `tunefind_search` tool to find the base URL for Grey's Anatomy, then use `get_shows_from_season` tool to get the songs from season 1.\nAction:\n```\n{\n  \"action\": \"tunefind_search\",\n  \"action_input\": \"Grey's Anatomy\"\n}\n```\n\nObservation: Grey's Anatomy can be found at https://www.tunefind.com/show/greys-anatomy\nThought:Now that we have the base URL for Grey's Anatomy, we can use the `get_shows_from_season` tool to get the songs from season 1.\nAction:\n```\n{\n  \"action\": \"get_shows_from_season\",\n  \"action_input\": \"https://www.tunefind.com/show/greys-anatomy/season-1\"\n}\n```\n\n\nObservation: [&lt;h3 class=\"EpisodeListItem_title__PkSzj\"&gt;&lt;a href=\"/show/greys-anatomy/season-1/238\"&gt;S1 · E1 · A Hard Day's Night&lt;/a&gt;&lt;/h3&gt;, &lt;h3 class=\"EpisodeListItem_title__PkSzj\"&gt;&lt;a href=\"/show/greys-anatomy/season-1/239\"&gt;S1 · E2 · The First Cut is the Deepest&lt;/a&gt;&lt;/h3&gt;, &lt;h3 class=\"EpisodeListItem_title__PkSzj\"&gt;&lt;a href=\"/show/greys-anatomy/season-1/240\"&gt;S1 · E3 · Winning a Battle, Losing the War&lt;/a&gt;&lt;/h3&gt;, &lt;h3 class=\"EpisodeListItem_title__PkSzj\"&gt;&lt;a href=\"/show/greys-anatomy/season-1/255\"&gt;S1 · E4 · No Man's Land&lt;/a&gt;&lt;/h3&gt;, &lt;h3 class=\"EpisodeListItem_title__PkSzj\"&gt;&lt;a href=\"/show/greys-anatomy/season-1/256\"&gt;S1 · E5 · Shake Your Groove Thing&lt;/a&gt;&lt;/h3&gt;, &lt;h3 class=\"EpisodeListItem_title__PkSzj\"&gt;&lt;a href=\"/show/greys-anatomy/season-1/280\"&gt;S1 · E6 · If Tomorrow Never Comes&lt;/a&gt;&lt;/h3&gt;, &lt;h3 class=\"EpisodeListItem_title__PkSzj\"&gt;&lt;a href=\"/show/greys-anatomy/season-1/283\"&gt;S1 · E7 · The Self-Destruct Button&lt;/a&gt;&lt;/h3&gt;, &lt;h3 class=\"EpisodeListItem_title__PkSzj\"&gt;&lt;a href=\"/show/greys-anatomy/season-1/378\"&gt;S1 · E8 · Save Me&lt;/a&gt;&lt;/h3&gt;, &lt;h3 class=\"EpisodeListItem_title__PkSzj\"&gt;&lt;a href=\"/show/greys-anatomy/season-1/381\"&gt;S1 · E9 · Who's Zoomin' Who?&lt;/a&gt;&lt;/h3&gt;]\nThought:We need to extract the URLs for each episode in season 1 so we can use the `get_shows_from_episode` tool to get the songs from each episode.\nAction:\n```\n{\n  \"action\": \"get_shows_from_episode\",\n  \"action_input\": \"https://www.tunefind.com/show/greys-anatomy/season-1/238\"\n}\n```\n\n\nObservation: [{\"artist\": \"Rilo Kiley\", \"title\": \"Portions for Foxes\"}, {\"artist\": \"BANG sugar BANG\", \"title\": \"Portions for Foxes\"}, {\"artist\": \"Jem\", \"title\": \"Super Cool\"}, {\"artist\": \"The O.A.O.T.'s\", \"title\": \"Super Cool\"}, {\"artist\": \"Vaughan Penn\", \"title\": \"They\"}, {\"artist\": \"Butterfly Boucher\", \"title\": \"They\"}, {\"artist\": \"Thirteen Senses\", \"title\": \"Dance\"}, {\"artist\": \"Psapp\", \"title\": \"Dance\"}, {\"artist\": \"Jem\", \"title\": \"Ready to Rise\"}, {\"artist\": \"Butterfly Boucher\", \"title\": \"Ready to Rise\"}, {\"artist\": \"Thirteen Senses\", \"title\": \"Life Is Short\"}, {\"artist\": \"Keane\", \"title\": \"Life Is Short\"}, {\"artist\": \"Psapp\", \"title\": \"Into the Fire\"}]\nThought:\n\n\nValueError: Could not parse LLM output: We need to repeat the `get_shows_from_episode` tool for each episode in season 1 and combine the results into a single list of all the songs from season 1.\nAction:\n\nEpisode 1\n{ “action”: “get_shows_from_episode”, “action_input”: “https://www.tunefind.com/show/greys-anatomy/season-1/238” }\n\n\nEpisode 2\n{ “action”: “get_shows_from_episode”, “action_input”: “https://www.tunefind.com/show/greys-anatomy/season-1/239” }\n\n\nEpisode 3\n{ “action”: “get_shows_from_episode”, “action_input”: “https://www.tunefind.com/show/greys-anatomy/season-1/240” }\n\n\nEpisode 4\n{ “action”: “get_shows_from_episode”, “action_input”: “https://www.tunefind.com/show/greys-anatomy/season-1/255” }\n\n\nEpisode 5\n{ “action”: “get_shows_from_episode”, “action_input”: “https://www.tunefind.com/show/greys-anatomy/season-1/256” }\n\n\nEpisode 6\n{ “action”: “get_shows_from_episode”, “action_input”: “https://www.tunefind.com/show/greys-anatomy/season-1/280” }\n\n\nEpisode 7\n{ “action”: “get_shows_from_episode”, “action_input”: “https://www.tunefind.com/show/greys-anatomy/season-1/283” }\n\n\nEpisode 8\n{ “action”: “get_shows_from_episode”, “action_input”: “https://www.tunefind.com/show/greys-anatomy/season-1/378” }\n\n\nEpisode 9\n{ “action”: “get_shows_from_episode”, “action_input”: “https://www.tunefind.com/show/greys-anatomy/season-1/381” }"
  },
  {
    "objectID": "drafts/data-integrity-kor.html",
    "href": "drafts/data-integrity-kor.html",
    "title": "How to use LLMs for data cleaning and extraction with GPT and kor",
    "section": "",
    "text": "%load_ext dotenv\n%dotenv\n\n\ncontent = \"\"\"\nN.Y.P.D. Officers Leave in Droves for Better Pay in Smaller Towns\n\nThis year has seen the highest number of resignations in two decades.\n\nEarlier this year, the chief of police in Aurora, Colo., needed to find a few dozen officers to join his force.\n\nThe chief, Dan Oates, was 50 officers short to patrol Aurora, a city of roughly 400,000 people just east of Denver. But he knew limiting his search to Colorado would not be enough: Like many other leaders in law enforcement, he has found that fewer people these days want to be cops.\n\nSo Chief Oates and his team began to seek recruits at agencies where they believed pay and morale were low. They settled on New York City, and in August, he flew about 1,800 miles to meet with New York Police Department officers. He convinced 14 of them to move out west.\n\n“I feel bad raiding my home agency,” said Chief Oates, who once served as a deputy chief in New York City. “But frankly it’s a cutthroat environment right now among police chiefs to recruit talent, and we all desperately need it.”\n\nThe departure of those officers was no anomaly. The New York Police Department, with about 34,000 officers, has seen more resignations this year than at any time in the past two decades as other agencies have become more aggressive in recruiting from its ranks.\n\nThrough November, about 1,225 officers resigned before even reaching five years of service, according to New York City Police Pension Fund statistics obtained by The New York Times. Many left for other New York State agencies or police departments outside the state.\n\nThat figure, which represents the largest such departure since at least 2002, compares to 870 resignations last year and 477 in 2020. The total number of officers who left the department through November, including retirees, is about 3,200. It is the highest overall number since November 2002.\n\nNew York Police Department officers are particularly susceptible to being wooed by other agencies. Lower salaries for new recruits are a big reason. So too are longer hours amid increased attention to crime from the mayor and the public, particularly in the subway system.\n\n“Other communities are recognizing the talent and are poaching our members,” Patrick J. Lynch, president of the New York City Police Benevolent Association, the union that represents the city’s police officers, said in a phone interview. “If we pay our police officers a market rate of pay, they will stay here. We know that’s the answer because that’s what these other departments and jurisdictions are doing, with success.”\n\n\"\"\"\n\n\nquery = \"\"\"\nWho are the people or organizations used for quotes or as information sources in the article below?\nBe as specific as possible.\n\n\"\"\" + content\n\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    messages=[\n        {\"role\": \"user\", \"content\": query}\n    ]\n)\n\n\nprint(response['choices'][0]['message']['content'])\n\n1. Chief of police in Aurora, Colo., Dan Oates\n2. New York Police Department officers\n3. New York City Police Pension Fund\n4. Patrick J. Lynch, president of the New York City Police Benevolent Association\n\n\n\nquery = \"\"\"\nWho are the people or organizations used for quotes or as information sources in the article below?\nBe as specific as possible. Express the answer using JSON.\n\n\"\"\" + content\n\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    messages=[\n        {\"role\": \"user\", \"content\": query}\n    ]\n)\n\n\nprint(response['choices'][0]['message']['content'])\n\n{\n    \"people\": [\n        \"Dan Oates\",\n        \"Patrick J. Lynch\"\n    ],\n    \"organizations\": [\n        \"Aurora Police Department\",\n        \"New York Police Department\",\n        \"New York City Police Pension Fund\",\n        \"New York City Police Benevolent Association\"\n    ]\n}\n\n\n\nquery = (\nWho are the sources used for quotes or as information sources in the article below?\nBe as specific as possible. Express the answer as a JSON list. Each item in the list should\n\"have a key 'source', a key 'role', and a key 'type' (which can be either 'person' or 'organization').\n)\n\"\"\" + content\n\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    messages=[\n        {\"role\": \"user\", \"content\": query}\n    ]\n)\n\n\nprint(response['choices'][0]['message']['content'])\n\n[{'source': 'Dan Oates', 'role': 'chief of police in Aurora, Colo.', 'type': 'person'}, {'source': 'New York Police Department officers', 'role': 'resigning officers', 'type': 'organization'}, {'source': 'New York City Police Pension Fund', 'role': 'providing statistics', 'type': 'organization'}, {'source': 'Patrick J. Lynch', 'role': 'president of the New York City Police Benevolent Association', 'type': 'person'}, {'source': 'The New York Times', 'role': 'obtaining statistics', 'type': 'organization'}]\n\n\n\nKor\n\nfrom langchain.chat_models import ChatOpenAI\nfrom kor import create_extraction_chain, Object, Text, Number\n\nllm = ChatOpenAI(\n  model_name=\"gpt-3.5-turbo\",\n  temperature=0,\n  max_tokens=2000,\n  frequency_penalty=0,\n  presence_penalty=0,\n  top_p=1.0,\n)\n\nschema = Object(\nid=\"player\",\ndescription=(\n    \"User is controling a music player to select songs, pause or start them or play\"\n    \" music by a particular artist.\"\n),\nattributes=[\n    Text(\n        id=\"song\",\n        description=\"User wants to play this song\",\n        examples=[],\n        many=True,\n    ),\n    Text(\n        id=\"album\",\n        description=\"User wants to play this album\",\n        examples=[],\n        many=True,\n    ),\n    Text(\n        id=\"artist\",\n        description=\"Music by the given artist\",\n        examples=[(\"Songs by paul simon\", \"paul simon\")],\n        many=True,\n    ),\n    Text(\n        id=\"action\",\n        description=\"Action to take one of: `play`, `stop`, `next`, `previous`.\",\n        examples=[\n            (\"Please stop the music\", \"stop\"),\n            (\"play something\", \"play\"),\n            (\"play a song\", \"play\"),\n            (\"next song\", \"next\"),\n        ],\n    ),\n],\nmany=False,\n)\n\nchain = create_extraction_chain(llm, schema, encoder_or_encoder_class='json')\nchain.predict_and_parse(text=\"play songs by paul simon and led zeppelin and the doors\")['data']\n\n\nprint(response['choices'][0]['message']['content'])\n\n[{'source': 'Dan Oates', 'role': 'chief of police in Aurora, Colo.', 'type': 'person'}, {'source': 'New York Police Department officers', 'role': 'resigning officers', 'type': 'organization'}, {'source': 'New York City Police Pension Fund', 'role': 'providing statistics', 'type': 'organization'}, {'source': 'Patrick J. Lynch', 'role': 'president of the New York City Police Benevolent Association', 'type': 'person'}, {'source': 'The New York Times', 'role': 'obtaining statistics', 'type': 'organization'}]\n\n\n\nfrom langchain.chat_models import ChatOpenAI\n\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain import LLMChain\n\nllm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n\n\nprompt = PromptTemplate(\n    input_variables=[\"story_url\"],\n    template=\"What are the sources quoted in {story_url}?\",\n)\n\nchain = LLMChain(llm=llm, prompt=prompt)\n\n\nhttps://www.nytimes.com/2022/12/09/nyregion/new-york-police-department-attrition.html"
  },
  {
    "objectID": "drafts/api-autodoc-human-feedback.html",
    "href": "drafts/api-autodoc-human-feedback.html",
    "title": "j soma",
    "section": "",
    "text": "%load_ext dotenv\n%dotenv\n\n\nfrom langchain.agents import initialize_agent, Tool\nfrom langchain.tools import BaseTool\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.agents import tool\nimport requests\nfrom bs4 import BeautifulSoup\nimport json\n\nfrom langchain import LLMChain\n\n\nllm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n\n\nfrom langchain.prompts import PromptTemplate\n\nprompt = PromptTemplate(\n    input_variables=[\"api_url\"],\n    template=\"Write documentation for the API endpoint: {api_url}\",\n)\n\n\nfrom langchain.prompts import HumanMessagePromptTemplate\n\nprompt = HumanMessagePromptTemplate.from_template(\"{text}\")\n\nchain = LLMChain(\n    llm=llm,\n    verbose=True,\n    prompt=prompt\n)\n\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.llms import OpenAI\nfrom langchain.agents import load_tools, initialize_agent\nimport sys\n\n\nllm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n\n\ntools = load_tools(\n    [\"human\"], \n    llm=llm,\n)\n\n\n# agent_chain = initialize_agent(\n#     tools,\n#     llm,\n#     agent=\"chat-zero-shot-react-description\",\n#     verbose=True,\n# )\n\nagent = initialize_agent(tools, llm, agent=\"chat-zero-shot-react-description\", verbose=True)\n\n\nimport sys\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.llms import OpenAI\nfrom langchain.agents import load_tools, initialize_agent\n\nllm = ChatOpenAI(temperature=0.0)\nmath_llm = OpenAI(temperature=0.0)\ntools = load_tools(\n    [\"llm-math\"], \n    llm=math_llm,\n)\n\nagent_chain = initialize_agent(\n    tools,\n    llm,\n    agent=\"chat-zero-shot-react-description\",\n    verbose=True,\n)\n\n\n\n\nKeyError: 'tools'"
  },
  {
    "objectID": "documenting-apis-gpt-langchain.html",
    "href": "documenting-apis-gpt-langchain.html",
    "title": "Exploring and explaining undocumented APIs with ChatGPT and LangChain",
    "section": "",
    "text": "Hi, I’m Soma! You can find me on email at jonathan.soma@gmail.com, on Twitter at @dangerscarf, or maybe even on this newsletter I’ve never sent."
  },
  {
    "objectID": "documenting-apis-gpt-langchain.html#setup",
    "href": "documenting-apis-gpt-langchain.html#setup",
    "title": "Exploring and explaining undocumented APIs with ChatGPT and LangChain",
    "section": "Setup",
    "text": "Setup\nGive me one moment to set things up! First we’ll pull in all of our API keys using python-dotenv, then we’ll use a thousand and one imports to bring in LangChain and friends.\n\n%load_ext dotenv\n%dotenv\n\n\n# LangChain imports\nfrom langchain.agents import initialize_agent\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import APIChain\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.callbacks import get_openai_callback\nfrom langchain import LLMChain\n\n# Normal human imports\nimport requests\nfrom bs4 import BeautifulSoup\nimport json\n\nEven though GPT-4 is out, we’ll be using GPT-3.5-turbo for this one. At the moment it’s infinitely cheaper, and every penny matters!\n\nllm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)"
  },
  {
    "objectID": "documenting-apis-gpt-langchain.html#success-rates",
    "href": "documenting-apis-gpt-langchain.html#success-rates",
    "title": "Exploring and explaining undocumented APIs with ChatGPT and LangChain",
    "section": "Success rates",
    "text": "Success rates\nBelieve it or not, the implicit approach has a much higher success rate!\nIn writing this piece, I’ve had to tweak the prompt that generates the explicit documentation again and again. ChatGPT seems to go out of its way to lie about the API, and not even in subtle ways:\n\nIt loves to introduce new, non-existent features\nTurn the genre=rap&genre=folk parameter into various flavors of genre=rap,folk, and just generally sacrifices the reality of the API for\nInvent lower and upper bounds for page sizes and ratings\n\nWhile these might be good architectural changes or useful new features, they absolutely aren’t implied by the original URL! Without adding a push to be conservative to the prompt, we end up with docs that are completely misleading.\nWhen you take the misleading documentation and feed it to the APIChain prompt, it winds up screwing up the request a large portion of the time. Overall I’ve found it breaks about a third of the time on the explicitly-documented example!\nOn the other hand, explicit documentation allows you to clean up and customize the docs. If you find out the maximum and minimum page size, you’re free to add it! If other genres get released or removed you’re more than welcome to edit the list.\nBut if you’re lazy, and just looking for a shortcut? Implicit docs always peform better."
  },
  {
    "objectID": "documenting-apis-gpt-langchain.html#costs",
    "href": "documenting-apis-gpt-langchain.html#costs",
    "title": "Exploring and explaining undocumented APIs with ChatGPT and LangChain",
    "section": "Costs",
    "text": "Costs\nWhile we’re all very excited about using the various OpenAI APIs, they do cost money. Yes, GPT-3.5-turbo is remarkably inexpensive compared to its peers, but we aren’t here to waste money! If we can keep the prompt smaller our queries cost less, and saving money is the second-quickest route to happiness.\nLet’s use LangChain’s get_openai_callback to examine the token count and cost of our explicit vs implicit requests. Note that we’re using verbose=False here to reduce clutter.\n\nexplicit_chain = APIChain.from_llm_and_api_docs(llm, explicit_docs, verbose=False)\n\nwith get_openai_callback() as cb:\n    response = explicit_chain.run(\"What was the first rap album reviewed by pitchfork?\")\n    print(f\"Response: {response}\")\n    print(f\"Total Tokens: {cb.total_tokens}\")\n    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n    print(f\"Completion Tokens: {cb.completion_tokens}\")\n    print(f\"Total Cost (USD): ${cb.total_cost}\")\n\nResponse: The first rap album reviewed by Pitchfork was Roots Manuva's \"Brand New Secondhand\" released in 1999, with a review published on March 23, 1999, with a rating of 9.5.\nTotal Tokens: 3058\nPrompt Tokens: 2970\nCompletion Tokens: 88\nTotal Cost (USD): $0.006116\n\n\n\nimplicit_chain = APIChain.from_llm_and_api_docs(llm, implicit_docs, verbose=False)\n\nwith get_openai_callback() as cb:\n    response = implicit_chain.run(\"What was the first rap album reviewed by pitchfork?\")\n    print(f\"Response: {response}\")\n    print(f\"Total Tokens: {cb.total_tokens}\")\n    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n    print(f\"Completion Tokens: {cb.completion_tokens}\")\n    print(f\"Total Cost (USD): ${cb.total_cost}\")\n\nResponse: The first rap album reviewed by Pitchfork was \"Brand New Secondhand\" by Roots Manuva, with a rating of 9.5. The review was written by Paul Cooper and was published on March 23, 1999.\nTotal Tokens: 1811\nPrompt Tokens: 1722\nCompletion Tokens: 89\nTotal Cost (USD): $0.0036220000000000002\n\n\nAlong with being consistent less correct, the explicit chain costs around twice as much! Just above 0.6 cents for explicit compared to 0.36 for implicit. Depending on how wordy GPT decides to be, some of my tests have seen it up to tree times as much!\nThe wildest part about this large difference is that both queries almost always making the same API response, which I assume would take up the bulk of the tokens (secret fact: Pitchfork’s API is so wordy I changed the example to size=5 so that it would fit in the GPT-3.5-turbo context window)."
  },
  {
    "objectID": "documenting-apis-gpt-langchain.html#the-takeaway",
    "href": "documenting-apis-gpt-langchain.html#the-takeaway",
    "title": "Exploring and explaining undocumented APIs with ChatGPT and LangChain",
    "section": "The takeaway",
    "text": "The takeaway\nUnless you enjoy editing or spending money, it looks like less is more."
  },
  {
    "objectID": "multi-language-qa-gpt.html",
    "href": "multi-language-qa-gpt.html",
    "title": "Multi-language document Q&A with LangChain and GPT-3.5-turbo",
    "section": "",
    "text": "Hi, I’m Soma! You can find me on email at jonathan.soma@gmail.com, on Twitter at @dangerscarf, or maybe even on this newsletter I’ve never sent."
  },
  {
    "objectID": "multi-language-qa-gpt.html#using-gpt-langchain-and-vector-stores-to-ask-questions-of-documents-in-languages-you-dont-speak",
    "href": "multi-language-qa-gpt.html#using-gpt-langchain-and-vector-stores-to-ask-questions-of-documents-in-languages-you-dont-speak",
    "title": "Multi-language document Q&A with LangChain and GPT-3.5-turbo",
    "section": "Using GPT, LangChain, and vector stores to ask questions of documents in languages you don’t speak",
    "text": "Using GPT, LangChain, and vector stores to ask questions of documents in languages you don’t speak\nI don’t speak Hungarian, but I demand to have my questions about Hungarian folktales answered! Let’s use GPT to do this for us.\nThis might be useful if you’re doing a cross-border investigation, are interested in academic papers outside of your native tongue, or are just interested in learning how LangChain and document Q&A works.\nIn this tutorial, we’ll look at:\n\nWhy making ChatGPT read an whole book is impossible\nHow to provide GPT (and other AI tools) with context to provide answers\n\nIf you don’t want to read all of this nonsense you can go directly to the LangChain source and check out Question Answering or Question Answering with Sources. This just adds a bit of multi-language sparkle on top!"
  },
  {
    "objectID": "multi-language-qa-gpt.html#our-source-material",
    "href": "multi-language-qa-gpt.html#our-source-material",
    "title": "Multi-language document Q&A with LangChain and GPT-3.5-turbo",
    "section": "Our source material",
    "text": "Our source material\nWe’ll begin by downloading the source material. If your original documents are in PDF form or anything like that, you’ll want to convert them to text first.\nOur reference is a book of folktales called Eredeti népmesék by László Arany on Project Gutenberg. It’s just a basic text file so we can download it easily.\n\nimport requests\nimport re\n\n# Gutenberg pretends everything is English, which\n# means \"Hát gyöngyömadta\" gets really mangled\nresponse = requests.get(\"https://www.gutenberg.org/files/38852/38852-0.txt\")\ntext = response.content.decode(\"utf-8\")\n\n# Cleaning up newlines\ntext = text.replace(\"\\r\", \"\")\ntext = re.sub(\"\\n(?=[^\\n])\", \"\", text)\n\n# Saving the book\nwith open('book.txt', 'w') as f:\n    f.write(text)\n\nAnd the text is indeed in Hungarian:\n\nprint(text[3000:4500])\n\nbe, de az is épen úgy járt, mint abátyja, ez is kiszaladt a szobából.\nHarmadik nap a legfiatalabb királyfin volt a sor; a bátyjai be se’akarták ereszteni, hogy ha ők ki nem tudták venni az apjokból, biz’ e’se’ sokra megy, de a királyfi nem tágitott, hanem bement. Mikor elmondtahogy m’ért jött, ehez is hozzá vágta az öreg király a nagy kést, de eznem ugrott félre, hanem megállt mint a peczek, kicsibe is mult, hogybele nem ment a kés, a sipkáját kicsapta a fejéből, úgy állt meg azajtóban. De a királyfi még ettől se’ ijedt meg, kihúzta a kést azajtóból, odavitte az apjának. ,,Itt van a kés felséges király atyám, hamegakar ölni, öljön meg, de elébb mondja meg mitől gyógyulna meg aszeme, hogy a bátyáim megszerezhessék.’’\nNagyon megilletődött ezen a beszéden a király, nemhogy megölte volnaezért a fiát, hanem össze-vissza ölelte, csókolta. No kedves fiam –mondja neki – nem hiában voltál te egész életemben nekem legkedvesebbfiam, de látom most is te szántad el magad legjobban a halálra az énmeggyógyulásomért, (mert a kést is csak azért hajitottam utánatok, hogymeglássam melyikötök szállna értem szembe a halállal), most hát nekedmegmondom, hogy mitől gyógyulna meg a szemem. Hát kedves fiam,messze-messze a Verestengeren is túl, a hármashegyen is túl lakik egykirály, annak van egy aranytollu madara, ha én annak a madárnak csakegyszer hallhatnám meg a gyönyörű éneklését, mindjárt meggyógyulnéktőle; de nincs annyi kincs, hogy od’adná érte az a király, mert annyiannak az országában az aran\n\n\nLuckily for us, GPT speaks Hungarian! So if we tell it to read the book, it’ll be able to answer all of our English-language questions without a problem. But there’s one problem: the book is not a short tiny paragraph.\nLife would be nice if we could just feed it directly to ChatGPT and start asking questions, but you can’t make ChatGPT read a whole book. After it gets partway through the book ChatGPT starts forgetting the earlier pieces!\nThere are a few tricks to get around this when asking a question. We’ll work with one of the simplest for now:\n\nSplit our original text up into smaller passages\nFind the passages most relevant to our question\nSend those passages to GPT along with our question\n\nNewer LLMs can deal with a lot more tokens at a time – GPT-4 has both an 8k and 32k version – but hey, I don’t have an invite and we work with what we’ve got."
  },
  {
    "objectID": "multi-language-qa-gpt.html#part-1-split-our-original-text-up-into-passages",
    "href": "multi-language-qa-gpt.html#part-1-split-our-original-text-up-into-passages",
    "title": "Multi-language document Q&A with LangChain and GPT-3.5-turbo",
    "section": "Part 1: Split our original text up into passages",
    "text": "Part 1: Split our original text up into passages\nTo do pretty much everything from here on out we’re relying on LangChain, a really fun library that allows you to bundle together different common tasks when working with language models. It’s best trick is chaining together AI at different steps in the process, but for the moment we’re just using its text search abilities.\nWe’re going to split our text up into 1000-character chunks, which should be around 150-200 words apiece. I’m also going to add a little overlap.\n\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nloader = TextLoader('book.txt')\ndocuments = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\ndocs = text_splitter.split_documents(documents)\n\nTechnically speaking I’m using a RecursiveCharacterTextSplitter, which tries to keep paragraphs and sentences and all of those things together, so it might go above or below 1000. But it should generally hit the mark.\n\nlen(docs)\n\n440\n\n\nOverall this gave us just over 400 documents. Let’s pick one at random to check out, just to make sure things went okay.\n\ndocs[109]\n\nDocument(page_content='Mikor aztán eljött a lakodalom napja, felöltözött, de olyan ruhába, hogyTündérországban se igen látni párját sátoros ünnepkor se, csak elfogta acselédje szemefényét. Mire a királyi palotához ért, már ott ugyancsakszólott a muzsika, úgy tánczoltak, majd leszakadt a ház, még a süketnekis bokájába ment a szép muzsika.', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0)\n\n\nIt’s a little short, but it’s definitely part of the folktales. According to Google Translate:\n\nWhen the day of the wedding came, she dressed up, but in such a dress that one would not see her partner in a fairyland even during a tent festival, she only caught the eye of her mistress. By the time he got to the royal palace, the music was already playing there too, they were dancing like that, and then the house was torn apart, the beautiful music even went to the deaf man’s ankles\n\nSounds like a pretty fun party!"
  },
  {
    "objectID": "multi-language-qa-gpt.html#part-2-find-the-passages-most-relevant-to-our-question",
    "href": "multi-language-qa-gpt.html#part-2-find-the-passages-most-relevant-to-our-question",
    "title": "Multi-language document Q&A with LangChain and GPT-3.5-turbo",
    "section": "Part 2: Find the passages most relevant to our question",
    "text": "Part 2: Find the passages most relevant to our question\n\nUnderstanding text embeddings and semantic search\nIf we’re asking questions about a wedding, we can’t just look for the text wedding – our documents are in Hungarian, so that’s lakodalom (I think). Instead, we’re going to use someting called embeddings.\nEmbeddings take a word, sentence, or snippet of text and turn it into a string of numbers. Take the sentences below as an example: I’ve scored each one of them as to how much they’re about shopping, home, and animals.\n\n\n\nsentence\nshopping\nhome\nanimals\nresult\n\n\n\n\nYou should buy a house\n0.9\n0.8\n0\n(0.9, 0.8, 0.0)\n\n\nThe cat is in the house\n0\n1\n0.8\n(0.0, 1.0, 0.8)\n\n\nThe dog bought a pet mouse\n1\n0.2\n1\n(1.0, 0.2, 1.0)\n\n\n\nLet’s say we have a fourth sentence – the dog is at home. I’ve decided it scores (0.0 1.0 0.9) since it’s about home and animals, but not shipping. How can we find a similar text?\nThe cat is in the house is the best match from our original list, even though it doesn’t have any words that match. But if we ignore the words and look at the scores, it’s clearly the best match! That’s more or less the basic idea behind text embeddings and semantic search.\nInstead of reasonable categories like mine, actual embeddings are something like 384 or 512 different dimensions your text is scored on. And unlike “shopping” or “animal” above, the dimensions aren’t anything you can understand. They’re generated by computers that have read a lot lot lot of the internet, so we just have to trust them!\n\nYou might want to read my introduction to word embeddings and conceptual document similarity for more details.\n\n\n\nCreating and searching our embeddings database\nThere are many, many embeddings out there, and they each score text differently. We need one that supports English (for our queries) and Hungarian (for the dataset): while not all of them support multiple languages, it isn’t hard to find some that do!\nWe’re going to pick paraphrase-multilingual-MiniLM-L12-v2 since it supports a delightful 50 languages. That way we can ask questions in French or Italian, or maybe add some Japanese folklore to the mix later on.\n\nfrom langchain.embeddings import HuggingFaceEmbeddings\nembeddings = HuggingFaceEmbeddings(model_name='paraphrase-multilingual-MiniLM-L12-v2')\n\nThese multilingual embeddings have read enough sentences across the all-languages-speaking internet to somehow know things like that cat and lion and Katze and tygrys and 狮 are all vaguely feline. At this point don’t need to know how it works, just that it gets the job done!\nIn order to find the most relevant pieces of text, we’ll also need something that can store and search embeddings. That way when we want to find anything about weddings it won’t have a problem finding lakodalom.\nWe’re going to use Chroma for no real reason, just because it has a convenient LangChain extension. It sets the whole thing up in one line of code - we just need to give it our documents and the embeddings model.\n\n# You'll probably need to install chromadb\n# !pip install chromadb\n\n\nfrom langchain.vectorstores import Chroma\n\ndb = Chroma.from_documents(docs, embeddings)\n\nRunning Chroma using direct local API.\nUsing DuckDB in-memory for database. Data will be transient.\n\n\nNow that everything is stored in our searchable Chroma database, we can look for passages about weddings at a festival.\n\n# k=1 because we only want one result\ndb.similarity_search(\"weddings at a festival with loud music\", k=1)\n\n[Document(page_content='Eltelt az egy hónap, elérkezett az esküvő napja, ott volt a sok vendég,köztök a boltos is, csak a vőlegényt meg a menyasszonyt nem lehetettlátni. Bekövetkezett az ebéd ideje is, mindnyájan vígan ültek le azasztalhoz, elkezdtek enni. Az volt a szokás a gróf házánál, hogy mindenembernek egy kis külön tálban vitték az ételt; a boltos amint a magatáljából szedett levest, hát csak alig tudta megenni, olyan sótalanvolt, nézett körül só után, de nem volt az egész asztalon; a másodikétel még sótalanabb volt, a harmadik meg már olyan volt, hogy hozzá se’tudott nyúlni. Kérdezték tőle hogy mért nem eszik? tán valami baja vanaz ételnek? amint ott vallatták, eszébe jutott a lyánya, hogy az nekiazt mondta, hogy úgy szereti, mint a sót, elkezdett sírni; kérdeztékaztán tőle, hogy mért sír, akkor elbeszélt mindent, hogy volt neki egylyánya, az egyszer neki azt mondta, hogy úgy szereti mint a sót, őmegharagudott érte, elkergette a házától, lám most látja, hogy milyenigazságtalan volt iránta, milyen jó a só, ,,de hej ha még egyszervisszahozná az isten hozzám, majd meg is becsülném, első lenne aházamnál; meg is bántam én azt már sokszor, de már akkor késő volt.’’', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0)]\n\n\nIt’s a match! In the next step we’ll use this process to find passages related to our question, then we’ll then pass those along to GPT as context for our questions."
  },
  {
    "objectID": "multi-language-qa-gpt.html#part-3-send-the-matches-to-gpt-along-with-our-question",
    "href": "multi-language-qa-gpt.html#part-3-send-the-matches-to-gpt-along-with-our-question",
    "title": "Multi-language document Q&A with LangChain and GPT-3.5-turbo",
    "section": "Part 3: Send the matches to GPT along with our question",
    "text": "Part 3: Send the matches to GPT along with our question\nThis is the part where LangChain really shines. We just say “hey, go get the relevant passages from our database, then go talk to GPT for us!”\nFirst, we’ll fire up our connection to GPT (you’ll need to provide your own API key!). In this case we’re specifically using GPT-3.5-turbo, because we aren’t cool enough to have GPT-4 yet.\n\nfrom langchain.llms import OpenAI\n\n# Connect to GPT-3.5 turbo\nopenai_api_key = \"sk-...\"\n\n# Use temperature=0 to get the same results every time\nllm = OpenAI(\n    model_name=\"gpt-3.5-turbo\",\n    temperature=0,\n    openai_api_key=openai_api_key)\n\nSecond, we’ll put together our vector-based Q&A. This is a custom LangChain tool that takes our original question, finds relevant passages, and packages it all up to send over to the large language model (in this case, GPT).\n\n# Vector-database-based Q&A\nqa = VectorDBQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    vectorstore=db\n)"
  },
  {
    "objectID": "multi-language-qa-gpt.html#lets-see-it-in-action",
    "href": "multi-language-qa-gpt.html#lets-see-it-in-action",
    "title": "Multi-language document Q&A with LangChain and GPT-3.5-turbo",
    "section": "Let’s see it in action!",
    "text": "Let’s see it in action!\nI’m going to ask some questions about Zsuzska, who according to some passages apparently stole some of the devil’s belongings (I don’t really know anything about her, this is just from a couple random passages I translated for myself!).\n\nquery = \"What did Zsuzska steal from the devil?\"\nqa.run(query)\n\n'The tenger-ütő pálczát (sea-beating stick).'\n\n\n\nquery = \"Why did Zsuzska steal from the devil?\"\nqa.run(query)\n\n\"Zsuzska was forced to steal from the devil by the king, who threatened her with death if she didn't.\"\n\n\nA previous time I ran this query GPT explained that the king’s aunts were jealous of Zsuzska, and they were the ones who convinced the king to make the demand of her. Since it’s been lost to the sands of time, maybe GPT can provide some more details.\n\nquery = \"Why were the king's aunts jealous of Zsuzska?\"\nqa.run(query)\n\n\"The king's aunts were jealous of Zsuzskát because the king had grown to love her and they wanted to undermine her by claiming that she could not steal the devil's golden cabbage head.\"\n\n\nThat’s a good amount of information about Zsuzska! Let’s try another character, Janko.\n\nquery = \"Who did Janko marry?\"\nqa.run(query)\n\n'Janko married a beautiful princess.'\n\n\n\nquery = \"How did Janko meet the princess?\"\nqa.run(query)\n\n\"The context does not provide information on a character named Janko meeting the king's daughter.\"\n\n\nI know for a fact that Janko met the princess because he stole her clothes while she was swimming in a lake, but I guess the appropriate context didn’t get sent to GPT. It actually used to get the question right before I changed the embeddings! In the next section we’ll see how to provide more context and hopefully get better answers.\nThere’s also a big long story about a red or bloody row that had to do with a character’s mother coming back to protect him. Let’s see what we can learn about it!\n\nquery = \"Who was the bloody cow?\"\nqa.run(query)\n\n'The bloody cow was a cow that Ferkó rode away on after throwing the lasso at it.'\n\n\n\nquery = \"Why was Ferko's mother disguised as a cow?\"\nqa.run(query)\n\n\"Ferko's mother was not disguised as a cow, but rather the red cow was actually Ferko's mother, the first queen.\""
  },
  {
    "objectID": "multi-language-qa-gpt.html#improving-our-answers-from-gpt",
    "href": "multi-language-qa-gpt.html#improving-our-answers-from-gpt",
    "title": "Multi-language document Q&A with LangChain and GPT-3.5-turbo",
    "section": "Improving our answers from GPT",
    "text": "Improving our answers from GPT\nWhen we asked what was stolen from the devil, we were told “The tenger-ütő pálczát (sea-beating stick).” I know for a fact more things were stolen than that!\nIf we provide better context, we can hopefully get better answers. Usually “better context” means “more context,” so we have two major options:\n\nIncrease the size of our window/include more overlap so passages are longer\nProvide more passages to GPT as context when asking for an answer\n\nSince I haven’t seen the second one show up too many places, let’s do that one. We’ll increase the number of passages to provide as context by adding k=8 (by default it sends 4 passages).\n\nqa = VectorDBQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    vectorstore=db,\n    k=8\n)\n\nAt this point we have to be careful of two things: money and token limits.\n\nMoney: Larger requests that include more tokens (words, characters) cost more.\nToken limits: We have around 3,000 words to work with for each GPT-3.5 request. If each chunk is up to 250 words long, this gets us up to 2,000 words before we add in our question. We should be safe!\n\nBut we want good answers, right??? Let’s see if it works:\n\nquery = \"What did Zsuzská steal from the devil?\"\nqa.run(query)\n\n\"Zsuzska stole the devil's tenger-ütő pálczája (sea-beating stick), tenger-lépő czipője (sea-stepping shoes), and arany kis gyermek (golden baby) in an arany bölcső (golden cradle). She also previously stole the devil's tenger-ütőpálczát (sea-beating stick) and arany fej káposztát (golden head cabbage).\"\n\n\nPerfect! That gold cabbage sounds great, and it’s almost time for lunch, so let’s wrap up with one more thing."
  },
  {
    "objectID": "multi-language-qa-gpt.html#seeing-the-context",
    "href": "multi-language-qa-gpt.html#seeing-the-context",
    "title": "Multi-language document Q&A with LangChain and GPT-3.5-turbo",
    "section": "Seeing the context",
    "text": "Seeing the context\nIf you’re having trouble getting good answers to your questions, it might be because the context you’re providing isn’t very good.\nI was actually having not-so-great answers earlier, but when I changed from the distiluse-base-multilingual-cased-v2 embeddings to the paraphrase-multilingual-MiniLM-L12-v2 embeddings all the context passages became so much more relavant! I honestly don’t know the difference between them, just that one provided more useful snippets to GPT.\nTo help debug similar situations, let’s look at how to inspect the context that is being provided to GPT with each search!\n\nMethod one: Context from the question\nWe can plug right into our VectorDBQA to see what context is being sent to GPT. To do this, just include the return_source_documents=True parameter.\n\nqa = VectorDBQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    vectorstore=db,\n    return_source_documents=True\n)\n\n\nquery = \"What did Zsuzská steal from the devil?\"\nresult = qa({\"query\": query})\n\nNow the response has two pieces instead of just being plain text:\n\nresult is the actual text response\nsource_documents are the passages provided as context\n\n\nresult[\"result\"]\n\n'Zsuzská stole the tenger-ütő pálczát (sea-beater stick) from the devil.'\n\n\n\nresult[\"source_documents\"]\n\n[Document(page_content='Hiába tagadta szegény Zsuzska, nem használt semmit, elindult hát nagyszomorúan. Épen éjfél volt, mikor az ördög házához ért, aludt az ördögis, a felesége is. Zsuzska csendesen belopódzott, ellopta a tenger-ütőpálczát, avval bekiáltott az ablakon.\\n– Hej ördög, viszem ám már a tenger-ütő pálczádat is.\\n– Hej kutya Zsuzska, megöletted három szép lyányomat, elloptad atenger-lépő czipőmet, most viszed a tenger-ütő pálczámat, de majdmeglakolsz te ezért.\\nUtána is szaladt, de megint csak a tengerparton tudott közel jutnihozzá, ott meg Zsuzska megütötte a tengert a tenger-ütő pálczával,kétfelé vált előtte, utána meg összecsapódott, megint nem foghatta megaz ördög. Zsuzska ment egyenesen a királyhoz.\\n– No felséges király, elhoztam már a tengerütő pálczát is.', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0),\n Document(page_content='De Zsuzska nem adta;,,Tán bolond vagyok, hogy visszaadjam, mikor kivülvagyok már vele az udvaron?!’’ Az ördög kergette egy darabig, de sehogyse tudta utolérni, utoljára is visszafordult, Zsuzska pedig mentegyenesen a király elibe, od’adta neki az arany fej káposztát.\\n– No felséges király elhoztam már ezt is!\\nA két nénjét Zsuzskának, majd hogy meg nem ütötte a guta, mikormegtudták, hogy Zsuzskának most se’ lett semmi baja, másnap megintbementek a királyhoz.\\n– Jaj felséges király van még annak az ördögnek egy arany kis gyermekeis arany bölcsőben, Zsuzska azt beszéli fűnek-fának, hogy ő azt is eltudná lopni.\\nMegint behivatta a király Zsuzskát.\\n– Fiam Zsuzska, azt hallottam, hogy van annak az ördögnek egy arany kisgyermeke is, arany bölcsőben, te azt is el tudod lopni, azt beszélted,azért ha az éjjel el nem lopod, halálnak halálával halsz meg.', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0),\n Document(page_content='– No felséges király, elhoztam már a tengerütő pálczát is.\\nA király még jobban megszerette Zsuzskát, hogy olyan életre való, de anénjei még jobban irigykedtek rá, csakhamar megint avval árulták be,hogy van annak az ördögnek egy arany fej káposztája is, Zsuzska azt isel tudná lopni, azt mondta. A király megint ráparancsolt Zsuzskára erősparancsolattal, hogy ha a káposztát el nem lopja, halálnak halálával halmeg.\\nElindult hát szegény Zsuzska megint, el is ért szerencsésen épen éjfélreaz ördög kertjibe, levágta az arany fej káposztát, avval bekiáltott azablakon.\\n– Hej ördög, viszem ám már az arany fej káposztádat is.\\n– Hej kutya Zsuzska, megöletted három szép lyányomat, elloptad atenger-lépő czipőmet, elloptad a tenger-ütő pálczámat, most viszed azarany fej káposztámat, csak ezt az egyet add vissza, soha szemedre sevetem.', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0),\n Document(page_content='Zsuzska csak nevette, de majd hogy sírás nem lett a nevetésből, mert azördög utána iramodott, Zsuzska meg nem igen tudott a nehéz bölcsővelszaladni, úgy annyira, hogy mire a tengerparthoz értek, tiz lépés nemsok, de annyi se volt köztök, hanem ott aztán Zsuzska felrántotta atenger-lépő czipőt, úgy átlépte vele a tengert, mint ha ott se lettvolna, avval mént egyenesen a király elibe, od’adta neki az arany kisgyermeket.\\nA király a mint meglátta, csak egy szikrába mult, hogy össze-vissza nemcsókolta Zsuzskát, de az is csak egy cseppbe mult ám, hogy a két nénjemeg nem pukkadt mérgibe, mikor meghallotta, hogy Zsuzska megintvisszakerült. Fúrta az oldalukat rettenetesen az irigység, mert látták,hogy a király napról-napra jobban szereti Zsuzskát. Bementek hát akirályhoz megint, azt hazudták neki hogy Zsuzska azt mondta, hogy vanannak az ördögnek egy zsák arany diója, ő azt is el tudná lopni.\\nMaga elibe parancsolta a király megint Zsuzskát:', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0)]\n\n\n\n\nMethod two: Just ask your database\nIf you already know what GPT is going to say in response and you’re debugging a specific query, you can just ask your database what the relevant snippets are! That way you avoid the costs of actually talking to the API.\n\ndb.similarity_search(\"What did Zsuzská steal from the devil?\", k=2)\n\n[Document(page_content='Hiába tagadta szegény Zsuzska, nem használt semmit, elindult hát nagyszomorúan. Épen éjfél volt, mikor az ördög házához ért, aludt az ördögis, a felesége is. Zsuzska csendesen belopódzott, ellopta a tenger-ütőpálczát, avval bekiáltott az ablakon.\\n– Hej ördög, viszem ám már a tenger-ütő pálczádat is.\\n– Hej kutya Zsuzska, megöletted három szép lyányomat, elloptad atenger-lépő czipőmet, most viszed a tenger-ütő pálczámat, de majdmeglakolsz te ezért.\\nUtána is szaladt, de megint csak a tengerparton tudott közel jutnihozzá, ott meg Zsuzska megütötte a tengert a tenger-ütő pálczával,kétfelé vált előtte, utána meg összecsapódott, megint nem foghatta megaz ördög. Zsuzska ment egyenesen a királyhoz.\\n– No felséges király, elhoztam már a tengerütő pálczát is.', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0),\n Document(page_content='De Zsuzska nem adta;,,Tán bolond vagyok, hogy visszaadjam, mikor kivülvagyok már vele az udvaron?!’’ Az ördög kergette egy darabig, de sehogyse tudta utolérni, utoljára is visszafordult, Zsuzska pedig mentegyenesen a király elibe, od’adta neki az arany fej káposztát.\\n– No felséges király elhoztam már ezt is!\\nA két nénjét Zsuzskának, majd hogy meg nem ütötte a guta, mikormegtudták, hogy Zsuzskának most se’ lett semmi baja, másnap megintbementek a királyhoz.\\n– Jaj felséges király van még annak az ördögnek egy arany kis gyermekeis arany bölcsőben, Zsuzska azt beszéli fűnek-fának, hogy ő azt is eltudná lopni.\\nMegint behivatta a király Zsuzskát.\\n– Fiam Zsuzska, azt hallottam, hogy van annak az ördögnek egy arany kisgyermeke is, arany bölcsőben, te azt is el tudod lopni, azt beszélted,azért ha az éjjel el nem lopod, halálnak halálával halsz meg.', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0)]\n\n\nYou can keep playing with your k values until you get what you think is enough context."
  },
  {
    "objectID": "multi-language-qa-gpt.html#improvements-and-next-steps",
    "href": "multi-language-qa-gpt.html#improvements-and-next-steps",
    "title": "Multi-language document Q&A with LangChain and GPT-3.5-turbo",
    "section": "Improvements and next steps",
    "text": "Improvements and next steps\nThis is a collection of folktales, not one long story. That means asking about something like a wedding might end up mixing together all sorts of different stories! Our next step will allow us to add other books, filter stories from one another, and more techniques that can help with larger, more complex datasets.\nIf you’re interested in hearing when it comes out, feel free to follow me @dangerscarf or hop on my mailing list. Questions, comments, and blind cat adoption inquiries can go to jonathan.soma@gmail.com."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An awful default creation of Jonathan Soma",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nApr 9, 2023\n\n\nExploring and explaining undocumented APIs with ChatGPT and LangChain\n\n\nJonathan Soma\n\n\n\n\nApr 8, 2023\n\n\nThree ways to scrape websites with GPT and custom LangChain tools\n\n\nJonathan Soma\n\n\n\n\nMar 27, 2023\n\n\nBuilding Spotify playlists based on vibes using LangChain and GPT\n\n\nJonathan Soma\n\n\n\n\nMar 18, 2023\n\n\nMulti-language document Q&A with LangChain and GPT-3.5-turbo\n\n\nJonathan Soma\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "three-ways-to-scrape-langchain.html",
    "href": "three-ways-to-scrape-langchain.html",
    "title": "Three ways to scrape websites with GPT and custom LangChain tools",
    "section": "",
    "text": "Hi, I’m Soma! You can find me on email at jonathan.soma@gmail.com, on Twitter at @dangerscarf, or maybe even on this newsletter I’ve never sent."
  },
  {
    "objectID": "three-ways-to-scrape-langchain.html#setup",
    "href": "three-ways-to-scrape-langchain.html#setup",
    "title": "Three ways to scrape websites with GPT and custom LangChain tools",
    "section": "Setup",
    "text": "Setup\nWe’ll start by setting up our API keys to access ChatGPT, along with importing a handful of tools.\n\n%load_ext dotenv\n%dotenv\n\n\nfrom langchain.agents import initialize_agent, Tool\nfrom langchain.tools import BaseTool\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.agents import tool\nimport requests\nfrom bs4 import BeautifulSoup\nimport json\n\nFor this use case we’re going to use GPT-4, as opposed to GPT-3.5-turbo (which is much cheaper). I’ve found GPT-4 is a lot better at understanding fragmented HTML, and we do some Python/HTML cross-pollination later on in this project which requres all the brainpower GPT can muster.\n\nllm = ChatOpenAI(model='gpt-4', temperature=0)"
  },
  {
    "objectID": "three-ways-to-scrape-langchain.html#final-thoughts",
    "href": "three-ways-to-scrape-langchain.html#final-thoughts",
    "title": "Three ways to scrape websites with GPT and custom LangChain tools",
    "section": "Final thoughts",
    "text": "Final thoughts\nLangChain tools are great! While there are certainly other approaches to what we did above – external requests plugins, for example – combinng a small amount of manual scraping while letting GPT handle the details is a good combination of convenient and cost-effectve. Instead of sending all of the HTML (too big, too expensive) or just sending the text (too unpredictable, also potentially too large), carving out the bits you’re actually interested in can do a lot for a tiny project.\nCombine this with something like kor or guardrails and your life is pretty much perfect."
  }
]