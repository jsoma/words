[
  {
    "objectID": "multi-language-qa-gpt.html",
    "href": "multi-language-qa-gpt.html",
    "title": "Multi-language document Q&A with LangChain and GPT-3.5-turbo",
    "section": "",
    "text": "Hi, I’m Soma! You can find me on email at jonathan.soma@gmail.com, on Twitter at @dangerscarf, or maybe even on this newsletter I’ve never sent."
  },
  {
    "objectID": "multi-language-qa-gpt.html#using-gpt-langchain-and-vector-stores-to-ask-questions-of-documents-in-languages-you-dont-speak",
    "href": "multi-language-qa-gpt.html#using-gpt-langchain-and-vector-stores-to-ask-questions-of-documents-in-languages-you-dont-speak",
    "title": "Multi-language document Q&A with LangChain and GPT-3.5-turbo",
    "section": "Using GPT, LangChain, and vector stores to ask questions of documents in languages you don’t speak",
    "text": "Using GPT, LangChain, and vector stores to ask questions of documents in languages you don’t speak\nI don’t speak Hungarian, but I demand to have my questions about Hungarian folktales answered! Let’s use GPT to do this for us.\nThis might be useful if you’re doing a cross-border investigation, are interested in academic papers outside of your native tongue, or are just interested in learning how LangChain and document Q&A works.\nIn this tutorial, we’ll look at:\n\nWhy making ChatGPT read an whole book is impossible\nHow to provide GPT (and other AI tools) with context to provide answers\n\nIf you don’t want to read all of this nonsense you can go directly to the LangChain source and check out Question Answering or Question Answering with Sources. This just adds a bit of multi-language sparkle on top!"
  },
  {
    "objectID": "multi-language-qa-gpt.html#our-source-material",
    "href": "multi-language-qa-gpt.html#our-source-material",
    "title": "Multi-language document Q&A with LangChain and GPT-3.5-turbo",
    "section": "Our source material",
    "text": "Our source material\nWe’ll begin by downloading the source material. If your original documents are in PDF form or anything like that, you’ll want to convert them to text first.\nOur reference is a book of folktales called Eredeti népmesék by László Arany on Project Gutenberg. It’s just a basic text file so we can download it easily.\n\nimport requests\nimport re\n\n# Gutenberg pretends everything is English, which\n# means \"Hát gyöngyömadta\" gets really mangled\nresponse = requests.get(\"https://www.gutenberg.org/files/38852/38852-0.txt\")\ntext = response.content.decode(\"utf-8\")\n\n# Cleaning up newlines\ntext = text.replace(\"\\r\", \"\")\ntext = re.sub(\"\\n(?=[^\\n])\", \"\", text)\n\n# Saving the book\nwith open('book.txt', 'w') as f:\n    f.write(text)\n\nAnd the text is indeed in Hungarian:\n\nprint(text[3000:4500])\n\nbe, de az is épen úgy járt, mint abátyja, ez is kiszaladt a szobából.\nHarmadik nap a legfiatalabb királyfin volt a sor; a bátyjai be se’akarták ereszteni, hogy ha ők ki nem tudták venni az apjokból, biz’ e’se’ sokra megy, de a királyfi nem tágitott, hanem bement. Mikor elmondtahogy m’ért jött, ehez is hozzá vágta az öreg király a nagy kést, de eznem ugrott félre, hanem megállt mint a peczek, kicsibe is mult, hogybele nem ment a kés, a sipkáját kicsapta a fejéből, úgy állt meg azajtóban. De a királyfi még ettől se’ ijedt meg, kihúzta a kést azajtóból, odavitte az apjának. ,,Itt van a kés felséges király atyám, hamegakar ölni, öljön meg, de elébb mondja meg mitől gyógyulna meg aszeme, hogy a bátyáim megszerezhessék.’’\nNagyon megilletődött ezen a beszéden a király, nemhogy megölte volnaezért a fiát, hanem össze-vissza ölelte, csókolta. No kedves fiam –mondja neki – nem hiában voltál te egész életemben nekem legkedvesebbfiam, de látom most is te szántad el magad legjobban a halálra az énmeggyógyulásomért, (mert a kést is csak azért hajitottam utánatok, hogymeglássam melyikötök szállna értem szembe a halállal), most hát nekedmegmondom, hogy mitől gyógyulna meg a szemem. Hát kedves fiam,messze-messze a Verestengeren is túl, a hármashegyen is túl lakik egykirály, annak van egy aranytollu madara, ha én annak a madárnak csakegyszer hallhatnám meg a gyönyörű éneklését, mindjárt meggyógyulnéktőle; de nincs annyi kincs, hogy od’adná érte az a király, mert annyiannak az országában az aran\n\n\nLuckily for us, GPT speaks Hungarian! So if we tell it to read the book, it’ll be able to answer all of our English-language questions without a problem. But there’s one problem: the book is not a short tiny paragraph.\nLife would be nice if we could just feed it directly to ChatGPT and start asking questions, but you can’t make ChatGPT read a whole book. After it gets partway through the book ChatGPT starts forgetting the earlier pieces!\nThere are a few tricks to get around this when asking a question. We’ll work with one of the simplest for now:\n\nSplit our original text up into smaller passages\nFind the passages most relevant to our question\nSend those passages to GPT along with our question\n\nNewer LLMs can deal with a lot more tokens at a time – GPT-4 has both an 8k and 32k version – but hey, I don’t have an invite and we work with what we’ve got."
  },
  {
    "objectID": "multi-language-qa-gpt.html#part-1-split-our-original-text-up-into-passages",
    "href": "multi-language-qa-gpt.html#part-1-split-our-original-text-up-into-passages",
    "title": "Multi-language document Q&A with LangChain and GPT-3.5-turbo",
    "section": "Part 1: Split our original text up into passages",
    "text": "Part 1: Split our original text up into passages\nTo do pretty much everything from here on out we’re relying on LangChain, a really fun library that allows you to bundle together different common tasks when working with language models. It’s best trick is chaining together AI at different steps in the process, but for the moment we’re just using its text search abilities.\nWe’re going to split our text up into 1000-character chunks, which should be around 150-200 words apiece. I’m also going to add a little overlap.\n\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nloader = TextLoader('book.txt')\ndocuments = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\ndocs = text_splitter.split_documents(documents)\n\nTechnically speaking I’m using a RecursiveCharacterTextSplitter, which tries to keep paragraphs and sentences and all of those things together, so it might go above or below 1000. But it should generally hit the mark.\n\nlen(docs)\n\n440\n\n\nOverall this gave us just over 400 documents. Let’s pick one at random to check out, just to make sure things went okay.\n\ndocs[109]\n\nDocument(page_content='Mikor aztán eljött a lakodalom napja, felöltözött, de olyan ruhába, hogyTündérországban se igen látni párját sátoros ünnepkor se, csak elfogta acselédje szemefényét. Mire a királyi palotához ért, már ott ugyancsakszólott a muzsika, úgy tánczoltak, majd leszakadt a ház, még a süketnekis bokájába ment a szép muzsika.', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0)\n\n\nIt’s a little short, but it’s definitely part of the folktales. According to Google Translate:\n\nWhen the day of the wedding came, she dressed up, but in such a dress that one would not see her partner in a fairyland even during a tent festival, she only caught the eye of her mistress. By the time he got to the royal palace, the music was already playing there too, they were dancing like that, and then the house was torn apart, the beautiful music even went to the deaf man’s ankles\n\nSounds like a pretty fun party!"
  },
  {
    "objectID": "multi-language-qa-gpt.html#part-2-find-the-passages-most-relevant-to-our-question",
    "href": "multi-language-qa-gpt.html#part-2-find-the-passages-most-relevant-to-our-question",
    "title": "Multi-language document Q&A with LangChain and GPT-3.5-turbo",
    "section": "Part 2: Find the passages most relevant to our question",
    "text": "Part 2: Find the passages most relevant to our question\n\nUnderstanding text embeddings and semantic search\nIf we’re asking questions about a wedding, we can’t just look for the text wedding – our documents are in Hungarian, so that’s lakodalom (I think). Instead, we’re going to use someting called embeddings.\nEmbeddings take a word, sentence, or snippet of text and turn it into a string of numbers. Take the sentences below as an example: I’ve scored each one of them as to how much they’re about shopping, home, and animals.\n\n\n\nsentence\nshopping\nhome\nanimals\nresult\n\n\n\n\nYou should buy a house\n0.9\n0.8\n0\n(0.9, 0.8, 0.0)\n\n\nThe cat is in the house\n0\n1\n0.8\n(0.0, 1.0, 0.8)\n\n\nThe dog bought a pet mouse\n1\n0.2\n1\n(1.0, 0.2, 1.0)\n\n\n\nLet’s say we have a fourth sentence – the dog is at home. I’ve decided it scores (0.0 1.0 0.9) since it’s about home and animals, but not shipping. How can we find a similar text?\nThe cat is in the house is the best match from our original list, even though it doesn’t have any words that match. But if we ignore the words and look at the scores, it’s clearly the best match! That’s more or less the basic idea behind text embeddings and semantic search.\nInstead of reasonable categories like mine, actual embeddings are something like 384 or 512 different dimensions your text is scored on. And unlike “shopping” or “animal” above, the dimensions aren’t anything you can understand. They’re generated by computers that have read a lot lot lot of the internet, so we just have to trust them!\n\nYou might want to read my introduction to word embeddings and conceptual document similarity for more details.\n\n\n\nCreating and searching our embeddings database\nThere are many, many embeddings out there, and they each score text differently. We need one that supports English (for our queries) and Hungarian (for the dataset): while not all of them support multiple languages, it isn’t hard to find some that do!\nWe’re going to pick paraphrase-multilingual-MiniLM-L12-v2 since it supports a delightful 50 languages. That way we can ask questions in French or Italian, or maybe add some Japanese folklore to the mix later on.\n\nfrom langchain.embeddings import HuggingFaceEmbeddings\nembeddings = HuggingFaceEmbeddings(model_name='paraphrase-multilingual-MiniLM-L12-v2')\n\nThese multilingual embeddings have read enough sentences across the all-languages-speaking internet to somehow know things like that cat and lion and Katze and tygrys and 狮 are all vaguely feline. At this point don’t need to know how it works, just that it gets the job done!\nIn order to find the most relevant pieces of text, we’ll also need something that can store and search embeddings. That way when we want to find anything about weddings it won’t have a problem finding lakodalom.\nWe’re going to use Chroma for no real reason, just because it has a convenient LangChain extension. It sets the whole thing up in one line of code - we just need to give it our documents and the embeddings model.\n\n# You'll probably need to install chromadb\n# !pip install chromadb\n\n\nfrom langchain.vectorstores import Chroma\n\ndb = Chroma.from_documents(docs, embeddings)\n\nRunning Chroma using direct local API.\nUsing DuckDB in-memory for database. Data will be transient.\n\n\nNow that everything is stored in our searchable Chroma database, we can look for passages about weddings at a festival.\n\n# k=1 because we only want one result\ndb.similarity_search(\"weddings at a festival with loud music\", k=1)\n\n[Document(page_content='Eltelt az egy hónap, elérkezett az esküvő napja, ott volt a sok vendég,köztök a boltos is, csak a vőlegényt meg a menyasszonyt nem lehetettlátni. Bekövetkezett az ebéd ideje is, mindnyájan vígan ültek le azasztalhoz, elkezdtek enni. Az volt a szokás a gróf házánál, hogy mindenembernek egy kis külön tálban vitték az ételt; a boltos amint a magatáljából szedett levest, hát csak alig tudta megenni, olyan sótalanvolt, nézett körül só után, de nem volt az egész asztalon; a másodikétel még sótalanabb volt, a harmadik meg már olyan volt, hogy hozzá se’tudott nyúlni. Kérdezték tőle hogy mért nem eszik? tán valami baja vanaz ételnek? amint ott vallatták, eszébe jutott a lyánya, hogy az nekiazt mondta, hogy úgy szereti, mint a sót, elkezdett sírni; kérdeztékaztán tőle, hogy mért sír, akkor elbeszélt mindent, hogy volt neki egylyánya, az egyszer neki azt mondta, hogy úgy szereti mint a sót, őmegharagudott érte, elkergette a házától, lám most látja, hogy milyenigazságtalan volt iránta, milyen jó a só, ,,de hej ha még egyszervisszahozná az isten hozzám, majd meg is becsülném, első lenne aházamnál; meg is bántam én azt már sokszor, de már akkor késő volt.’’', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0)]\n\n\nIt’s a match! In the next step we’ll use this process to find passages related to our question, then we’ll then pass those along to GPT as context for our questions."
  },
  {
    "objectID": "multi-language-qa-gpt.html#part-3-send-the-matches-to-gpt-along-with-our-question",
    "href": "multi-language-qa-gpt.html#part-3-send-the-matches-to-gpt-along-with-our-question",
    "title": "Multi-language document Q&A with LangChain and GPT-3.5-turbo",
    "section": "Part 3: Send the matches to GPT along with our question",
    "text": "Part 3: Send the matches to GPT along with our question\nThis is the part where LangChain really shines. We just say “hey, go get the relevant passages from our database, then go talk to GPT for us!”\nFirst, we’ll fire up our connection to GPT (you’ll need to provide your own API key!). In this case we’re specifically using GPT-3.5-turbo, because we aren’t cool enough to have GPT-4 yet.\n\nfrom langchain.llms import OpenAI\n\n# Connect to GPT-3.5 turbo\nopenai_api_key = \"sk-...\"\n\n# Use temperature=0 to get the same results every time\nllm = OpenAI(\n    model_name=\"gpt-3.5-turbo\",\n    temperature=0,\n    openai_api_key=openai_api_key)\n\nSecond, we’ll put together our vector-based Q&A. This is a custom LangChain tool that takes our original question, finds relevant passages, and packages it all up to send over to the large language model (in this case, GPT).\n\n# Vector-database-based Q&A\nqa = VectorDBQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    vectorstore=db\n)"
  },
  {
    "objectID": "multi-language-qa-gpt.html#lets-see-it-in-action",
    "href": "multi-language-qa-gpt.html#lets-see-it-in-action",
    "title": "Multi-language document Q&A with LangChain and GPT-3.5-turbo",
    "section": "Let’s see it in action!",
    "text": "Let’s see it in action!\nI’m going to ask some questions about Zsuzska, who according to some passages apparently stole some of the devil’s belongings (I don’t really know anything about her, this is just from a couple random passages I translated for myself!).\n\nquery = \"What did Zsuzska steal from the devil?\"\nqa.run(query)\n\n'The tenger-ütő pálczát (sea-beating stick).'\n\n\n\nquery = \"Why did Zsuzska steal from the devil?\"\nqa.run(query)\n\n\"Zsuzska was forced to steal from the devil by the king, who threatened her with death if she didn't.\"\n\n\nA previous time I ran this query GPT explained that the king’s aunts were jealous of Zsuzska, and they were the ones who convinced the king to make the demand of her. Since it’s been lost to the sands of time, maybe GPT can provide some more details.\n\nquery = \"Why were the king's aunts jealous of Zsuzska?\"\nqa.run(query)\n\n\"The king's aunts were jealous of Zsuzskát because the king had grown to love her and they wanted to undermine her by claiming that she could not steal the devil's golden cabbage head.\"\n\n\nThat’s a good amount of information about Zsuzska! Let’s try another character, Janko.\n\nquery = \"Who did Janko marry?\"\nqa.run(query)\n\n'Janko married a beautiful princess.'\n\n\n\nquery = \"How did Janko meet the princess?\"\nqa.run(query)\n\n\"The context does not provide information on a character named Janko meeting the king's daughter.\"\n\n\nI know for a fact that Janko met the princess because he stole her clothes while she was swimming in a lake, but I guess the appropriate context didn’t get sent to GPT. It actually used to get the question right before I changed the embeddings! In the next section we’ll see how to provide more context and hopefully get better answers.\nThere’s also a big long story about a red or bloody row that had to do with a character’s mother coming back to protect him. Let’s see what we can learn about it!\n\nquery = \"Who was the bloody cow?\"\nqa.run(query)\n\n'The bloody cow was a cow that Ferkó rode away on after throwing the lasso at it.'\n\n\n\nquery = \"Why was Ferko's mother disguised as a cow?\"\nqa.run(query)\n\n\"Ferko's mother was not disguised as a cow, but rather the red cow was actually Ferko's mother, the first queen.\""
  },
  {
    "objectID": "multi-language-qa-gpt.html#improving-our-answers-from-gpt",
    "href": "multi-language-qa-gpt.html#improving-our-answers-from-gpt",
    "title": "Multi-language document Q&A with LangChain and GPT-3.5-turbo",
    "section": "Improving our answers from GPT",
    "text": "Improving our answers from GPT\nWhen we asked what was stolen from the devil, we were told “The tenger-ütő pálczát (sea-beating stick).” I know for a fact more things were stolen than that!\nIf we provide better context, we can hopefully get better answers. Usually “better context” means “more context,” so we have two major options:\n\nIncrease the size of our window/include more overlap so passages are longer\nProvide more passages to GPT as context when asking for an answer\n\nSince I haven’t seen the second one show up too many places, let’s do that one. We’ll increase the number of passages to provide as context by adding k=8 (by default it sends 4 passages).\n\nqa = VectorDBQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    vectorstore=db,\n    k=8\n)\n\nAt this point we have to be careful of two things: money and token limits.\n\nMoney: Larger requests that include more tokens (words, characters) cost more.\nToken limits: We have around 3,000 words to work with for each GPT-3.5 request. If each chunk is up to 250 words long, this gets us up to 2,000 words before we add in our question. We should be safe!\n\nBut we want good answers, right??? Let’s see if it works:\n\nquery = \"What did Zsuzská steal from the devil?\"\nqa.run(query)\n\n\"Zsuzska stole the devil's tenger-ütő pálczája (sea-beating stick), tenger-lépő czipője (sea-stepping shoes), and arany kis gyermek (golden baby) in an arany bölcső (golden cradle). She also previously stole the devil's tenger-ütőpálczát (sea-beating stick) and arany fej káposztát (golden head cabbage).\"\n\n\nPerfect! That gold cabbage sounds great, and it’s almost time for lunch, so let’s wrap up with one more thing."
  },
  {
    "objectID": "multi-language-qa-gpt.html#seeing-the-context",
    "href": "multi-language-qa-gpt.html#seeing-the-context",
    "title": "Multi-language document Q&A with LangChain and GPT-3.5-turbo",
    "section": "Seeing the context",
    "text": "Seeing the context\nIf you’re having trouble getting good answers to your questions, it might be because the context you’re providing isn’t very good.\nI was actually having not-so-great answers earlier, but when I changed from the distiluse-base-multilingual-cased-v2 embeddings to the paraphrase-multilingual-MiniLM-L12-v2 embeddings all the context passages became so much more relavant! I honestly don’t know the difference between them, just that one provided more useful snippets to GPT.\nTo help debug similar situations, let’s look at how to inspect the context that is being provided to GPT with each search!\n\nMethod one: Context from the question\nWe can plug right into our VectorDBQA to see what context is being sent to GPT. To do this, just include the return_source_documents=True parameter.\n\nqa = VectorDBQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    vectorstore=db,\n    return_source_documents=True\n)\n\n\nquery = \"What did Zsuzská steal from the devil?\"\nresult = qa({\"query\": query})\n\nNow the response has two pieces instead of just being plain text:\n\nresult is the actual text response\nsource_documents are the passages provided as context\n\n\nresult[\"result\"]\n\n'Zsuzská stole the tenger-ütő pálczát (sea-beater stick) from the devil.'\n\n\n\nresult[\"source_documents\"]\n\n[Document(page_content='Hiába tagadta szegény Zsuzska, nem használt semmit, elindult hát nagyszomorúan. Épen éjfél volt, mikor az ördög házához ért, aludt az ördögis, a felesége is. Zsuzska csendesen belopódzott, ellopta a tenger-ütőpálczát, avval bekiáltott az ablakon.\\n– Hej ördög, viszem ám már a tenger-ütő pálczádat is.\\n– Hej kutya Zsuzska, megöletted három szép lyányomat, elloptad atenger-lépő czipőmet, most viszed a tenger-ütő pálczámat, de majdmeglakolsz te ezért.\\nUtána is szaladt, de megint csak a tengerparton tudott közel jutnihozzá, ott meg Zsuzska megütötte a tengert a tenger-ütő pálczával,kétfelé vált előtte, utána meg összecsapódott, megint nem foghatta megaz ördög. Zsuzska ment egyenesen a királyhoz.\\n– No felséges király, elhoztam már a tengerütő pálczát is.', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0),\n Document(page_content='De Zsuzska nem adta;,,Tán bolond vagyok, hogy visszaadjam, mikor kivülvagyok már vele az udvaron?!’’ Az ördög kergette egy darabig, de sehogyse tudta utolérni, utoljára is visszafordult, Zsuzska pedig mentegyenesen a király elibe, od’adta neki az arany fej káposztát.\\n– No felséges király elhoztam már ezt is!\\nA két nénjét Zsuzskának, majd hogy meg nem ütötte a guta, mikormegtudták, hogy Zsuzskának most se’ lett semmi baja, másnap megintbementek a királyhoz.\\n– Jaj felséges király van még annak az ördögnek egy arany kis gyermekeis arany bölcsőben, Zsuzska azt beszéli fűnek-fának, hogy ő azt is eltudná lopni.\\nMegint behivatta a király Zsuzskát.\\n– Fiam Zsuzska, azt hallottam, hogy van annak az ördögnek egy arany kisgyermeke is, arany bölcsőben, te azt is el tudod lopni, azt beszélted,azért ha az éjjel el nem lopod, halálnak halálával halsz meg.', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0),\n Document(page_content='– No felséges király, elhoztam már a tengerütő pálczát is.\\nA király még jobban megszerette Zsuzskát, hogy olyan életre való, de anénjei még jobban irigykedtek rá, csakhamar megint avval árulták be,hogy van annak az ördögnek egy arany fej káposztája is, Zsuzska azt isel tudná lopni, azt mondta. A király megint ráparancsolt Zsuzskára erősparancsolattal, hogy ha a káposztát el nem lopja, halálnak halálával halmeg.\\nElindult hát szegény Zsuzska megint, el is ért szerencsésen épen éjfélreaz ördög kertjibe, levágta az arany fej káposztát, avval bekiáltott azablakon.\\n– Hej ördög, viszem ám már az arany fej káposztádat is.\\n– Hej kutya Zsuzska, megöletted három szép lyányomat, elloptad atenger-lépő czipőmet, elloptad a tenger-ütő pálczámat, most viszed azarany fej káposztámat, csak ezt az egyet add vissza, soha szemedre sevetem.', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0),\n Document(page_content='Zsuzska csak nevette, de majd hogy sírás nem lett a nevetésből, mert azördög utána iramodott, Zsuzska meg nem igen tudott a nehéz bölcsővelszaladni, úgy annyira, hogy mire a tengerparthoz értek, tiz lépés nemsok, de annyi se volt köztök, hanem ott aztán Zsuzska felrántotta atenger-lépő czipőt, úgy átlépte vele a tengert, mint ha ott se lettvolna, avval mént egyenesen a király elibe, od’adta neki az arany kisgyermeket.\\nA király a mint meglátta, csak egy szikrába mult, hogy össze-vissza nemcsókolta Zsuzskát, de az is csak egy cseppbe mult ám, hogy a két nénjemeg nem pukkadt mérgibe, mikor meghallotta, hogy Zsuzska megintvisszakerült. Fúrta az oldalukat rettenetesen az irigység, mert látták,hogy a király napról-napra jobban szereti Zsuzskát. Bementek hát akirályhoz megint, azt hazudták neki hogy Zsuzska azt mondta, hogy vanannak az ördögnek egy zsák arany diója, ő azt is el tudná lopni.\\nMaga elibe parancsolta a király megint Zsuzskát:', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0)]\n\n\n\n\nMethod two: Just ask your database\nIf you already know what GPT is going to say in response and you’re debugging a specific query, you can just ask your database what the relevant snippets are! That way you avoid the costs of actually talking to the API.\n\ndb.similarity_search(\"What did Zsuzská steal from the devil?\", k=2)\n\n[Document(page_content='Hiába tagadta szegény Zsuzska, nem használt semmit, elindult hát nagyszomorúan. Épen éjfél volt, mikor az ördög házához ért, aludt az ördögis, a felesége is. Zsuzska csendesen belopódzott, ellopta a tenger-ütőpálczát, avval bekiáltott az ablakon.\\n– Hej ördög, viszem ám már a tenger-ütő pálczádat is.\\n– Hej kutya Zsuzska, megöletted három szép lyányomat, elloptad atenger-lépő czipőmet, most viszed a tenger-ütő pálczámat, de majdmeglakolsz te ezért.\\nUtána is szaladt, de megint csak a tengerparton tudott közel jutnihozzá, ott meg Zsuzska megütötte a tengert a tenger-ütő pálczával,kétfelé vált előtte, utána meg összecsapódott, megint nem foghatta megaz ördög. Zsuzska ment egyenesen a királyhoz.\\n– No felséges király, elhoztam már a tengerütő pálczát is.', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0),\n Document(page_content='De Zsuzska nem adta;,,Tán bolond vagyok, hogy visszaadjam, mikor kivülvagyok már vele az udvaron?!’’ Az ördög kergette egy darabig, de sehogyse tudta utolérni, utoljára is visszafordult, Zsuzska pedig mentegyenesen a király elibe, od’adta neki az arany fej káposztát.\\n– No felséges király elhoztam már ezt is!\\nA két nénjét Zsuzskának, majd hogy meg nem ütötte a guta, mikormegtudták, hogy Zsuzskának most se’ lett semmi baja, másnap megintbementek a királyhoz.\\n– Jaj felséges király van még annak az ördögnek egy arany kis gyermekeis arany bölcsőben, Zsuzska azt beszéli fűnek-fának, hogy ő azt is eltudná lopni.\\nMegint behivatta a király Zsuzskát.\\n– Fiam Zsuzska, azt hallottam, hogy van annak az ördögnek egy arany kisgyermeke is, arany bölcsőben, te azt is el tudod lopni, azt beszélted,azért ha az éjjel el nem lopod, halálnak halálával halsz meg.', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0)]\n\n\nYou can keep playing with your k values until you get what you think is enough context."
  },
  {
    "objectID": "multi-language-qa-gpt.html#improvements-and-next-steps",
    "href": "multi-language-qa-gpt.html#improvements-and-next-steps",
    "title": "Multi-language document Q&A with LangChain and GPT-3.5-turbo",
    "section": "Improvements and next steps",
    "text": "Improvements and next steps\nThis is a collection of folktales, not one long story. That means asking about something like a wedding might end up mixing together all sorts of different stories! Our next step will allow us to add other books, filter stories from one another, and more techniques that can help with larger, more complex datasets.\nIf you’re interested in hearing when it comes out, feel free to follow me @dangerscarf or hop on my mailing list. Questions, comments, and blind cat adoption inquiries can go to jonathan.soma@gmail.com."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An awful default creation of Jonathan Soma",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMar 27, 2023\n\n\nBuilding Spotify playlists based on vibes using LangChain and GPT\n\n\nJonathan Soma\n\n\n\n\nMar 18, 2023\n\n\nMulti-language document Q&A with LangChain and GPT-3.5-turbo\n\n\nJonathan Soma\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "custom-execution-chain.html",
    "href": "custom-execution-chain.html",
    "title": "Building Spotify playlists based on vibes using LangChain and GPT",
    "section": "",
    "text": "Hi, I’m Soma! You can find me on email at jonathan.soma@gmail.com, on Twitter at @dangerscarf, or maybe even on this newsletter I’ve never sent."
  },
  {
    "objectID": "custom-execution-chain.html#getting-my-api-keys",
    "href": "custom-execution-chain.html#getting-my-api-keys",
    "title": "Building Spotify playlists based on vibes using LangChain and GPT",
    "section": "Getting my API keys",
    "text": "Getting my API keys\nBoth GPT and Spotify require me to prove my identity using API keys. If you had my keys you’d be able to impersonate me, talk to my chatbots, and make a bunch of awful playlists – we don’t want any of those happening. Instead of putting the API keys in my notebook, I’m using dotenv-python to keep them nice and secret. I recommend it!\n\n%load_ext dotenv\n%dotenv\n\nThe dotenv extension is already loaded. To reload it, use:\n  %reload_ext dotenv"
  },
  {
    "objectID": "custom-execution-chain.html#accessing-openaigpt",
    "href": "custom-execution-chain.html#accessing-openaigpt",
    "title": "Building Spotify playlists based on vibes using LangChain and GPT",
    "section": "Accessing OpenAI/GPT",
    "text": "Accessing OpenAI/GPT\nTo access GPT-3.5-turbo, we’re use to use a LangChain chain.\nIt’s a little more complicated than when we were talking to fairy tales, but the former method of using a plain OpenAI object is being deprecated in favor of ChatOpenAI.\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain import LLMChain\n\nllm = ChatOpenAI(model_name='gpt-3.5-turbo')\n\nLet’s test it out with a sample PromptTemplate about energetic songs.\n\nprompt = PromptTemplate(\n    input_variables=[\"artist\"],\n    template=\"What are some of the more energetic songs by {artist}?\",\n)\n\nchain = LLMChain(llm=llm, prompt=prompt)\n\nNow that it’s assembled, let’s use it.\n\nresponse = chain.run(\"The Promise Ring\")\nprint(response)\n\nSome of the more energetic songs by The Promise Ring include:\n\n1. \"Is This Thing On?\"\n2. \"Emergency! Emergency!\"\n3. \"Red & Blue Jeans\"\n4. \"Happiness is All the Rage\"\n5. \"Make Me a Chevy\"\n6. \"Jersey Shore\"\n7. \"B Is for Bethlehem\"\n8. \"Why Did We Ever Meet?\"\n9. \"Stop Playing Guitar\"\n10. \"Pink Chimneys\"\n\n\nYes, that’s an answer, but it isn’t good enough. Asking GPT for energetic songs works, I want to at least pretend that we’re basing this on science! You never know if record labels from the late 90’s are funnelling money to OpenAI to bias the results.\nLuckily, Spotify has that information: their API includes access a track’s audio features including loudness, danceability and energy! With that in mind, our goal is now to create a way for GPT and Spotify to interface so that we can leverage that information when building our playlist."
  },
  {
    "objectID": "custom-execution-chain.html#accessing-spotify",
    "href": "custom-execution-chain.html#accessing-spotify",
    "title": "Building Spotify playlists based on vibes using LangChain and GPT",
    "section": "Accessing Spotify",
    "text": "Accessing Spotify\nWe’re going to use the Spotipy Python library to access Spotify. It handles all of the OAuth login, the refreshing of tokens (they’re only good for 5 minutes!), and everything of that ilk.\n\nimport os\nimport spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\n\nauth = SpotifyClientCredentials(\n    client_id=os.environ['SPOTIPY_CLIENT_ID'],\n    client_secret=os.environ['SPOTIPY_CLIENT_SECRET']\n)\nsp = spotipy.Spotify(auth_manager=auth)\n\nIf we want songs by the band Wet Leg, we can’t just say “give me Wet Legs top tracks.” Instead, we need to find Wet Leg’s URI - uniform resource indicator, Spotify’s cataloguing ID – and then use that to get the top tracks. It’s important to note that it’s almost always a multi-step process to get anything useful from Spotify.\nFor example, let’s say we want to filter for energetic songs, which means we want a track’s audio features. If you wanted the audio features for a track but only know the band name, the process looks like what is outlined below:\n\nStep one: Find the artist URI\nWe’ll use Spotify’s search to try to find the artist Wet Leg, and then assume the first result is the right one.\n\nresponse = sp.search('Wet Leg', type='artist')\nuri = response['artists']['items'][0]['uri']\nuri\n\n'spotify:artist:2TwOrUcYnAlIiKmVQkkoSZ'\n\n\n\n\nStep two: Find the tracks URIs\nWe’ll then use the artist’s URI to find some top tracks from that artist (there’s actually an endpoint for top tracks!).\n\nresponse = sp.artist_top_tracks(uri)\ntop_five = response['tracks'][:5]\n\nfor track in top_five:\n    print(track['popularity'], track['name'], track['uri'])\n\n70 Wet Dream spotify:track:260Ub1Yuj4CobdISTOBvM9\n66 Chaise Longue spotify:track:0nys6GusuHnjSYLW0PYYb7\n63 Being In Love spotify:track:4VBE0mwU8Nmm8hiqfCe4Ve\n62 Angelica spotify:track:3EwTIu5qka2l5ZekB0b6QC\n60 Ur Mum spotify:track:4ug5wsIcbAPBun8TCKn2t6\n\n\n\n\nStep three: Find the audio features\nInstead of coming with the track results, the danceability and all of those scores are in a completely different endpoint! So we’ll now use the track URIs to access the audio features.\n\nimport pandas as pd\n\nuris = [track['uri'] for track in top_five]\naudio_features = sp.audio_features(uris)\npd.DataFrame(audio_features)\n\n\n\n\n\n\n\n\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntype\nid\nuri\ntrack_href\nanalysis_url\nduration_ms\ntime_signature\n\n\n\n\n0\n0.721\n0.701\n2\n-5.941\n1\n0.0306\n0.000927\n0.026000\n0.234\n0.892\n130.091\naudio_features\n260Ub1Yuj4CobdISTOBvM9\nspotify:track:260Ub1Yuj4CobdISTOBvM9\nhttps://api.spotify.com/v1/tracks/260Ub1Yuj4Co...\nhttps://api.spotify.com/v1/audio-analysis/260U...\n140080\n3\n\n\n1\n0.684\n0.749\n7\n-6.565\n1\n0.0600\n0.001350\n0.111000\n0.141\n0.935\n160.021\naudio_features\n0nys6GusuHnjSYLW0PYYb7\nspotify:track:0nys6GusuHnjSYLW0PYYb7\nhttps://api.spotify.com/v1/tracks/0nys6GusuHnj...\nhttps://api.spotify.com/v1/audio-analysis/0nys...\n196905\n4\n\n\n2\n0.716\n0.687\n9\n-4.940\n0\n0.0342\n0.009220\n0.110000\n0.123\n0.342\n126.030\naudio_features\n4VBE0mwU8Nmm8hiqfCe4Ve\nspotify:track:4VBE0mwU8Nmm8hiqfCe4Ve\nhttps://api.spotify.com/v1/tracks/4VBE0mwU8Nmm...\nhttps://api.spotify.com/v1/audio-analysis/4VBE...\n122467\n4\n\n\n3\n0.491\n0.870\n0\n-5.138\n1\n0.0393\n0.000141\n0.000729\n0.368\n0.314\n131.989\naudio_features\n3EwTIu5qka2l5ZekB0b6QC\nspotify:track:3EwTIu5qka2l5ZekB0b6QC\nhttps://api.spotify.com/v1/tracks/3EwTIu5qka2l...\nhttps://api.spotify.com/v1/audio-analysis/3EwT...\n232320\n4\n\n\n4\n0.685\n0.720\n4\n-5.553\n1\n0.0280\n0.007020\n0.275000\n0.425\n0.554\n133.016\naudio_features\n4ug5wsIcbAPBun8TCKn2t6\nspotify:track:4ug5wsIcbAPBun8TCKn2t6\nhttps://api.spotify.com/v1/tracks/4ug5wsIcbAPB...\nhttps://api.spotify.com/v1/audio-analysis/4ug5...\n201253\n4\n\n\n\n\n\n\n\nSo what I’m saying is: we can’t just hit one endpoint and run away. This is a lot of work!"
  },
  {
    "objectID": "custom-execution-chain.html#how-the-apichain-works",
    "href": "custom-execution-chain.html#how-the-apichain-works",
    "title": "Building Spotify playlists based on vibes using LangChain and GPT",
    "section": "How the APIChain works",
    "text": "How the APIChain works\nAn APIChain can be used to access an API! This is a slightly adapted version of the APIChain example from the docs.\n\nfrom langchain.chains import APIChain\nfrom langchain.chains.api import open_meteo_docs\n\nchain_new = APIChain.from_llm_and_api_docs(llm, open_meteo_docs.OPEN_METEO_DOCS, verbose=True)\nchain_new.run('What is the weather like right now in Munich, Germany in degrees Farenheit? Do not include a forecast.')\n\n\n\n&gt; Entering new APIChain chain...\nhttps://api.open-meteo.com/v1/forecast?latitude=48.137154&longitude=11.576124&current_weather=true&temperature_unit=fahrenheit\n{\"latitude\":48.14,\"longitude\":11.58,\"generationtime_ms\":0.1989603042602539,\"utc_offset_seconds\":0,\"timezone\":\"GMT\",\"timezone_abbreviation\":\"GMT\",\"elevation\":526.0,\"current_weather\":{\"temperature\":50.0,\"windspeed\":16.1,\"winddirection\":254.0,\"weathercode\":3,\"time\":\"2023-03-26T16:00\"}}\n\n&gt; Finished chain.\n\n\n'The weather in Munich, Germany right now is 50 degrees Fahrenheit.'\n\n\nAn important thing to take note of here is open_meteo_docs.OPEN_METEO_DOCS: along with our prompt and an llm, we’re also sending the documentation for the Open-Meteo API. It looks like this:\n\nprint(open_meteo_docs.OPEN_METEO_DOCS[:1000])\n\nBASE URL: https://api.open-meteo.com/\n\nAPI Documentation\nThe API endpoint /v1/forecast accepts a geographical coordinate, a list of weather variables and responds with a JSON hourly weather forecast for 7 days. Time always starts at 0:00 today and contains 168 hours. All URL parameters are listed below:\n\nParameter   Format  Required    Default Description\nlatitude, longitude Floating point  Yes     Geographical WGS84 coordinate of the location\nhourly  String array    No      A list of weather variables which should be returned. Values can be comma separated, or multiple &hourly= parameter in the URL can be used.\ndaily   String array    No      A list of daily weather variable aggregations which should be returned. Values can be comma separated, or multiple &daily= parameter in the URL can be used. If daily weather variables are specified, parameter timezone is required.\ncurrent_weather Bool    No  false   Include current weather conditions in the JSON output.\ntemperature_unit    String  No  celsius If fahrenheit is set, al\n\n\nBut what is the chain doing with the Open-Meteo docs? If we dig around in the source code we can find a few lines of code that get into the details:\nget_request_chain = LLMChain(llm=llm, prompt=api_url_prompt)\nrequests_wrapper = RequestsWrapper(headers=headers)\nget_answer_chain = LLMChain(llm=llm, prompt=api_response_prompt)\nThese are used in a three-step process:\n\nGet the API URL\nUse the API URL to get the data\nProcess the data into an answer to the question\n\nThe first step builds an LLMChain to talk to GPT. LangChain then provides the API documentation to GPT, and asks it to determine the API endpoint to visit.\n\"\"\"You are given the below API Documentation:\n\n    {api_docs}\n\nUsing this documentation, generate the full API url to call for answering the user question.\nYou should build the API url in order to get a response that is as short as possible, while still getting the necessary information to answer the question. Pay attention to deliberately exclude any unnecessary pieces of data in the API call.\n\nQuestion:{question}\nAPI url:\"\"\"\nThe second step builds a RequestsWrapper to access the API URL and returns the response. But it isn’t a human-readable response to our question yet, it’s almost always going to be a bunch of JSON.\nThe final step uses another LLMChain to talk to GPT again: LangChain sends the API response to GPT and asks for a human-readable summary to answer the question.\n\"\"\"Here is the response from the API:\n\n{api_response}\n\nSummarize this response to answer the original question.\nSummary:\"\"\"\n\nWhy this doesn’t work for our Spotify use case\nEven though we want to talk to an API, we want to talk to an API through the Spotipy Python library, not a series of URLs. Since the APIChain is based around making an actual request to somewhere on the internet, this isn’t going to work for us.\nIf the Spotify API were a nice simple REST API we could just feed APIChain the documentation, but that isn’t the case."
  },
  {
    "objectID": "custom-execution-chain.html#how-the-palchain-works",
    "href": "custom-execution-chain.html#how-the-palchain-works",
    "title": "Building Spotify playlists based on vibes using LangChain and GPT",
    "section": "How the PALChain works",
    "text": "How the PALChain works\nA PALChain can be used to create and run arbitrary Python code! This is the PALChain example from the docs.\n\nfrom langchain.chains import PALChain\n\npal_chain = PALChain.from_math_prompt(llm, verbose=True)\nquestion = \"Jan has three times the number of pets as Marcia. Marcia has two more pets than Cindy. If Cindy has four pets, how many total pets do the three have?\"\npal_chain.run(question)\n\n\n\n&gt; Entering new PALChain chain...\ndef solution():\n    \"\"\"Jan has three times the number of pets as Marcia. Marcia has two more pets than Cindy. If Cindy has four pets, how many total pets do the three have?\"\"\"\n    cindy_pets = 4\n    marcia_pets = cindy_pets + 2\n    jan_pets = marcia_pets * 3\n    total_pets = cindy_pets + marcia_pets + jan_pets\n    result = total_pets\n    return result\n\n&gt; Finished chain.\n\n\n'28'\n\n\nIf we look at the code for the math chain’s prompt it’s very long. Here’s a portion of it:\nfrom langchain.prompts.prompt import PromptTemplate\n\ntemplate = (\n    '''\nQ: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n\n# solution in Python:\n\n\ndef solution():\n    \"\"\"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\"\"\"\n    money_initial = 23\n    bagels = 5\n    bagel_cost = 3\n    money_spent = bagels * bagel_cost\n    money_left = money_initial - money_spent\n    result = money_left\n    return result\n\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n\n# solution in Python:\n\n\ndef solution():\n    \"\"\"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\"\"\"\n    trees_initial = 15\n    trees_after = 21\n    trees_added = trees_after - trees_initial\n    result = trees_added\n    return result\n\n\n\n\n\nQ: {question}\n\n# solution in Python:\n'''.strip()\n    + \"\\n\\n\\n\"\n)\nMATH_PROMPT = PromptTemplate(input_variables=[\"question\"], template=template)\nThat prompt only gives you Python code, though, not the actual result! To see what happens with the result we need to check the PALChain code itself, lightly edited for clarity:\ndef _call(self, inputs: Dict[str, str]) -&gt; Dict[str, str]:\n    llm_chain = LLMChain(llm=self.llm, prompt=self.prompt)\n    code = llm_chain.predict(stop=[self.stop], **inputs)\n    repl = PythonREPL(_globals=self.python_globals, _locals=self.python_locals)\n    res = repl.run(code + f\"\\n{self.get_answer_expr}\")\n    output = {self.output_key: res.strip()}\n    return output\nThe code looks a little wild, but the process is pretty simple:\n\nUse the LLM and the prompt to generate some code\nCreate a Python REPL to run the generated code, sending along some global and local variables\nRun the code contained in self.get_answer_expr to get the result of the solution.\n\nIn this case, the get_answer_expr is print(solution()). Our prompt insists the answer be put in a function called solution, so this is how we call the function and obtain the result.\n\nWhy this doesn’t work for our Spotify use case\nWith a little work, we actually can make it work! It just needs a little extra effort and inspiration from APIChain."
  },
  {
    "objectID": "custom-execution-chain.html#the-spotipy-code-prompt",
    "href": "custom-execution-chain.html#the-spotipy-code-prompt",
    "title": "Building Spotify playlists based on vibes using LangChain and GPT",
    "section": "The Spotipy code prompt",
    "text": "The Spotipy code prompt\nHow do we get GPT to write Spotipy code for us? It’s similar to the API example – giving the documentation to GPT along with our question – but in this case we wouldn’t use Spotify’s API documentation, we’d use documentation for the Spotipy library.\nWhile we could try feeding GPT the entire documentation page for Spotipy, it’s too long to do it all at once. We could chunk it and feed it into a reference database that is selectively queried for relevant content… but that’s just too much work. We want something simple.\nInstead, we’re going to go the lazy route: the Spotify library has been around for ages and GPT already knows how it works, so we’ll just rely on its in-built knowledge. We just need to provide a few examples of how we like to work with the library and what we need returned, and GPT will follow our lead.\nHere’s our prompt for generating Spotipy code to access the Spotify API:\n\nfrom langchain.prompts.prompt import PromptTemplate\n\n\nSPOTIPY_PROMPT_TEMPLATE = (\n    '''\nAPI LIMITATIONS TO NOTE\n* When requesting track information, the limit is 50 at a time\n* When requesting audio features, the limit is 100 at a time\n* When selecting multiple artists, the limit is 50 at a time\n* When asking for recommendations, the limit is 100 at a time\n=====\n\nQ: What albums has the band Green Day made?\n\n# solution in Python:\n\n\ndef solution():\n    \"\"\"What albums has the band Green Day made?\"\"\"\n    search_results = sp.search(q='Green Day', type='artist')\n    uri = search_results['artists']['items'][0]['uri']\n    albums = sp.artist_albums(green_day_uri, album_type='album')\n    return albums\n\n\n\n\nQ: Who are some musicians similar to Fiona Apple?\n\n# solution in Python:\n\n\ndef solution():\n    \"\"\"Who are some musicians similar to Fiona Apple?\"\"\"\n    search_results = sp.search(q='Fiona Apple', type='artist')\n    uri = search_results['artists']['items'][0].get('uri')\n    artists = sp.artist_related_artists(uri)\n    return artists\n\n\n\nQ: Tell me what songs by The Promise Ring sound like\n\n# solution in Python:\n\n\ndef solution():\n    \"\"\"Tell me what songs by The Promise Ring sound like?\"\"\"\n    search_results = sp.search(q='The Promise Ring', type='artist')\n    uri = search_results['artists']['items'][0].get('uri')\n    tracks = sp.artist_top_tracks(uri)\n    track_uris = [track.get('uri') for track in tracks['tracks']]\n    audio_details = sp.audio_features(track_uris)\n    return audio_details\n\n\n\nQ: Get me the URI for the album The Colour And The Shape\n\n# solution in Python:\n\n\ndef solution():\n    \"\"\"Get me the URI for the album The Colour And The Shape\"\"\"\n    search_results = sp.search(q='The Colour And The Shape', type='album')\n    uri = search_results['albums']['items'][0].get('uri')\n    return uri\n\n\n\nQ: What are the first three songs on Diet Cig's Over Easy?\n\n# solution in Python:\n\n\ndef solution():\n    \"\"\"What are the first three songs on Diet Cig's Over Easy?\"\"\"\n    # Get the URI for the album\n    search_results = sp.search(q='Diet Cig Over Easy', type='album')\n    album = search_results['albums']['items'][0]\n    album_uri = album['uri']\n    # Get the album tracks\n    album_tracks = sp.album_tracks(album_uri)['items']\n    # Sort the tracks by duration\n    first_three = album_tracks[:3]\n    tracks = []\n    # Only include relevant fields\n    for i, track in enumerate(first_three):\n        # track['album'] does NOT work with sp.album_tracks\n        # you need to use album['name'] instead\n        tracks.append({{\n            'position': i+1,\n            'song_name': track.get('name'),\n            'song_uri': track['artists'][0].get('uri'),\n            'artist_uri': track['artists'][0].get('uri'),\n            'album_uri': album.get('uri'),\n            'album_name': album.get('name')\n        }})\n    return tracks\n\n\nQ: What are the thirty most danceable songs by Metallica?\n\n# solution in Python:\n\n\ndef solution():\n    \"\"\"What are most danceable songs by Metallica?\"\"\"\n    search_results = sp.search(q='Metallica', type='artist')\n    uri = search_results['artists']['items'][0]['uri']\n    albums = sp.artist_albums(uri, album_type='album')\n    album_uris = [album['uri'] for album in albums['items']]\n    tracks = []\n    for album_uri in album_uris:\n        album_tracks = sp.album_tracks(album_uri)\n        tracks.extend(album_tracks['items'])\n    track_uris = [track['uri'] for track in tracks]\n    danceable_tracks = []\n    # You can only have 100 at a time\n    for i in range(0, len(track_uris), 100):\n        subset_track_uris = track_uris[i:i+100]\n        audio_details = sp.audio_features(subset_track_uris)\n        for j, details in enumerate(audio_details):\n            if details['danceability'] &gt; 0.7:\n                track = tracks[i+j]\n                danceable_tracks.append({{\n                    'song': track.get('name')\n                    'album': track.get('album').get('name')\n                    'danceability': details.get('danceability'),\n                    'tempo': details.get('tempo'),\n                }})\n                # Be sure to add the audio details to the track\n                danceable_tracks.append(track)\n    return danceable_tracks\n\n\n\nQ: {question}. Return a list or dictionary, only including the fields necessary to answer the question, including relevant scores and the uris to the albums/songs/artists mentioned. Only return the data – if the prompt asks for a format such as markdown or a simple string, ignore it: you are only meant to provide the information, not the formatting. A later step in the process will convert the data into the new format (table, sentence, etc).\n\n# solution in Python:\n'''.strip()\n    + \"\\n\\n\\n\"\n)\n\nSPOTIPY_PROMPT = PromptTemplate(input_variables=[\"question\"], template=SPOTIPY_PROMPT_TEMPLATE)"
  },
  {
    "objectID": "custom-execution-chain.html#palchain-for-data-access",
    "href": "custom-execution-chain.html#palchain-for-data-access",
    "title": "Building Spotify playlists based on vibes using LangChain and GPT",
    "section": "PALChain for data access",
    "text": "PALChain for data access\nThe PALChain example from the docs makes it look so simple:\npal_chain = PALChain.from_math_prompt(llm, verbose=True)\nquestion = \"Jan has three times the number of pets as Marcia. Marcia has two more pets than Cindy. If Cindy has four pets, how many total pets do the three have?\"\npal_chain.run(question)\nBut a lot of work is happening behind the scenes! In our case, we’re going to be building a PALChain from scratch instead of relying on a constructor.\nThere are a couple important additions we make as we initialize the PALChain. First, we need to provide our initialized and authenticated Spotipy sp instance so the PythonREPL can access Spotipy. We’ll do this using python_globals=.\npython_globals={\n    'sp': sp\n},\nThe next step is wrangling our data. Results from chains come as strings, but the Spotify API returns JSON (or more specifically, a Python dictionary). To nicely convert our dictionary into a string we’ll be using json.dumps. The json module isn’t included by default, so this requires importing hte json library before we do the conversion.\nBoth of these steps are squished into the get_answer_expr parameter. It’s a bit garish but it works!\nget_answer_expr=\"import json; print(json.dumps(solution()))\",\nFinally, we’re also adding return_intermediate_steps=True to make sure it returns the result of the code running and the code it ran.\nThis is what it looks like all put together:\n\nfrom langchain.chains import PALChain\n\nspotify_chain = PALChain(\n    llm=llm,\n    prompt=SPOTIPY_PROMPT,\n    python_globals={\n        'sp': sp\n    },\n    stop='\\n\\n\\n',\n    verbose=True,\n    return_intermediate_steps=True,\n    get_answer_expr=\"import json; print(json.dumps(solution()))\",\n)\n\nIt’s complicated enough, but does it work?\n\nspotify_response = spotify_chain({'question': \"What are the most popular Bouncing Souls songs?\"})\nspotify_response['result']\n\n\n\n&gt; Entering new PALChain chain...\ndef solution():\n    \"\"\"What are the most popular Bouncing Souls songs?\"\"\"\n    search_results = sp.search(q='Bouncing Souls', type='artist')\n    uri = search_results['artists']['items'][0].get('uri')\n    top_tracks = sp.artist_top_tracks(uri)\n    top_track_uris = [track.get('uri') for track in top_tracks['tracks']]\n    audio_details = sp.audio_features(top_track_uris)\n    popular_songs = []\n    for i, track in enumerate(top_tracks['tracks']):\n        details = audio_details[i]\n        popular_songs.append({\n            'song_name': track.get('name'),\n            'song_uri': track.get('uri'),\n            'artist_name': track.get('artists')[0].get('name'),\n            'artist_uri': track.get('artists')[0].get('uri'),\n            'album_name': track.get('album').get('name'),\n            'album_uri': track.get('album').get('uri'),\n            'popularity': track.get('popularity'),\n            'danceability': details.get('danceability'),\n            'energy': details.get('energy'),\n            'key': details.get('key'),\n            'loudness': details.get('loudness'),\n            'mode': details.get('mode'),\n            'speechiness': details.get('speechiness'),\n            'acousticness': details.get('acousticness'),\n            'instrumentalness': details.get('instrumentalness'),\n            'liveness': details.get('liveness'),\n            'valence': details.get('valence'),\n            'tempo': details.get('tempo'),\n        })\n    return popular_songs[:10] # Return top 10 songs\n\n&gt; Finished chain.\n\n\n'[{\"song_name\": \"True Believers\", \"song_uri\": \"spotify:track:4fRmFVMd0c1SGfzazBJIM8\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"How I Spent My Summer Vacation\", \"album_uri\": \"spotify:album:64zbLX1ze8N3kcAMX0qq7G\", \"popularity\": 55, \"danceability\": 0.237, \"energy\": 0.981, \"key\": 0, \"loudness\": -4.32, \"mode\": 1, \"speechiness\": 0.0989, \"acousticness\": 0.000296, \"instrumentalness\": 3.81e-05, \"liveness\": 0.202, \"valence\": 0.475, \"tempo\": 98.181}, {\"song_name\": \"Lean On Sheena\", \"song_uri\": \"spotify:track:7IR7GUO0dUyUsBp7BfQ3vJ\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"The Gold Record\", \"album_uri\": \"spotify:album:3MF7PvmrMjEXGvA8fP3L6l\", \"popularity\": 51, \"danceability\": 0.491, \"energy\": 0.866, \"key\": 11, \"loudness\": -4.431, \"mode\": 1, \"speechiness\": 0.0583, \"acousticness\": 0.16, \"instrumentalness\": 0.000211, \"liveness\": 0.13, \"valence\": 0.694, \"tempo\": 175.969}, {\"song_name\": \"Hopeless Romantic\", \"song_uri\": \"spotify:track:180mXjN61yhrKhbY2yQc0E\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"Hopeless Romantic\", \"album_uri\": \"spotify:album:56CbFyDsG65LI1Eoh7hsOT\", \"popularity\": 49, \"danceability\": 0.243, \"energy\": 0.981, \"key\": 4, \"loudness\": -5.251, \"mode\": 1, \"speechiness\": 0.074, \"acousticness\": 0.000164, \"instrumentalness\": 1.11e-05, \"liveness\": 0.207, \"valence\": 0.216, \"tempo\": 105.022}, {\"song_name\": \"Manthem\", \"song_uri\": \"spotify:track:5pSjxUAwOol5e0TWp1ecHC\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"How I Spent My Summer Vacation\", \"album_uri\": \"spotify:album:64zbLX1ze8N3kcAMX0qq7G\", \"popularity\": 46, \"danceability\": 0.524, \"energy\": 0.986, \"key\": 2, \"loudness\": -2.865, \"mode\": 1, \"speechiness\": 0.0634, \"acousticness\": 9.44e-05, \"instrumentalness\": 0.000214, \"liveness\": 0.0772, \"valence\": 0.724, \"tempo\": 94.348}, {\"song_name\": \"Sing Along Forever\", \"song_uri\": \"spotify:track:5feYKXxg4HL2APTQGCfAav\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"Anchors Aweigh\", \"album_uri\": \"spotify:album:1xgfRXjCoynPLqtdNu50pR\", \"popularity\": 46, \"danceability\": 0.592, \"energy\": 0.964, \"key\": 0, \"loudness\": -3.672, \"mode\": 1, \"speechiness\": 0.0817, \"acousticness\": 0.00912, \"instrumentalness\": 0, \"liveness\": 0.27, \"valence\": 0.585, \"tempo\": 101.252}, {\"song_name\": \"Say Anything\", \"song_uri\": \"spotify:track:06peZfvxR5721oGqHwogha\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"The Bouncing Souls\", \"album_uri\": \"spotify:album:7LgICzKkhaLV9Gttn8xM7a\", \"popularity\": 45, \"danceability\": 0.448, \"energy\": 0.995, \"key\": 6, \"loudness\": -3.111, \"mode\": 1, \"speechiness\": 0.0539, \"acousticness\": 0.00275, \"instrumentalness\": 0, \"liveness\": 0.297, \"valence\": 0.643, \"tempo\": 101.405}, {\"song_name\": \"Ole\", \"song_uri\": \"spotify:track:2McQQA5nCLVL0XvzcxWhFC\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"Hopeless Romantic\", \"album_uri\": \"spotify:album:56CbFyDsG65LI1Eoh7hsOT\", \"popularity\": 43, \"danceability\": 0.33, \"energy\": 0.833, \"key\": 7, \"loudness\": -6.507, \"mode\": 1, \"speechiness\": 0.0768, \"acousticness\": 0.0491, \"instrumentalness\": 0, \"liveness\": 0.687, \"valence\": 0.553, \"tempo\": 128.329}, {\"song_name\": \"Ten Stories High\", \"song_uri\": \"spotify:track:1t9Y1HGwikUCCo5xCupAnT\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"Ten Stories High\", \"album_uri\": \"spotify:album:0wdbr46ndnwB1cgZoNzT48\", \"popularity\": 30, \"danceability\": 0.361, \"energy\": 0.984, \"key\": 5, \"loudness\": -1.913, \"mode\": 1, \"speechiness\": 0.0883, \"acousticness\": 0.000322, \"instrumentalness\": 0.000873, \"liveness\": 0.329, \"valence\": 0.491, \"tempo\": 198.064}, {\"song_name\": \"Kids and Heroes\", \"song_uri\": \"spotify:track:7ru4QA7k7ViuLS9oDtdRBI\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"Anchors Aweigh\", \"album_uri\": \"spotify:album:1xgfRXjCoynPLqtdNu50pR\", \"popularity\": 43, \"danceability\": 0.405, \"energy\": 0.963, \"key\": 0, \"loudness\": -5.216, \"mode\": 1, \"speechiness\": 0.0697, \"acousticness\": 0.0127, \"instrumentalness\": 0.000256, \"liveness\": 0.289, \"valence\": 0.198, \"tempo\": 101.759}, {\"song_name\": \"Kate Is Great\", \"song_uri\": \"spotify:track:1VT2wLreLu0l7E4T0JDedh\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"The Bouncing Souls\", \"album_uri\": \"spotify:album:7LgICzKkhaLV9Gttn8xM7a\", \"popularity\": 42, \"danceability\": 0.358, \"energy\": 0.93, \"key\": 2, \"loudness\": -4.726, \"mode\": 1, \"speechiness\": 0.164, \"acousticness\": 0.0822, \"instrumentalness\": 0, \"liveness\": 0.0699, \"valence\": 0.809, \"tempo\": 175.011}]'\n\n\nLet’s look at the three separate keys the PALChain response gives us.\nFirst, the question:\n\nspotify_response['question']\n\n'What are the most popular Bouncing Souls songs?'\n\n\nSecond, the intermediate steps (the code that it ran):\n\nprint(spotify_response['intermediate_steps'])\n\ndef solution():\n    \"\"\"What are the most popular Bouncing Souls songs?\"\"\"\n    search_results = sp.search(q='Bouncing Souls', type='artist')\n    uri = search_results['artists']['items'][0].get('uri')\n    top_tracks = sp.artist_top_tracks(uri)\n    top_track_uris = [track.get('uri') for track in top_tracks['tracks']]\n    audio_details = sp.audio_features(top_track_uris)\n    popular_songs = []\n    for i, track in enumerate(top_tracks['tracks']):\n        details = audio_details[i]\n        popular_songs.append({\n            'song_name': track.get('name'),\n            'song_uri': track.get('uri'),\n            'artist_name': track.get('artists')[0].get('name'),\n            'artist_uri': track.get('artists')[0].get('uri'),\n            'album_name': track.get('album').get('name'),\n            'album_uri': track.get('album').get('uri'),\n            'popularity': track.get('popularity'),\n            'danceability': details.get('danceability'),\n            'energy': details.get('energy'),\n            'key': details.get('key'),\n            'loudness': details.get('loudness'),\n            'mode': details.get('mode'),\n            'speechiness': details.get('speechiness'),\n            'acousticness': details.get('acousticness'),\n            'instrumentalness': details.get('instrumentalness'),\n            'liveness': details.get('liveness'),\n            'valence': details.get('valence'),\n            'tempo': details.get('tempo'),\n        })\n    return popular_songs[:10] # Return top 10 songs\n\n\nFinally, the actual response. In the PALChain examples it’s mostly the result of a quick calculation, but this time it’s a whole big mess of JSON:\n\nspotify_response['result']\n\n'[{\"song_name\": \"True Believers\", \"song_uri\": \"spotify:track:4fRmFVMd0c1SGfzazBJIM8\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"How I Spent My Summer Vacation\", \"album_uri\": \"spotify:album:64zbLX1ze8N3kcAMX0qq7G\", \"popularity\": 55, \"danceability\": 0.237, \"energy\": 0.981, \"key\": 0, \"loudness\": -4.32, \"mode\": 1, \"speechiness\": 0.0989, \"acousticness\": 0.000296, \"instrumentalness\": 3.81e-05, \"liveness\": 0.202, \"valence\": 0.475, \"tempo\": 98.181}, {\"song_name\": \"Lean On Sheena\", \"song_uri\": \"spotify:track:7IR7GUO0dUyUsBp7BfQ3vJ\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"The Gold Record\", \"album_uri\": \"spotify:album:3MF7PvmrMjEXGvA8fP3L6l\", \"popularity\": 51, \"danceability\": 0.491, \"energy\": 0.866, \"key\": 11, \"loudness\": -4.431, \"mode\": 1, \"speechiness\": 0.0583, \"acousticness\": 0.16, \"instrumentalness\": 0.000211, \"liveness\": 0.13, \"valence\": 0.694, \"tempo\": 175.969}, {\"song_name\": \"Hopeless Romantic\", \"song_uri\": \"spotify:track:180mXjN61yhrKhbY2yQc0E\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"Hopeless Romantic\", \"album_uri\": \"spotify:album:56CbFyDsG65LI1Eoh7hsOT\", \"popularity\": 49, \"danceability\": 0.243, \"energy\": 0.981, \"key\": 4, \"loudness\": -5.251, \"mode\": 1, \"speechiness\": 0.074, \"acousticness\": 0.000164, \"instrumentalness\": 1.11e-05, \"liveness\": 0.207, \"valence\": 0.216, \"tempo\": 105.022}, {\"song_name\": \"Manthem\", \"song_uri\": \"spotify:track:5pSjxUAwOol5e0TWp1ecHC\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"How I Spent My Summer Vacation\", \"album_uri\": \"spotify:album:64zbLX1ze8N3kcAMX0qq7G\", \"popularity\": 46, \"danceability\": 0.524, \"energy\": 0.986, \"key\": 2, \"loudness\": -2.865, \"mode\": 1, \"speechiness\": 0.0634, \"acousticness\": 9.44e-05, \"instrumentalness\": 0.000214, \"liveness\": 0.0772, \"valence\": 0.724, \"tempo\": 94.348}, {\"song_name\": \"Sing Along Forever\", \"song_uri\": \"spotify:track:5feYKXxg4HL2APTQGCfAav\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"Anchors Aweigh\", \"album_uri\": \"spotify:album:1xgfRXjCoynPLqtdNu50pR\", \"popularity\": 46, \"danceability\": 0.592, \"energy\": 0.964, \"key\": 0, \"loudness\": -3.672, \"mode\": 1, \"speechiness\": 0.0817, \"acousticness\": 0.00912, \"instrumentalness\": 0, \"liveness\": 0.27, \"valence\": 0.585, \"tempo\": 101.252}, {\"song_name\": \"Say Anything\", \"song_uri\": \"spotify:track:06peZfvxR5721oGqHwogha\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"The Bouncing Souls\", \"album_uri\": \"spotify:album:7LgICzKkhaLV9Gttn8xM7a\", \"popularity\": 45, \"danceability\": 0.448, \"energy\": 0.995, \"key\": 6, \"loudness\": -3.111, \"mode\": 1, \"speechiness\": 0.0539, \"acousticness\": 0.00275, \"instrumentalness\": 0, \"liveness\": 0.297, \"valence\": 0.643, \"tempo\": 101.405}, {\"song_name\": \"Ole\", \"song_uri\": \"spotify:track:2McQQA5nCLVL0XvzcxWhFC\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"Hopeless Romantic\", \"album_uri\": \"spotify:album:56CbFyDsG65LI1Eoh7hsOT\", \"popularity\": 43, \"danceability\": 0.33, \"energy\": 0.833, \"key\": 7, \"loudness\": -6.507, \"mode\": 1, \"speechiness\": 0.0768, \"acousticness\": 0.0491, \"instrumentalness\": 0, \"liveness\": 0.687, \"valence\": 0.553, \"tempo\": 128.329}, {\"song_name\": \"Ten Stories High\", \"song_uri\": \"spotify:track:1t9Y1HGwikUCCo5xCupAnT\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"Ten Stories High\", \"album_uri\": \"spotify:album:0wdbr46ndnwB1cgZoNzT48\", \"popularity\": 30, \"danceability\": 0.361, \"energy\": 0.984, \"key\": 5, \"loudness\": -1.913, \"mode\": 1, \"speechiness\": 0.0883, \"acousticness\": 0.000322, \"instrumentalness\": 0.000873, \"liveness\": 0.329, \"valence\": 0.491, \"tempo\": 198.064}, {\"song_name\": \"Kids and Heroes\", \"song_uri\": \"spotify:track:7ru4QA7k7ViuLS9oDtdRBI\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"Anchors Aweigh\", \"album_uri\": \"spotify:album:1xgfRXjCoynPLqtdNu50pR\", \"popularity\": 43, \"danceability\": 0.405, \"energy\": 0.963, \"key\": 0, \"loudness\": -5.216, \"mode\": 1, \"speechiness\": 0.0697, \"acousticness\": 0.0127, \"instrumentalness\": 0.000256, \"liveness\": 0.289, \"valence\": 0.198, \"tempo\": 101.759}, {\"song_name\": \"Kate Is Great\", \"song_uri\": \"spotify:track:1VT2wLreLu0l7E4T0JDedh\", \"artist_name\": \"The Bouncing Souls\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_name\": \"The Bouncing Souls\", \"album_uri\": \"spotify:album:7LgICzKkhaLV9Gttn8xM7a\", \"popularity\": 42, \"danceability\": 0.358, \"energy\": 0.93, \"key\": 2, \"loudness\": -4.726, \"mode\": 1, \"speechiness\": 0.164, \"acousticness\": 0.0822, \"instrumentalness\": 0, \"liveness\": 0.0699, \"valence\": 0.809, \"tempo\": 175.011}]'\n\n\nLooks great! These don’t answer our question, they only provides the data, so we’ll need one more step."
  },
  {
    "objectID": "custom-execution-chain.html#llmchain-for-cleanup",
    "href": "custom-execution-chain.html#llmchain-for-cleanup",
    "title": "Building Spotify playlists based on vibes using LangChain and GPT",
    "section": "LLMChain for cleanup",
    "text": "LLMChain for cleanup\nThis is similar to what happens in the APIChain: we have an API response, but we want something a little more human. We’ll use an LLMChain to send the JSON to GPT along with our question, then get back a readable response.\n\nRESPONSE_CLEANUP_PROMPT_TEMPLATE = (\"\"\" \nUsing this code:\n\n```python\n{intermediate_steps}\n```\n\nWe got the following output from the Spotify API:\n\n```json\n{result}\n```\n\nUsing the output above as your data source, answer the question {question}. Don't describe the code or process, just answer the question.\nAnswer:\"\"\"\n)\n\nRESPONSE_CLEANUP_PROMPT = PromptTemplate(\n    input_variables=[\"question\", \"intermediate_steps\", \"result\"],\n    template=RESPONSE_CLEANUP_PROMPT_TEMPLATE,\n)\n\nIn the prompt above, we’re providing three things to the prompt:\n\nThe original question we want an answer to\nThe intermediate steps, which is the actual Python code the PALChain created\nThe result, the output of the Python code from the PALChain (aka the JSON)\n\nWe can now use this prompt with an LLMChain to turn the JSON into an actual answer.\n\nexplainer_chain = LLMChain(\n    llm=llm,\n    prompt=RESPONSE_CLEANUP_PROMPT,\n    verbose=True,\n    output_key='answer'\n)\n\nNow that we’ve built the structure of the explainer, let’s feed it the previous Spotify response and see what happens.\n\nexplainer_response = explainer_chain(spotify_response)\n\n\n\n&gt; Entering new LLMChain chain...\nPrompt after formatting:\n \nUsing this code:\n\n```python\ndef solution():\n    \"\"\"What are the most popular Bouncing Souls songs?\"\"\"\n    search_results = sp.search(q='Bouncing Souls', type='artist')\n    uri = search_results['artists']['items'][0]['uri']\n    top_tracks = sp.artist_top_tracks(uri)\n    tracks = []\n    for i, track in enumerate(top_tracks['tracks']):\n        # Only include relevant fields\n        tracks.append({\n            'position': i+1,\n            'song_name': track.get('name'),\n            'song_uri': track.get('uri'),\n            'artist_uri': uri,\n            'album_uri': track.get('album').get('uri'),\n            'album_name': track.get('album').get('name'),\n            'popularity': track.get('popularity')\n        })\n    return tracks\n```\n\nWe got the following output from the Spotify API:\n\n```json\n[{\"position\": 1, \"song_name\": \"True Believers\", \"song_uri\": \"spotify:track:4fRmFVMd0c1SGfzazBJIM8\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_uri\": \"spotify:album:64zbLX1ze8N3kcAMX0qq7G\", \"album_name\": \"How I Spent My Summer Vacation\", \"popularity\": 54}, {\"position\": 2, \"song_name\": \"Lean On Sheena\", \"song_uri\": \"spotify:track:7IR7GUO0dUyUsBp7BfQ3vJ\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_uri\": \"spotify:album:3MF7PvmrMjEXGvA8fP3L6l\", \"album_name\": \"The Gold Record\", \"popularity\": 51}, {\"position\": 3, \"song_name\": \"Hopeless Romantic\", \"song_uri\": \"spotify:track:180mXjN61yhrKhbY2yQc0E\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_uri\": \"spotify:album:56CbFyDsG65LI1Eoh7hsOT\", \"album_name\": \"Hopeless Romantic\", \"popularity\": 49}, {\"position\": 4, \"song_name\": \"Manthem\", \"song_uri\": \"spotify:track:5pSjxUAwOol5e0TWp1ecHC\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_uri\": \"spotify:album:64zbLX1ze8N3kcAMX0qq7G\", \"album_name\": \"How I Spent My Summer Vacation\", \"popularity\": 46}, {\"position\": 5, \"song_name\": \"Sing Along Forever\", \"song_uri\": \"spotify:track:5feYKXxg4HL2APTQGCfAav\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_uri\": \"spotify:album:1xgfRXjCoynPLqtdNu50pR\", \"album_name\": \"Anchors Aweigh\", \"popularity\": 46}, {\"position\": 6, \"song_name\": \"Say Anything\", \"song_uri\": \"spotify:track:06peZfvxR5721oGqHwogha\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_uri\": \"spotify:album:7LgICzKkhaLV9Gttn8xM7a\", \"album_name\": \"The Bouncing Souls\", \"popularity\": 45}, {\"position\": 7, \"song_name\": \"Ole\", \"song_uri\": \"spotify:track:2McQQA5nCLVL0XvzcxWhFC\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_uri\": \"spotify:album:56CbFyDsG65LI1Eoh7hsOT\", \"album_name\": \"Hopeless Romantic\", \"popularity\": 43}, {\"position\": 8, \"song_name\": \"Kids and Heroes\", \"song_uri\": \"spotify:track:7ru4QA7k7ViuLS9oDtdRBI\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_uri\": \"spotify:album:1xgfRXjCoynPLqtdNu50pR\", \"album_name\": \"Anchors Aweigh\", \"popularity\": 42}, {\"position\": 9, \"song_name\": \"Kate Is Great\", \"song_uri\": \"spotify:track:1VT2wLreLu0l7E4T0JDedh\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_uri\": \"spotify:album:7LgICzKkhaLV9Gttn8xM7a\", \"album_name\": \"The Bouncing Souls\", \"popularity\": 41}, {\"position\": 10, \"song_name\": \"Ten Stories High\", \"song_uri\": \"spotify:track:0Wz9RJySVFtUTFQk8sjRBv\", \"artist_uri\": \"spotify:artist:3mvTAjG7rcyk7DQzLwauzV\", \"album_uri\": \"spotify:album:5xEwUAv3WJiDtHScEPliQl\", \"album_name\": \"Ten Stories High\", \"popularity\": 40}]\n```\n\nUsing the output above as your data source, answer the question What are the most popular Bouncing Souls songs?. Don't describe the code or process, just answer the question.\nAnswer:\n\n&gt; Finished chain.\n\n\n\nprint(explainer_response['answer'])\n\nThe most popular Bouncing Souls songs, based on the provided data, are:\n\n1. True Believers\n2. Lean On Sheena\n3. Hopeless Romantic\n4. Manthem\n5. Sing Along Forever\n6. Say Anything\n7. Ole\n8. Kids and Heroes\n9. Kate Is Great\n10. Ten Stories High\n\n\nI completely disagree with everyone’s taste in music, but it’s a perfect response!"
  }
]