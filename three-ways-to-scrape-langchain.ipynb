{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0a29db27",
   "metadata": {},
   "source": [
    "---\n",
    "title: Three ways to scrape websites with GPT and custom LangChain tools \n",
    "description: An introduction to LangChain tools and several approaches to parsing scraped data with LLMs.\n",
    "date: \"2023-04-08\"\n",
    "author: Jonathan Soma\n",
    "published: true\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32674cef",
   "metadata": {},
   "source": [
    "Hi, I'm Soma! You can find me on email at [jonathan.soma@gmail.com](mailto:jonathan.soma@gmail.com), on Twitter at [@dangerscarf](https://twitter.com/dangerscarf), or maybe even on [this newsletter I've never sent](https://tinyletter.com/jsoma)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734122ea",
   "metadata": {},
   "source": [
    "# Introducing our old friend BeautifulSoup to our new best pal ChatGPT\n",
    "\n",
    "This tutorial is two-in-one: how to build **custom LangChain tools** powered by large language models, along with how to combine a tiny bit of **Python scraping** abilities with **GPT-4**'s processing power!\n",
    "\n",
    "Using [everyone's favorite library LangChain](https://langchain.readthedocs.io/) and the classic Python scraping library [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), we'll look at three use cases:\n",
    "\n",
    "1. Extracting one single part of a page to feed ChatGPT information\n",
    "2. Converting a section (or sections) of a page into GPT-parseable data without doing much prep work\n",
    "3. Saving effort and money by pre-processing pages we're sending to the LLM for analysis\n",
    "\n",
    "Along the way you'll learn how to build custom langchain tools, writng proper descriptions for them and providing the \"right\" kind of data when they're done doing their work! By the end we'll have a fully-functioning scraper that can answer natural-language questions about songs featured in TV shows.\n",
    "\n",
    "## Setup\n",
    "\n",
    "We'll start by setting up our API keys to access ChatGPT, along with importing a handful of tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b6693d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4abbf4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import tool\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898ac7e1",
   "metadata": {},
   "source": [
    "For this use case we're going to use GPT-4, as opposed to GPT-3.5-turbo (which is much cheaper). I've found GPT-4 is a *lot* better at understanding fragmented HTML, and we do some Python/HTML cross-pollination later on in this project which requres all the brainpower GPT can muster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "af0458cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model='gpt-4', temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756ed961",
   "metadata": {},
   "source": [
    "# Method one: Single-element extraction\n",
    "\n",
    "Sometimes when you're scraping it isn't too hard: the URL is simple, and you're **just trying to grab one thing off of the page.** Our first situation is like that: we'll start on the Tunefind search results page.\n",
    "\n",
    "The URL is a simple fill-in-the-blanks with `https://www.tunefind.com/search/site?q=SHOW_NAME`.\n",
    "\n",
    "![Search results page for Grey's Anatomy](assets/greys-anatomy-1.png)\n",
    "\n",
    "The results page provides a list of shows. We pull out the frst match – where `class` is `tf-promo-tile` – and send back both the URL to the show as well as the show name.\n",
    "\n",
    "If we did this manually, it might look like the code below.\n",
    "\n",
    "```python\n",
    "# Build the URL\n",
    "query = \"grey's anatomy\"\n",
    "url = f\"https://www.tunefind.com/search/site?q={query}\"\n",
    "\n",
    "# Make the request\n",
    "headers = { 'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0' }\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Extract the link\n",
    "soup = BeautifulSoup(response.text)\n",
    "link = soup.select_one(\".tf-promo-tile\")\n",
    "\n",
    "# Save the URL and name\n",
    "url = f\"https://www.tunefind.com{link['href']}\"\n",
    "name = link['title']\n",
    "```\n",
    "\n",
    "That's nice and fine, but **we want to turn this into a [LangChain tool](https://python.langchain.com/en/latest/modules/agents/tools.html)** that can be used to interact with the outside world. That requires two changes:\n",
    "\n",
    "* We change this code into a function\n",
    "* We write a description to the LLM can understand how it works\n",
    "\n",
    "A simple version might look something like the code below. You provide a `query` and get back a sentence with the name and URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "670ece67",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def tunefind_search(query: str) -> str:\n",
    "    \"\"\"Searches Tunefind for a given TV show. Required to find the base URL for\n",
    "    a given show so you can find its seasons, episodes or songs.\n",
    "    The input to this tool should be the show we're searching for.\"\"\"\n",
    "\n",
    "    url = f\"https://www.tunefind.com/search/site?q={query}\"\n",
    "    headers = { 'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0' }\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(response.text)\n",
    "\n",
    "    link = soup.select_one(\".tf-promo-tile\")\n",
    "    url = f\"https://www.tunefind.com{link['href']}\"\n",
    "    name = link['title']\n",
    "\n",
    "    return f\"{name} can be found at {url}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05950d4",
   "metadata": {},
   "source": [
    "To allow LangChain – and us! – to use this new search tool, we'll create an agent that has access to it. When there's a question that might be answered by our new tool (based on the tool description), the agent will run off and try to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e05baecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tool-using agent\n",
    "tools = [tunefind_search]\n",
    "agent = initialize_agent(tools, llm, agent=\"chat-zero-shot-react-description\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad26c385",
   "metadata": {},
   "source": [
    "Using this setup is as simple as `agent.run` with our question. Between the knowledge that the GPT already has and the ability to use the tool, it will try to answer our question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "46b6daa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find the base Tunefind URL for Grey's Anatomy.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"tunefind_search\",\n",
      "  \"action_input\": \"Grey's Anatomy\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mGrey's Anatomy can be found at https://www.tunefind.com/show/greys-anatomy\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The Tunefind URL for Grey's Anatomy is https://www.tunefind.com/show/greys-anatomy\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The Tunefind URL for Grey's Anatomy is https://www.tunefind.com/show/greys-anatomy\n"
     ]
    }
   ],
   "source": [
    "# Get the results\n",
    "result = agent.run(\"What's the Tunefind URL for Grey's Anatomy?\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88c7790",
   "metadata": {},
   "source": [
    "Perfect! When given access to the tool GPT now sees the sentence `\"Grey's Anatomy can be found at https://www.tunefind.com/show/greys-anatomy\"` which allows it to determine the show's URL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624ad179",
   "metadata": {},
   "source": [
    "# Method two: Searching parts of the page\n",
    "\n",
    "Sometimes the page you want to scrape is a little more complicated. You don't just want a tiny piece of content of the page, but rather a **specific portion of the page** or **several separate elements**.\n",
    "\n",
    "If we visit the [episode list of a Grey's Anatomy season](https://www.tunefind.com/show/greys-anatomy) we're presented with a ton of links, one for each episode:\n",
    "\n",
    "![Episode list page of Grey's Anatomy](assets/greys-anatomy-2.png)\n",
    "\n",
    "If we want GPT to have access a list of the seasons and their URLs, we have a few options. Let's work through them one by one.\n",
    "\n",
    "### Scrape this page in a traditional way and return some sort of formatted list\n",
    "\n",
    "While it's certainly possible to extract the data through **traditional scraping**, we're too lazy for that! We want GPT to do the work for us. That's the whole reason we're on this page to begin with!\n",
    "\n",
    "### Send the whole page to GPT and let it figure things out\n",
    "\n",
    "Sending the whole page to GPT along with the question of \"find the links\" has two downsides:\n",
    "\n",
    "First, **the page might be too long for GPT to handle.** When you send data to GPT along with a question, it can only handle so much text! Web pages often have too much content for GPT to be able to process them all at once.\n",
    "\n",
    "Second, all of the unnecessary information **drives up our OpenAI bill!** We're being charged on how much text we send and receive, so if we can pare things down we can be a little thrifter.\n",
    "\n",
    "### Carve out the portions of the page we're interested in\n",
    "\n",
    "With just a little familiarity with scraping, we can pursue a third option that leverages the benefits of the first two: we just grab the sections of the page that we're interested in and send them on over to GPT along with our question!\n",
    "\n",
    "We don't need to do anything that would qualify as complicated scraping: no loops, nothing nested, no `try`/`pass` statements. Just \"hey, take this data!\"\n",
    "\n",
    "Before it goes into a LangChain tool, the code might look like the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ba5a6017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[<h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-1\">Season 1</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-2\">Season 2</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-3\">Season 3</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-4\">Season 4</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-5\">Season 5</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-6\">Season 6</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-7\">Season 7</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-8\">Season 8</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-9\">Season 9</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-10\">Season 10</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-11\">Season 11</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-12\">Season 12</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-13\">Season 13</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-14\">Season 14</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-15\">Season 15</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-16\">Season 16</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-17\">Season 17</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-18\">Season 18</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-19\">Season 19</a></h3>]'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the URL and make the request\n",
    "url = \"https://www.tunefind.com/show/greys-anatomy\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Grab all of the elements with a class that includes with \"EpisodeListItem_title\"\n",
    "soup = BeautifulSoup(response.text)\n",
    "elements = soup.select(\"[class*='EpisodeListItem_title']\")\n",
    "str(elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cad7a2",
   "metadata": {},
   "source": [
    "Check out the last line: when you use `str(...)` with a BeautifulSoup object (or a list of them) it returns the HTML representation of the objects (or in this case, a list).\n",
    "\n",
    "Is the result ugly? Absolutely! It's a mishmash of all of the parts of the page that might have information for us – the season links – shoehorned into an awful combination of Python list and HTML elements.\n",
    "\n",
    "It doesn't matter how we feel about it, though: will GPT be able to understand it? **Absolutely!** And that's what counts.\n",
    "\n",
    "**Now let's turn it into a LangChain tool.** It's the same process as last time, adding `@tool`, making it a function, adding a description. Let's take a look at how we describe it:\n",
    "\n",
    "```\n",
    "Queries Tunefind for the episodes from a given season, given the URL of the season.\n",
    "The input to this tool should be a URL to that season.\n",
    "Season URLs are formed by takng the show's Tunefind URL and adding /season-NUM after it.\n",
    "For example, if the show A Million Little Things is at https://www.tunefind.com/show/a-million-little-things/\n",
    "Season 3 of A Million Little Things could be found at https://www.tunefind.com/show/a-million-little-things/season-3\n",
    "```\n",
    "\n",
    "The important thing to note is that even though our last tool only found the show URL at `https://www.tunefind.com/show/greys-anatomy`, ChatGPT is smart enough to add `/season-1` on it if it knows we're looking for season 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a16efdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_shows_from_season(url: str) -> str:\n",
    "    \"\"\"Queries Tunefind for the episode list from a given season, given the URL of the season.\n",
    "    The input to this tool should be a URL to that season.\n",
    "\n",
    "    Season URLs are formed by takng the show's Tunefind URL and adding /season-NUM after it.\n",
    "    For example, if a show's URL is https://www.tunefind.com/show/a-million-little-things/\n",
    "    you can find episode links for season 3 at https://www.tunefind.com/show/a-million-little-things/season-3\n",
    "    \"\"\"\n",
    "    \n",
    "    headers = { 'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0' }\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    elements = soup.select(\"[class*='EpisodeListItem_title']\")\n",
    "\n",
    "    return str(elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f358d32a",
   "metadata": {},
   "source": [
    "Now that we've built it, it's time to **use our new tool.**\n",
    "\n",
    "Same process as before, but this time our agent has access to two tools: the tool that can find the show page, and the tool that can pull the episode lists from the episode page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7483563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tool-using agent\n",
    "tools = [tunefind_search, get_shows_from_season]\n",
    "agent = initialize_agent(tools, llm, agent=\"chat-zero-shot-react-description\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "01e41f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: First, I need to find the base URL for Grey's Anatomy on Tunefind.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"tunefind_search\",\n",
      "  \"action_input\": \"Grey's Anatomy\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mGrey's Anatomy can be found at https://www.tunefind.com/show/greys-anatomy\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow that I have the base URL for Grey's Anatomy, I can find the episodes for season 3.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"get_shows_from_season\",\n",
      "  \"action_input\": \"https://www.tunefind.com/show/greys-anatomy/season-3\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m[<h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-3/2019\">S3 · E1 · Time Has Come Today</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-3/2046\">S3 · E2 · I Am a Tree</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-3/2047\">S3 · E3 · Sometimes a Fantasy</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-3/2048\">S3 · E4 · What I Am</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-3/2049\">S3 · E5 · Oh, The Guilt</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-3/2050\">S3 · E6 · Let The Angels Commit</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-3/2051\">S3 · E7 · Where the Boys Are</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-3/2119\">S3 · E8 · Staring at the Sun</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-3/2120\">S3 · E9 · From a Whisper to a Scream</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-3/2190\">S3 · E10 · Don't Stand So Close to Me</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-3/2191\">S3 · E11 · Six Days (Part 1)</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-3/2252\">S3 · E12 · Six Days (Part 2)</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-3/2216\">S3 · E13 · Great Expectations</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-3/2306\">S3 · E14 · Wishin' and Hopin'</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-3/2253\">S3 · E15 · Walk on Water</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-3/2307\">S3 · E16 · Drowning on Dry Land</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-3/2369\">S3 · E17 · Some Kind of Miracle</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-3/2370\">S3 · E18 · Scars and Souvenirs</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-3/2371\">S3 · E19 · My Favorite Mistake</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-3/2476\">S3 · E20 · Time After Time</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-3/2477\">S3 · E21 · Desire</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-3/2554\">S3 · E22 · The Other Side of This Life</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-3/2555\">S3 · E23 · Testing 1-2-3</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-3/2556\">S3 · E24 · Didn't We Almost Have It All</a></h3>]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the name and URL for episode 8 of season 3 for Grey's Anatomy.\n",
      "Final Answer: The name of episode 8 for season 3 of Grey's Anatomy is \"Staring at the Sun\" and the URL is https://www.tunefind.com/show/greys-anatomy/season-3/2119\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The name of episode 8 for season 3 of Grey's Anatomy is \"Staring at the Sun\" and the URL is https://www.tunefind.com/show/greys-anatomy/season-3/2119\n"
     ]
    }
   ],
   "source": [
    "result = agent.run(\"What is the name and URL for episode 8 season 3 for Grey's Anatomy?\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace8cada",
   "metadata": {},
   "source": [
    "Success again!\n",
    "\n",
    "Note that along with understanding that giant mishmash of HTML, GPT also turned `/show/greys-anatomy/season-3/2119` into `https://www.tunefind.com/show/greys-anatomy/season-3/2119`. So polite!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29269f3a",
   "metadata": {},
   "source": [
    "# Method three: Convert your HTML\n",
    "\n",
    "Let's say things are even *more* complicated. Here's a look at the songs from [Season 1, Episode 3](https://www.tunefind.com/show/greys-anatomy/season-1/240) of Grey's Anatomy:\n",
    "\n",
    "![Songs from season 1 episode 3](assets/greys-anatomy-3.png)\n",
    "\n",
    "There's just so much there! Long classes, a million tags, just a ton of garbage.\n",
    "\n",
    "Here's the thing: *I don't want to send all that to ChatGPT*. I'm remarkably frugal, and while it's great to be lazy and send everything to GPT, **being lazy while using ChatGPT's API incurs direct financial consequences.** By reducing the data we send to GPT we get charged less, and that makes my wallet very very happy!\n",
    "\n",
    "In this case we're going to write a whole scraper for the page! When given a URL, we'll give back a list of artists and song titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5197c362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"artist\": \"The Ditty Bops\", \"title\": \"There\\'s a Girl\"}, {\"artist\": \"Tegan and Sara\", \"title\": \"There\\'s a Girl\"}, {\"artist\": \"The Ditty Bops\", \"title\": \"I Won\\'t Be Left\"}, {\"artist\": \"stuart reid\", \"title\": \"I Won\\'t Be Left\"}, {\"artist\": \"Reindeer Section\", \"title\": \"Wishful Thinking\"}, {\"artist\": \"Lisa Loeb\", \"title\": \"Wishful Thinking\"}, {\"artist\": \"Psapp\", \"title\": \"Hear You Breathing\"}, {\"artist\": \"Rilo Kiley\", \"title\": \"Hear You Breathing\"}, {\"artist\": \"Keane\", \"title\": \"You Are My Joy\"}, {\"artist\": \"Interpol\", \"title\": \"You Are My Joy\"}, {\"artist\": \"Psapp\", \"title\": \"Fools Like Me\"}, {\"artist\": \"Tegan and Sara\", \"title\": \"Fools Like Me\"}]'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the page\n",
    "url = \"https://www.tunefind.com/show/greys-anatomy/season-1/240\"\n",
    "headers = { 'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0' }\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Scrape the content\n",
    "soup = BeautifulSoup(response.text)\n",
    "titles = soup.select(\"[class^='SongTitle']\")\n",
    "artists = soup.select(\"[class^='ArtistSub']\")\n",
    "results = [{'artist': a.text, 'title': t.text} for t, a in (zip(titles, artists))]\n",
    "\n",
    "# Turn into a JSON string\n",
    "json.dumps(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e5b612",
   "metadata": {},
   "source": [
    "A lot nicer than what we saw last time, right? While both work fine with GPT-4, this one took a little more effort but costs noticeably less.\n",
    "\n",
    "One important thing to note is that **we're usng `json.dumps`** , which converts the Python object – a list of dictionaries that include artists and titles – into the string representation. We need to do this because every LangChain tool must return a string: that's what language models understand, so that's what we send. It looks exactly the same, but without the `json.dumps` it just won't work.\n",
    "\n",
    "Now let's apply this as our tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d290e6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_songs_from_episode(url: str) -> str:\n",
    "    \"\"\"Queries Tunefind for the songs for the specific episode of a show.\n",
    "    The input to this tool should be the URL to an episode.\n",
    "    The URL will look like https://www.tunefind.com/show/greys-anatomy/season-6/4120\n",
    "    You must visit the season page to obtain an episode URL\n",
    "    \"\"\"\n",
    "\n",
    "    headers = { 'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0' }\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(response.text)\n",
    "\n",
    "    titles = soup.select(\"[class^='SongTitle']\")\n",
    "    artists = soup.select(\"[class^='ArtistSub']\")\n",
    "    results = [{'artist': a.text, 'title': t.text} for t, a in (zip(titles, artists))]\n",
    "\n",
    "    return json.dumps(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6631d0d",
   "metadata": {},
   "source": [
    "Notice the grumpy demand: `You must visit the season page to obtain an episode URL`. Just like ChatGPT likes to hallucinate journalism stories and academic papers, it loves to think it knows an episode URL just by guessing a number after the season. That sentence is enough to keep it in line.\n",
    "\n",
    "Finally, **we'll string all three tools together**. The show search, the episode lister, and the song lister."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ef65af6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buld the agent using all three tools\n",
    "tools = [tunefind_search, get_shows_from_season, get_songs_from_episode]\n",
    "agent = initialize_agent(tools, llm, agent=\"chat-zero-shot-react-description\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b1adac",
   "metadata": {},
   "source": [
    "And now, the moment of truth!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e47bfdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find the base URL for Grey's Anatomy on Tunefind.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"tunefind_search\",\n",
      "  \"action_input\": \"Grey's Anatomy\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mGrey's Anatomy can be found at https://www.tunefind.com/show/greys-anatomy\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow I need to find the episodes for season 1.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"get_shows_from_season\",\n",
      "  \"action_input\": \"https://www.tunefind.com/show/greys-anatomy/season-1\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m[<h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-1/238\">S1 · E1 · A Hard Day's Night</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-1/239\">S1 · E2 · The First Cut is the Deepest</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-1/240\">S1 · E3 · Winning a Battle, Losing the War</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-1/255\">S1 · E4 · No Man's Land</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-1/256\">S1 · E5 · Shake Your Groove Thing</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-1/280\">S1 · E6 · If Tomorrow Never Comes</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-1/283\">S1 · E7 · The Self-Destruct Button</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-1/378\">S1 · E8 · Save Me</a></h3>, <h3 class=\"EpisodeListItem_title__PkSzj\"><a href=\"/show/greys-anatomy/season-1/381\">S1 · E9 · Who's Zoomin' Who?</a></h3>]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI found the episode URL for season 1 episode 3, now I need to get the songs from that episode.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"get_songs_from_episode\",\n",
      "  \"action_input\": \"https://www.tunefind.com/show/greys-anatomy/season-1/240\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m[{\"artist\": \"The Ditty Bops\", \"title\": \"There's a Girl\"}, {\"artist\": \"Tegan and Sara\", \"title\": \"There's a Girl\"}, {\"artist\": \"The Ditty Bops\", \"title\": \"I Won't Be Left\"}, {\"artist\": \"stuart reid\", \"title\": \"I Won't Be Left\"}, {\"artist\": \"Reindeer Section\", \"title\": \"Wishful Thinking\"}, {\"artist\": \"Lisa Loeb\", \"title\": \"Wishful Thinking\"}, {\"artist\": \"Psapp\", \"title\": \"Hear You Breathing\"}, {\"artist\": \"Rilo Kiley\", \"title\": \"Hear You Breathing\"}, {\"artist\": \"Keane\", \"title\": \"You Are My Joy\"}, {\"artist\": \"Interpol\", \"title\": \"You Are My Joy\"}, {\"artist\": \"Psapp\", \"title\": \"Fools Like Me\"}, {\"artist\": \"Tegan and Sara\", \"title\": \"Fools Like Me\"}]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI found the song by Lisa Loeb in Grey's Anatomy season 1 episode 3.\n",
      "Final Answer: The song by Lisa Loeb in Grey's Anatomy season 1 episode 3 is \"Wishful Thinking\".\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The song by Lisa Loeb in Grey's Anatomy season 1 episode 3 is \"Wishful Thinking\".\n"
     ]
    }
   ],
   "source": [
    "result = agent.run(\"What was the song by Lisa Loeb on Grey's Anatomy season 1 episode 3?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384d24b2",
   "metadata": {},
   "source": [
    "Perfect!\n",
    "\n",
    "## Final thoughts\n",
    "\n",
    "LangChain tools are great! While there are certainly other approaches to what we did above – external requests plugins, for example – combinng a small amount of manual scraping while letting GPT handle the details is a good combination of convenient and cost-effectve. Instead of sending all of the HTML (too big, too expensive) or just sending the text (too unpredictable, also potentially too large), carving out the bits you're actually interested in can do a lot for a tiny project.\n",
    "\n",
    "Combine this with something like [kor](https://github.com/eyurtsev/kor) or [guardrails](https://shreyar.github.io/guardrails/) and your life is pretty much perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae02ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
