{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4251111c",
   "metadata": {},
   "source": [
    "---\n",
    "title: Multi-language document Q&A with LangChain and GPT-3.5-turbo\n",
    "description: How to ask questions of documents when the documents are in foreign languages\n",
    "date: 2023-03-18\n",
    "author: Jonathan Soma\n",
    "format:\n",
    "  html:\n",
    "    toc: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e87fb5",
   "metadata": {},
   "source": [
    "Hi, I'm Soma! You can find me on email at [jonathan.soma@gmail.com](mailto:jonathan.soma@gmail.com), on Twitter at [@dangerscarf](https://twitter.com/dangerscarf), or maybe even on [this newsletter I've never sent](https://tinyletter.com/jsoma)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e956942c",
   "metadata": {},
   "source": [
    "# Multi-language document Q&A with LangChain and GPT-3.5-turbo \n",
    "\n",
    "## Using GPT, LangChain, and vector stores to ask questions of documents in languages you don't speak\n",
    "\n",
    "I don't speak Hungarian, but **I demand to have my questions about Hungarian folktales answered!** Let's use GPT to do this for us!\n",
    "\n",
    "*This might be useful if you're doing a cross-border investigation, are interested in academic papers outside of your native tongue, or are just interested in learning how LangChain and document Q&A works.*\n",
    "\n",
    "In this tutorial, we'll look at:\n",
    "\n",
    "1. Why making ChatGPT read an whole book is impossible\n",
    "2. How to provide GPT (and other AI tools) with context to provide answers\n",
    "\n",
    "If you don't want to read all of this nonsense you can go directly to the source and check out [Question Answering](https://langchain.readthedocs.io/en/latest/use_cases/question_answering.html) or [Question Answering with Sources\n",
    "](https://langchain.readthedocs.io/en/latest/modules/indexes/chain_examples/qa_with_sources.html). This just adds a bit of multi-language sparkle on top!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76fec50",
   "metadata": {},
   "source": [
    "## Our source material\n",
    "\n",
    "**We'll begin by downloading the source material.** If your original documents are in PDF form or anything like that, you'll want to convert them to text first.\n",
    "\n",
    "Our reference is a book of folktales called [Eredeti népmesék](https://www.gutenberg.org/ebooks/38852) by László Arany on Project Gutenberg. It's just [a basic text file](https://www.gutenberg.org/files/38852/38852-0.txt) so we can download it easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38106b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "# Gutenberg pretends everything is English, which\n",
    "# means \"Hát gyöngyömadta\" gets really mangled\n",
    "response = requests.get(\"https://www.gutenberg.org/files/38852/38852-0.txt\")\n",
    "text = response.content.decode(\"utf-8\")\n",
    "\n",
    "# Cleaning up newlines\n",
    "text = text.replace(\"\\r\", \"\")\n",
    "text = re.sub(\"\\n(?=[^\\n])\", \"\", text)\n",
    "\n",
    "# Saving the book\n",
    "with open('book.txt', 'w') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f0b498",
   "metadata": {},
   "source": [
    "And the text is indeed in Hungarian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7caa6abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be, de az is épen úgy járt, mint abátyja, ez is kiszaladt a szobából.\n",
      "Harmadik nap a legfiatalabb királyfin volt a sor; a bátyjai be se’akarták ereszteni, hogy ha ők ki nem tudták venni az apjokból, biz’ e’se’ sokra megy, de a királyfi nem tágitott, hanem bement. Mikor elmondtahogy m’ért jött, ehez is hozzá vágta az öreg király a nagy kést, de eznem ugrott félre, hanem megállt mint a peczek, kicsibe is mult, hogybele nem ment a kés, a sipkáját kicsapta a fejéből, úgy állt meg azajtóban. De a királyfi még ettől se’ ijedt meg, kihúzta a kést azajtóból, odavitte az apjának. ,,Itt van a kés felséges király atyám, hamegakar ölni, öljön meg, de elébb mondja meg mitől gyógyulna meg aszeme, hogy a bátyáim megszerezhessék.’’\n",
      "Nagyon megilletődött ezen a beszéden a király, nemhogy megölte volnaezért a fiát, hanem össze-vissza ölelte, csókolta. No kedves fiam –mondja neki – nem hiában voltál te egész életemben nekem legkedvesebbfiam, de látom most is te szántad el magad legjobban a halálra az énmeggyógyulásomért, (mert a kést is csak azért hajitottam utánatok, hogymeglássam melyikötök szállna értem szembe a halállal), most hát nekedmegmondom, hogy mitől gyógyulna meg a szemem. Hát kedves fiam,messze-messze a Verestengeren is túl, a hármashegyen is túl lakik egykirály, annak van egy aranytollu madara, ha én annak a madárnak csakegyszer hallhatnám meg a gyönyörű éneklését, mindjárt meggyógyulnéktőle; de nincs annyi kincs, hogy od’adná érte az a király, mert annyiannak az országában az aran\n"
     ]
    }
   ],
   "source": [
    "print(text[3000:4500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634db9df",
   "metadata": {},
   "source": [
    "Luckily for us, GPT speaks Hungarian! So if we tell it to read the book, it'll be able to answer all of our English-language questions without a problem. But there's one problem: the book is *not* a short tiny paragraph.\n",
    "\n",
    "Life would be nice if we could just feed it directly to ChatGPT and start asking questions, but **you can't make ChatGPT read a whole book**. After it gets partway through the book ChatGPT starts forgetting the earlier pieces!\n",
    "\n",
    "There are a few tricks to get around this. We'll work with one of the simplest for now:\n",
    "\n",
    "1. Split our original text up into pieces\n",
    "2. Find the pieces most relevant to our question\n",
    "3. Send those pieces to GPT along with our question\n",
    "\n",
    "Newer LLMs can deal with a lot more tokens at a time – GPT-4 has both an 8k and 32k version – but hey, we work with what we've got."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa39ba7",
   "metadata": {},
   "source": [
    "## Part 1: Split our original text up into pieces\n",
    "\n",
    "To do pretty much everything from here on out we're relying on [LangChain](https://langchain.readthedocs.io/), a really fun library that allows you to bundle together different common tasks when working with language models. It's best trick is chaining together AI at different steps in the process, but for the moment we're just using its text search abilities.\n",
    "\n",
    "We're going to split our text up into 1000-character chunks, which should be around 150-200 words apiece. I'm also going to add a little overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "15505e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader('book.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663599a4",
   "metadata": {},
   "source": [
    "Technically speaking I'm using a `RecursiveCharacterTextSplitter`, which tries to keep paragraphs and sentences and all of those things together, so it might go above or below 1000. But it should *generally* hit the mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c189e83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "440"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0e0f30",
   "metadata": {},
   "source": [
    "Overall this gave us just over 400 documents. Let's pick one at random to check out, just to make sure things went okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e8a8c6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Mikor aztán eljött a lakodalom napja, felöltözött, de olyan ruhába, hogyTündérországban se igen látni párját sátoros ünnepkor se, csak elfogta acselédje szemefényét. Mire a királyi palotához ért, már ott ugyancsakszólott a muzsika, úgy tánczoltak, majd leszakadt a ház, még a süketnekis bokájába ment a szép muzsika.', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[109]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8982027e",
   "metadata": {},
   "source": [
    "It's a little short, but it's definitely part of the folktales. According to Google Translate:\n",
    "\n",
    "> When the day of the wedding came, she dressed up, but in such a dress that one would not see her partner in a fairyland even during a tent festival, she only caught the eye of her mistress. By the time he got to the royal palace, the music was already playing there too, they were dancing like that, and then the house was torn apart, the beautiful music even went to the deaf man's ankles\n",
    "\n",
    "Sounds like a pretty fun party!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbac66b",
   "metadata": {},
   "source": [
    "## Part 2: Find the pieces most relevant to our question\n",
    "\n",
    "### Understanding text embeddings and semantic search\n",
    "\n",
    "If we're asking questions about a wedding, we can't just look for the text *wedding* – our documents are in Hungarian, so that's *lakodalom* (I think). Instead, we're going to use someting called **embeddings**.\n",
    "\n",
    "Embeddings take a word, sentence, or snippet of text and turn it into a string of numbers. Take the sentences below as an example: I've scored each one of them as to how much they're about shopping, home, and animals.\n",
    "\n",
    "|sentence|shopping|home|animals|result|\n",
    "|---|---|---|---|---|\n",
    "|You should buy a house|0.9|0.8|0|`(0.9, 0.8, 0.0)`|\n",
    "|The cat is in the house|0|1|0.8|`(0.0, 1.0, 0.8)`|\n",
    "|The dog bought a pet mouse|1|0.2|1|`(1.0, 0.2, 1.0)`|\n",
    "\n",
    "Let's say we have a fourth sentence – **the dog is at home**. I've decided it scores `(0.0 1.0 0.9)` since it's about home and animals, but not shipping. How can we find a similar text?\n",
    "\n",
    "**The cat is in the house** is the best match from our original list, *even though it doesn't have any words that match*. But if we ignore the words and look at the scores, it's clearly the best match! That's more or less the basic idea behind text embeddings and semantic search.\n",
    "\n",
    "Instead of reasonable categories like mine, actual embeddings are something like 384 or 512 different dimensions your text is scored on. And unlike \"shopping\" or \"animal\" above, the dimensions aren't anything you can understand. They're generated by computers that have read a lot lot lot of the internet, so we just have to trust them!\n",
    "\n",
    ":::{.callout-info}\n",
    "You might want to read [my introduction to word embeddings](https://investigate.ai/text-analysis/word-embeddings/) for more details and [conceptual document similarity](https://investigate.ai/text-analysis/document-similarity-using-word-embeddings/).\n",
    ":::\n",
    "\n",
    "### Creating and searching our embeddings database\n",
    "\n",
    "There are many, many embeddings out there, and they each score text differently. We need one that supports English (for our queries) and Hungarian (for the dataset): while not all of them support multiple languages, [it isn't hard to find some that do](https://www.sbert.net/docs/pretrained_models.html#multi-lingual-models)!\n",
    "\n",
    "We're going to pick `paraphrase-multilingual-MiniLM-L12-v2` since it supports a delightful 50 languages. That way we can ask questions in French or Italian, or maybe add some Japanese folklore to the mix later on.\n",
    "\n",
    "These multilingual embeddings have read enough sentences across the all-languages-speaking internet to *somehow* know things like that cat and lion and Katze and tygrys and 狮 are all vaguely feline. At this point don't need to know how it works, just that it gets the job done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b7b65bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name='paraphrase-multilingual-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ec8b96",
   "metadata": {},
   "source": [
    "In order to find the most relevant pieces of text, we'll also need something that can store and search embeddings. That way when we want to find anything about *weddings* it won't have a problem finding *lakodalom*.\n",
    "\n",
    "We're going to use [Chroma](https://github.com/chroma-core/chroma) for no real reason, just because it has a convenient LangChain extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e7e57d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll probably need to install chromadb\n",
    "# !pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4b5f951e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Chroma using direct local API.\n",
      "Using DuckDB in-memory for database. Data will be transient.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "db = Chroma.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795f10de",
   "metadata": {},
   "source": [
    "Now that this is stored, we can search for weddings at a festival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a94cc024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Eltelt az egy hónap, elérkezett az esküvő napja, ott volt a sok vendég,köztök a boltos is, csak a vőlegényt meg a menyasszonyt nem lehetettlátni. Bekövetkezett az ebéd ideje is, mindnyájan vígan ültek le azasztalhoz, elkezdtek enni. Az volt a szokás a gróf házánál, hogy mindenembernek egy kis külön tálban vitték az ételt; a boltos amint a magatáljából szedett levest, hát csak alig tudta megenni, olyan sótalanvolt, nézett körül só után, de nem volt az egész asztalon; a másodikétel még sótalanabb volt, a harmadik meg már olyan volt, hogy hozzá se’tudott nyúlni. Kérdezték tőle hogy mért nem eszik? tán valami baja vanaz ételnek? amint ott vallatták, eszébe jutott a lyánya, hogy az nekiazt mondta, hogy úgy szereti, mint a sót, elkezdett sírni; kérdeztékaztán tőle, hogy mért sír, akkor elbeszélt mindent, hogy volt neki egylyánya, az egyszer neki azt mondta, hogy úgy szereti mint a sót, őmegharagudott érte, elkergette a házától, lám most látja, hogy milyenigazságtalan volt iránta, milyen jó a só, ,,de hej ha még egyszervisszahozná az isten hozzám, majd meg is becsülném, első lenne aházamnál; meg is bántam én azt már sokszor, de már akkor késő volt.’’', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0)]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search(\"weddings at a festival with loud music\", k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a50e26",
   "metadata": {},
   "source": [
    "It's a match! Now we'll use this to find passages relevant to our question, that we'll then pass along to GPT as context for our questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8a0b5d",
   "metadata": {},
   "source": [
    "## Part 3: Send the matches to GPT along with our question\n",
    "\n",
    "This is the part where [LangChain](https://langchain.readthedocs.io/) really shines. We just say \"hey, go get the relevant pieces from our database, then go talk to GPT for us!\"\n",
    "\n",
    "First, we'll fire up our connection to GPT (you'll need to provide your own API key!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8b297ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Connect to GPT-3.5 turbo\n",
    "openai_api_key = \"sk-...\"\n",
    "\n",
    "llm = OpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81798339",
   "metadata": {},
   "source": [
    "Second, we'll put together our vector-based Q&A. This is a custom LangChain tool that takes our original question, finds relevant passages, and packages it all up to send over to GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "06e59ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector-database-based Q&A\n",
    "qa = VectorDBQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    vectorstore=db\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ce119c",
   "metadata": {},
   "source": [
    "## Let's see it in action!\n",
    "\n",
    "I'm going to ask some questions about Zsuzska, who according to some passages apparently stole some of the devil's belongings! I don't really know anything about her, this is just from a couple random passages I translated for myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b196f451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The tenger-ütő pálczát (sea-beating stick).'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What did Zsuzska steal from the devil?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5fbafa11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Zsuzska was forced to steal from the devil by the king, who threatened her with death if she didn't.\""
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Why did Zsuzska steal from the devil?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "749eb4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The king's aunts were jealous of Zsuzskát because the king had grown to love her and they wanted to undermine her by claiming that she could not steal the devil's golden cabbage head.\""
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Why were the king's aunts jealous of Zsuzska?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4938ab5a",
   "metadata": {},
   "source": [
    "That's a good amount of information about Zsuzskat! Let's try another character, Janko."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b1a72c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Janko married a beautiful princess.'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who did Janko marry?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8207cd20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The context does not provide information on a character named Janko meeting the king's daughter.\""
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How did Janko meet the princess?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7ff897",
   "metadata": {},
   "source": [
    "I know for a fact that Janko met the princess because *he stole her clothes while she was swimming in a lake*, but I guess the appropriate context didn't get sent to GPT. **It actually used to get the question right before I changed the embeddings!** In the next section we'll see how to provide more context and hopefully get better answers.\n",
    "\n",
    "There's also a big long story about a red or bloody row that had to do with a character's mother coming back to protect him. Let's see what we can learn about it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a3a9551a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The bloody cow was a cow that Ferkó rode away on after throwing the lasso at it.'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who was the bloody cow?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "de082682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ferko's mother was not disguised as a cow, but rather the red cow was actually Ferko's mother, the first queen.\""
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Why was Ferko's mother disguised as a cow?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd0454b",
   "metadata": {},
   "source": [
    "## Improving our answers from GPT\n",
    "\n",
    "When we asked what was stolen from the devil, we were told \"The tenger-ütő pálczát (sea-beating stick).\" I know for a fact more things were stolen than that!\n",
    "\n",
    "If we provide better context, we can hopefully get better answers. Usually \"better context\" means \"more context,\" so we have two major options:\n",
    "\n",
    "* Increase the size of our window/include more overlap so passages are longer\n",
    "* Provide more passages to GPT as context when asking for an answer (the default is 4)\n",
    "\n",
    "Since I haven't seen the second one show up too many places, let's do that instead. We'll increase the number of results to provide as context to eight by passing with `k=8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ccf3ec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = VectorDBQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    vectorstore=db,\n",
    "    k=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942c07cc",
   "metadata": {},
   "source": [
    "At this point we have to be careful of two things: money and token limits.\n",
    "\n",
    "1. **Money:** Larger requests cost more.\n",
    "2. **Token limits** We have around 3,000 words to work with for each GPT-3.5 request. If each chunk is up to 250 words long, this gets us up to 2,000 words. We should be safe!\n",
    "\n",
    "But we want good answers, right??? **Let's see if it works:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c66b6c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Zsuzska stole the devil's tenger-ütő pálczája (sea-beating stick), tenger-lépő czipője (sea-stepping shoes), and arany kis gyermek (golden baby) in an arany bölcső (golden cradle). She also previously stole the devil's tenger-ütőpálczát (sea-beating stick) and arany fej káposztát (golden head cabbage).\""
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What did Zsuzská steal from the devil?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0b9e26",
   "metadata": {},
   "source": [
    "Perfect! That gold cabbage sounds great, and it's almost time for lunch, so let's wrap up with *one more thing*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad38b3e1",
   "metadata": {},
   "source": [
    "## Seeing the context\n",
    "\n",
    "If you're having trouble getting good answers to your questions, it might be because the **context you're providing isn't very good.** I was actually having this issue earlier on with `distiluse-base-multilingual-cased-v2` before I switched to `paraphrase-multilingual-MiniLM-L12-v2`! I honestly don't know the difference between them, just that one provided more relevant snippets to GPT.\n",
    "\n",
    "Let's see what context is being provided to GPT for each search!\n",
    "\n",
    "### Method one: Context from the question\n",
    "\n",
    "To see what context is being sent to GPT, include the `return_source_documents=True` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f2d9e594",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = VectorDBQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    vectorstore=db,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b580af11",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did Zsuzská steal from the devil?\"\n",
    "result = qa({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "631d2850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Zsuzská stole the tenger-ütő pálczát (sea-beater stick) from the devil.'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "74455157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Hiába tagadta szegény Zsuzska, nem használt semmit, elindult hát nagyszomorúan. Épen éjfél volt, mikor az ördög házához ért, aludt az ördögis, a felesége is. Zsuzska csendesen belopódzott, ellopta a tenger-ütőpálczát, avval bekiáltott az ablakon.\\n– Hej ördög, viszem ám már a tenger-ütő pálczádat is.\\n– Hej kutya Zsuzska, megöletted három szép lyányomat, elloptad atenger-lépő czipőmet, most viszed a tenger-ütő pálczámat, de majdmeglakolsz te ezért.\\nUtána is szaladt, de megint csak a tengerparton tudott közel jutnihozzá, ott meg Zsuzska megütötte a tengert a tenger-ütő pálczával,kétfelé vált előtte, utána meg összecsapódott, megint nem foghatta megaz ördög. Zsuzska ment egyenesen a királyhoz.\\n– No felséges király, elhoztam már a tengerütő pálczát is.', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0),\n",
       " Document(page_content='De Zsuzska nem adta;,,Tán bolond vagyok, hogy visszaadjam, mikor kivülvagyok már vele az udvaron?!’’ Az ördög kergette egy darabig, de sehogyse tudta utolérni, utoljára is visszafordult, Zsuzska pedig mentegyenesen a király elibe, od’adta neki az arany fej káposztát.\\n– No felséges király elhoztam már ezt is!\\nA két nénjét Zsuzskának, majd hogy meg nem ütötte a guta, mikormegtudták, hogy Zsuzskának most se’ lett semmi baja, másnap megintbementek a királyhoz.\\n– Jaj felséges király van még annak az ördögnek egy arany kis gyermekeis arany bölcsőben, Zsuzska azt beszéli fűnek-fának, hogy ő azt is eltudná lopni.\\nMegint behivatta a király Zsuzskát.\\n– Fiam Zsuzska, azt hallottam, hogy van annak az ördögnek egy arany kisgyermeke is, arany bölcsőben, te azt is el tudod lopni, azt beszélted,azért ha az éjjel el nem lopod, halálnak halálával halsz meg.', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0),\n",
       " Document(page_content='– No felséges király, elhoztam már a tengerütő pálczát is.\\nA király még jobban megszerette Zsuzskát, hogy olyan életre való, de anénjei még jobban irigykedtek rá, csakhamar megint avval árulták be,hogy van annak az ördögnek egy arany fej káposztája is, Zsuzska azt isel tudná lopni, azt mondta. A király megint ráparancsolt Zsuzskára erősparancsolattal, hogy ha a káposztát el nem lopja, halálnak halálával halmeg.\\nElindult hát szegény Zsuzska megint, el is ért szerencsésen épen éjfélreaz ördög kertjibe, levágta az arany fej káposztát, avval bekiáltott azablakon.\\n– Hej ördög, viszem ám már az arany fej káposztádat is.\\n– Hej kutya Zsuzska, megöletted három szép lyányomat, elloptad atenger-lépő czipőmet, elloptad a tenger-ütő pálczámat, most viszed azarany fej káposztámat, csak ezt az egyet add vissza, soha szemedre sevetem.', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0),\n",
       " Document(page_content='Zsuzska csak nevette, de majd hogy sírás nem lett a nevetésből, mert azördög utána iramodott, Zsuzska meg nem igen tudott a nehéz bölcsővelszaladni, úgy annyira, hogy mire a tengerparthoz értek, tiz lépés nemsok, de annyi se volt köztök, hanem ott aztán Zsuzska felrántotta atenger-lépő czipőt, úgy átlépte vele a tengert, mint ha ott se lettvolna, avval mént egyenesen a király elibe, od’adta neki az arany kisgyermeket.\\nA király a mint meglátta, csak egy szikrába mult, hogy össze-vissza nemcsókolta Zsuzskát, de az is csak egy cseppbe mult ám, hogy a két nénjemeg nem pukkadt mérgibe, mikor meghallotta, hogy Zsuzska megintvisszakerült. Fúrta az oldalukat rettenetesen az irigység, mert látták,hogy a király napról-napra jobban szereti Zsuzskát. Bementek hát akirályhoz megint, azt hazudták neki hogy Zsuzska azt mondta, hogy vanannak az ördögnek egy zsák arany diója, ő azt is el tudná lopni.\\nMaga elibe parancsolta a király megint Zsuzskát:', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0)]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"source_documents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087f8265",
   "metadata": {},
   "source": [
    "### Method two: Just ask your database\n",
    "\n",
    "If you already know what GPT is going to say in response and you're debugging on specific query, you can just ask your database what the relevant snippets are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7dd5396f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Hiába tagadta szegény Zsuzska, nem használt semmit, elindult hát nagyszomorúan. Épen éjfél volt, mikor az ördög házához ért, aludt az ördögis, a felesége is. Zsuzska csendesen belopódzott, ellopta a tenger-ütőpálczát, avval bekiáltott az ablakon.\\n– Hej ördög, viszem ám már a tenger-ütő pálczádat is.\\n– Hej kutya Zsuzska, megöletted három szép lyányomat, elloptad atenger-lépő czipőmet, most viszed a tenger-ütő pálczámat, de majdmeglakolsz te ezért.\\nUtána is szaladt, de megint csak a tengerparton tudott közel jutnihozzá, ott meg Zsuzska megütötte a tengert a tenger-ütő pálczával,kétfelé vált előtte, utána meg összecsapódott, megint nem foghatta megaz ördög. Zsuzska ment egyenesen a királyhoz.\\n– No felséges király, elhoztam már a tengerütő pálczát is.', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0),\n",
       " Document(page_content='De Zsuzska nem adta;,,Tán bolond vagyok, hogy visszaadjam, mikor kivülvagyok már vele az udvaron?!’’ Az ördög kergette egy darabig, de sehogyse tudta utolérni, utoljára is visszafordult, Zsuzska pedig mentegyenesen a király elibe, od’adta neki az arany fej káposztát.\\n– No felséges király elhoztam már ezt is!\\nA két nénjét Zsuzskának, majd hogy meg nem ütötte a guta, mikormegtudták, hogy Zsuzskának most se’ lett semmi baja, másnap megintbementek a királyhoz.\\n– Jaj felséges király van még annak az ördögnek egy arany kis gyermekeis arany bölcsőben, Zsuzska azt beszéli fűnek-fának, hogy ő azt is eltudná lopni.\\nMegint behivatta a király Zsuzskát.\\n– Fiam Zsuzska, azt hallottam, hogy van annak az ördögnek egy arany kisgyermeke is, arany bölcsőben, te azt is el tudod lopni, azt beszélted,azért ha az éjjel el nem lopod, halálnak halálával halsz meg.', lookup_str='', metadata={'source': 'book.txt'}, lookup_index=0)]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search(\"What did Zsuzská steal from the devil?\", k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9c69f3",
   "metadata": {},
   "source": [
    "You can keep playing with your `k` values until you get what you think is enough context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cb77c8",
   "metadata": {},
   "source": [
    "## Improvements and next steps\n",
    "\n",
    "This is a collection of folktales, not one long story. That means asking about something like a wedding might end up mixing together all sorts of different stories! Our next step will allow us to add other books, filter stories from one another, and more techniques that can help with larger, more complex datasets.\n",
    "\n",
    "If you're interested in hearing when it comes out, feel free to follow me [@dangerscarf](https://twitter.com/dangerscarf) or [hop on my mailing list](https://tinyletter.com/jsoma). Questions, comments, and blind cat adoption inquiries can go to [jonathan.soma@gmail.com](mailto:jonathan.soma@gmail.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1f3fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
